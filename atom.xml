<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>benjamin.pizza</title>
    <link href="http://www.benjamin.pizza/atom.xml" rel="self" />
    <link href="http://www.benjamin.pizza" />
    <id>http://www.benjamin.pizza/atom.xml</id>
    <author>
        <name>Benjamin Hodgson</name>
        <email>bhodgson@stackoverflow.com</email>
    </author>
    <updated>2021-05-18T00:00:00Z</updated>
    <entry>
    <title>Some Notes on Variables and Binding</title>
    <link href="http://www.benjamin.pizza/posts/2021-05-18-some-notes-on-variables-and-binding.html" />
    <id>http://www.benjamin.pizza/posts/2021-05-18-some-notes-on-variables-and-binding.html</id>
    <published>2021-05-18T00:00:00Z</published>
    <updated>2021-05-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>I've been thinking a bit about how to build a library of tools to handle variables, capture-avoiding substitution, etc, on top of <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a>. Pretty much every compiler needs to deal with variables, and they're notoriously tricky to handle correctly.</p>
<p>I'm a way off having a final design, but I thought I'd publish my unfinished notes, hastily written in an afternoon. You'll see a few Thinking Emojis throughout this article, indicating things I haven't figured out yet. Please do get in touch if any of this gives you ideas.</p>
<h2 id="a-quick-look-at-some-prior-art"><a href="#a-quick-look-at-some-prior-art">A quick look at some prior art

</a></h2>
<p><a href="https://hackage.haskell.org/package/unbound"><strong><code>unbound</code></strong></a>/<a href="https://hackage.haskell.org/package/unbound-generics"><strong><code>unbound-generics</code></strong></a></p>
<ul>
<li><code>Name</code> is an abstract data type with fixed representation.
<ul>
<li><code>Bound</code> is a depth and an index. Requires patterns to have a canonical ordering, and bind each name only once
</li>
<li><code>Bound</code> discards the original name, but library API expects patterns to contain <code>Name</code>s
</li>
</ul>
</li>
<li>Statically encoded patterns with built in type combinators.
<ul>
<li>I like:
<ul>
<li>Statically declare structure of binders
</li>
<li>Embed your own datatypes as required by language syntax — method header can be <code>[(Name, Type)]</code>
</li>
</ul>
</li>
<li>I don't like:
<ul>
<li>Generics magic to find/traverse patterns
</li>
<li>Type constructor soup, eg <code>| LetRec (Bind (Rec [(Name, Embed Expr)]) Expr)</code>. Would be even noisier in C#.
</li>
</ul>
</li>
</ul>
</li>
<li>Quite closely integrated with generics libs
<ul>
<li>Most users will be using <code>RepLib</code>/<code>GHC.Generics</code>
</li>
<li>Lib uses <code>RepLib</code>/<code>GHC.Generics</code> internally to traverse patterns
</li>
</ul>
</li>
</ul>
<p><a href="https://www.schoolofhaskell.com/user/edwardk/bound"><strong><code>bound</code></strong></a></p>
<ul>
<li>Choose your own name (<code>data Expr a = Var a | ...</code>).
</li>
<li>Choose your own pattern/index.
<ul>
<li>Library explicitly doesn't manage patterns or names. It just does substitution using your monad. <code>Scope</code> doesn't contain a &quot;pattern&quot; object.
</li>
<li>Me gusta
</li>
</ul>
</li>
<li>Nested datatype manages de Bruijn depth statically.
<ul>
<li>Nested datatype is quite awkward in practice — doesn't play nicely with mutual recursion or Plated.
</li>
<li>Clever trick in <code>Scope</code> to speed up shifting at cost of canonicity. Prob a bit of a gimmick unless you're dealing with huge trees — but also, not super costly complexity-wise.
</li>
</ul>
</li>
<li>Works really nicely with standard classes: <code>Foldable</code>/<code>Traversable</code> to look at FVs, <code>Functor</code> for renaming, <code>Monad</code> for substitution. <code>Eq1</code> for alpha equivalence, etc. Very pleasing.
<ul>
<li>OTOH, <a href="https://stackoverflow.com/questions/39057576/mutually-recursive-syntaxes-with-bound">doesn't work with a non-monadic AST</a>.
</li>
</ul>
</li>
<li>Overall, probably not such a good fit for C# since we don't have HKTs, <code>Monad</code> etc. Not very library-able.
</li>
</ul>
<p><a href="https://github.com/brendanzab/moniker"><strong><code>moniker</code></strong></a></p>
<ul>
<li>More or less a port of <code>unbound</code>.
</li>
<li>Choose your own name (<code>Free&lt;N&gt;</code>/<code>Bound&lt;N&gt;</code>; <code>trait BoundTerm&lt;N&gt;</code>).
</li>
<li>Library defines a couple of traits (<code>BoundTerm</code>/<code>BoundPattern</code>) to locate variables, do substitution, etc. <code>unbound</code> uses generics for this.
</li>
<li>Traverse your own terms, but <code>derive</code> magic assists with implementing the library's traits.
</li>
</ul>
<h2 id="representing-names"><a href="#representing-names">Representing Names

</a></h2>
<p>I definitely want to use de Bruijn indexes (managed by the library) for bound variables.</p>
<pre><code class="language-csharp">readonly struct Bound
{
    public int Depth { get; }  // how many binding sites up should I look?
    public int Index { get; }  // where inside the binding site should I look?
}
</code></pre>
<p>For now, I'm not storing names in <code>Bound</code>. If you want to know the original name, go and find it in the pattern. Use of <code>int</code> for <code>Index</code> requires patterns to have a canonical ordering (and bind each variable only once).</p>
<p>Free variables: need an easy way to freshen them.</p>
<pre><code class="language-csharp">readonly struct Free
{
    public string OriginalName { get; }
    public int Freshness { get; }
}
</code></pre>
<p>Now just union them to represent names.</p>
<pre><code class="language-csharp">readonly struct Name
{
    public Bound Bound { get; }
    public Free Free { get; }
    public Name(Bound bound)
    {
        Bound = bound;
        Free = default;
    }
    public Name(Free free)
    {
        Bound = default;
        Free = free;
    }
    public bool IsBound =&gt; free.OriginalName == null;
    public bool IsFree =&gt; free.OriginalName != null;
}
</code></pre>
<p>How much of this should be public? Should these be structs or classes? (<code>Name</code> is like 4 words.) 🤔</p>
<h2 id="representing-scopes"><a href="#representing-scopes">Representing Scopes

</a></h2>
<p>I definitely want an explicit type to represent a scope. I personally think <code>BindingSite</code> is a slightly better name, although perhaps that should be reserved for names inside patterns? 🤔</p>
<p>The idea is to treat <code>BindingSite</code> as an abstract data type. To create a <code>BindingSite</code> (such as when parsing a lambda expression), you call <code>Binder.Bind(variables, body)</code>. The binder traverses the <code>body</code> AST, looking for variables which are bound by the lambda and converting them to <code>Bound</code> de Bruijn indexes. Likewise, when you need to go inside a <code>BindingSite</code>, you have to explicitly unbind it.</p>
<p>Don't want to discard the original names, of course.</p>
<p><strong>Idea 1</strong>: Store the original name with the <code>Bound</code> variable (just as an annotation, ignore in alpha-equivalence etc).</p>
<pre><code class="language-csharp">readonly struct Bound
{
    public int Depth { get; }
    public int Index { get; }
    public Free OriginalName { get; }
}
</code></pre>
<p>Potential consistency issues — different mentions of the same variable could end up with different tags.</p>
<p><strong>Idea 2</strong>: Store a flat list of original names in the <code>BindingSite</code>.</p>
<pre><code class="language-csharp">readonly struct BindingSite&lt;T&gt;  // aka Scope (bound/moniker) or Bind (unbound)
{
    // How much of this should be public? 🤔
    public ImmutableArray&lt;Name&gt; OriginalNames { get; }
    public T Body { get; }
}
</code></pre>
<p>It feels kinda ugly. Also potential consistency issues — if your binding sites have more syntactic structure than that (patterns, type signatures, etc), you're gonna end up duplicating information. Also, it doesn't seem like this would easily support recursive patterns such as letrec or Agda's dot notation.</p>
<p><strong>Idea 3</strong>: Store the actual pattern in the <code>BindingSite</code>. This is more in line with how <code>unbound</code>/<code>moniker</code> do it.</p>
<pre><code class="language-csharp">readonly struct BindingSite&lt;T&gt;
{
    public Pattern Pattern { get; }
    public T Body { get; }
}
abstract class Pattern {}
class Variable : Pattern  // Do I need to support &quot;anonymous&quot; variables? 🤔
{
    public Name Name { get; }
}
class Embed&lt;T&gt; : Pattern
{
    public T Embedded { get; }
}
class Telescope : Pattern  // known as Rebind in other libs
{
    public BindingSite&lt;Pattern&gt; BindingSite { get; }
}
class Rec : Pattern
{
    // technically this is a binding site too. 🤔
    public Pattern Body { get; }
}
</code></pre>
<p>Some things about this work really well. We can make <code>Pattern</code> extensible by having it implement <code>IRewritable</code> abstractly. I define a few base patterns which my library recognises, and you can add your own. To (eg) find the variables in a pattern I just need to be able to traverse your custom pattern types with <code>GetChildren</code>.</p>
<pre><code class="language-csharp">abstract class Pattern : IRewritable&lt;Pattern&gt;
{
    // ...
    public IEnumerable&lt;Free&gt; GetNames()
        =&gt; this
            .SelfAndDescendants()
            .OfType&lt;Variable&gt;()
            // names which aren't free in a pattern are recursive
            // references to other names bound in this pattern
            .Where(v =&gt; v.Name.IsFree)
            .Select(x =&gt; x.Name.Free);
}
</code></pre>
<p>Ways this doesn't scale:</p>
<ul>
<li>If your language has multiple types of patterns (eg, type variables and value variables) you can't statically distinguish them.
</li>
<li>The structure of the pattern lives only at runtime — it all gets stuffed into a <code>Pattern</code>-typed variable. You can't <em>statically</em> encode the syntactic structure of the binding site (at which point it's not a whole lot better than Idea 2).
</li>
<li>&quot;Rewrite all the embedded terms&quot; is an ad-hoc affair. Long-standing shortcoming of <code>IRewritable</code>, for which I haven't come up with a satisfactorily simple solution.
</li>
</ul>
<p>What should be the advice for implementing <code>IRewritable</code> on objects containing a binding site? Should you traverse to the body? (Problematic because de Bruijn indexes from different scopes are not comparable.) Are you meant to unbind the body? That would mean you can't use <code>IRewritable</code> as is, because <code>IRewritable</code> doesn't have a parameter for a source of fresh names. Perhaps we should be using <code>IRewriter</code> and not <code>IRewritable</code>, although that's a messier API. 🤔</p>
<blockquote>
<p>This ☝🏻 is a pressing concern because <code>Telescope</code> features a <code>BindingSite</code>!</p>
</blockquote>
<p><strong>Idea 3.1</strong>: Statically type the <code>Pattern</code> using generics.</p>
<pre><code class="language-csharp">readonly struct BindingSite&lt;TPattern, T&gt; where TPattern : Pattern
{
    public TPattern Pattern { get; }
    public T Body { get; }
}
class Telescope&lt;L, R&gt; : Pattern
    where L : Pattern
    where R : Pattern
{
    // open question on how to treat BindingSite with IRewritable
    public BindingSite&lt;L, R&gt; BindingSite { get; }
}
class Rec&lt;T&gt; : Pattern
    where T : Pattern
{
    public T Body { get; }
}
</code></pre>
<p>Some things about this work really well too, as evidenced by its use in <code>unbound</code>/<code>moniker</code>. I find the &quot;type soup&quot; usability issue a bit concerning, but more pressingly it doesn't play nicely with <code>IRewritable</code>.</p>
<pre><code class="language-csharp">class Telescope&lt;L, R&gt;
{
    // ...
    public int CountChildren() =&gt; 2;
    public void GetChildren(Span&lt;Pattern&gt; children)
    {
        // debatably we should only be descending to the Pattern,
        // because the Body is in a different scope
        children[0] = BindingSite.Pattern;
        children[1] = BindingSite.Body;
    }
    public Pattern SetChildren(ReadOnlySpan&lt;Pattern&gt; children)
        =&gt; new Telescope&lt;L, R&gt;((L)children[0], (R)children[1]);
        //                      ^ damn!
}
</code></pre>
<p>That unsafe cast really is unsafe, because a <code>Rewrite</code> operation is liable to change the type of the pattern. Maybe patterns should be read-only? Do I need to split the read/write parts of <code>IRewritable</code>? 🤔</p>
<p><strong>Idea 4</strong>: A reflection-based API, perhaps with attributes? Seems like it definitely has legs, but I find reflection-based APIs distasteful — they're hard to reason about and hard to program with. (It's much easier to manipulate values than code.) I'd prefer to come up with a <em>model</em>, which you can manipulate directly, and then layer a reflection API on top.</p>
<h2 id="asts-with-binding-structure"><a href="#asts-with-binding-structure">ASTs with Binding Structure

</a></h2>
<p>This part is fairly worked out I think. I'm assuming that your syntax tree has a case for variables, containing a <code>Name</code> (and no children) and a case for binding sites, containing a <code>BindingSite</code> (and no children). So all we need to add to <code>IRewritable</code> is a way to identify those nodes:</p>
<pre><code class="language-csharp">interface IBindable&lt;T&gt; : IRewritable&lt;T&gt; where T : IBindable&lt;T&gt;
{
    bool TryGetName(out Name name);
    T SetName(Name newName);

    bool TryGetBindingSite(out BindingSite&lt;T&gt; bindingSite);
    T SetBindingSite(BindingSite&lt;T&gt; newBindingSite);
}
</code></pre>
<p>Not so different than <a href="/posts/2019-12-15-generic-unification-with-sawmill.html"><code>IUnifiable</code></a>.</p>
<h2 id="freshening"><a href="#freshening">Freshening

</a></h2>
<p>When you go under a binder you (usually) want to replace the de Bruijn indexes in there with named variables. Those variables need to be <em>fresh</em>, that is, they need to not clash with (or <em>capture</em>) any other variables in scope. As a UX concern, you probably want the fresh name to be somewhat similar to the name the programmer typed.</p>
<p>So we have <code>IFreshener</code>:</p>
<pre><code class="language-csharp">interface IFreshener
{
    Free Freshen(Free name);
}
</code></pre>
<p>And a straightforward implementation with a global counter.</p>
<pre><code class="language-csharp">class Freshener : IFreshener
{
    private int _counter = 0;
    public Free Freshen(Free name)
    {
        _counter++;  // Interlocked.Increment if you need thread safety
        return new Free(name.OriginalName, _counter);
    }
}
</code></pre>
<p>As an optimisation (and as a UX improvement), you can avoid freshening variables if you're sure they don't clash with any other names which are in scope. That means keeping track of the set of names we want to avoid:</p>
<pre><code class="language-csharp">interface IFreshener
{
    Free Freshen(Free name);
    void EnterScope(IEnumerable&lt;Free&gt; names);
    void ExitScope();  // free up the names from the last EnterScope call
}
class Freshener : IFreshener
{
    public Free Freshen(Free name) { /* ... */ }
    public void EnterScope(IEnumerable&lt;Free&gt; names) {}
    public void ExitScope() {}
}
class LocalFreshener : IFreshener
{
    // use a better data structure in practice, obv
    private readonly Stack&lt;IEnumerable&lt;Free&gt;&gt; _names = new();
    public Free Freshen(Free name)
    {
        while (_names.Any(ns =&gt; ns.Contains(name)))
        {
            name = name.IncrementFreshness();
        }
        return name;
    }
    public void EnterScope(IEnumerable&lt;Free&gt; names)
    {
        _names.Push(names);
    }
    public void ExitScope()
    {
        _names.Pop();
    }
}
</code></pre>
<h2 id="traversing-the-tree"><a href="#traversing-the-tree">Traversing the tree

</a></h2>
<p>OK, finally we have everything we need to bind and unbind variables in ASTs. Let's suppose we went with <strong>Idea 2</strong> from above — just storing a flat list of names at the binding site.</p>
<pre><code class="language-csharp">static class Binder
{
    public static BindingSite&lt;T&gt; Bind&lt;T&gt;(ImmutableArray&lt;Free&gt; variables, T body) where T : IBindable&lt;T&gt;
    {
        var variablesLookup = variables
            .Select((x, i) =&gt; new KeyValuePair&lt;Free, int&gt;(x, i))
            .ToImmutableDictionary();

        var level = 0;
        T Go(T x)
        {
            if (x.TryGetName(out var name))  // x is a variable
            {
                if (name.IsFree &amp;&amp; variablesLookup.TryGetValue(name.Free, out var ix))
                {
                    return x.SetName(new Name(new Bound(level, ix)));
                }
                return x;
            }
            else if (x.TryGetBindingSite(out var bs))  // x is a binding site
            {
                level++;
                var newBody = Go(bs.Body);
                level--;
                return x.SetBindingSite(bs.WithBody(newBody));
            }
            else
            {
                return x.RewriteChildren(Go);
            }
        }
        return new BindingSite&lt;T&gt;(variables, Go(body));
    }

    public static (ImmutableArray&lt;Free&gt;, T) Unbind&lt;T&gt;(BindingSite&lt;T&gt; bindingSite, IFreshener freshener) where T : IBindable&lt;T&gt;
    {
        var variables = bindingSite
            .Variables
            .Select(freshener.Freshen)
            .ToImmutableArray();

        var level = 0;
        T Go(T x)
        {
            if (x.TryGetName(out var name))
            {
                if (name.IsBound &amp;&amp; name.Bound.Level == level)
                {
                    return x.SetName(new Name(variables[name.Bound.Index]));
                }
                return x;
            }
            else if (x.TryGetBindingSite(out var bs))
            {
                level++;
                var newBody = Go(bs.Body);
                level--;
                if (!ReferenceEquals(bs.Body, newBody))
                {
                    return x.SetBindingSite(bs.WithBody(newBody));
                }
                return x;
            }
            else
            {
                return x.RewriteChildren(Go);
            }
        }
        return (variables, Go(bindingSite.Body));
    }
}
</code></pre>
<p>You can develop versions of <code>Rewrite</code>, <code>SelfAndDescendants</code>, and so on, which appropriately <code>Bind</code> and <code>Unbind</code> variables inside the tree.</p>
<p>There are a couple of design issues regarding the performance of <code>Bind</code>/<code>Unbind</code>. Each call traverses and rewrites the whole tree and potentially chews through a bunch of fresh names, so if you bind and unbind a lot then you're gonna have performance problems.</p>
<p>There might be room for improvement here — off the top of my head, you could rewrite everything up to the next binding site and just store a &quot;substitution to be applied&quot; there. You can defer applying the changes until you're asked to open that binding site. I suspect that'd give you asymptotic speedups (quadratic -&gt; linear?) for certain algorithms which do lots of renaming.</p>
<p>Also, some tree operations don't require you to <code>Unbind</code> and re-<code>Bind</code> every single binding site. If you're doing some sort of operation where you're not generating new names or binding sites, or moving nodes across binding sites, you can just leave <code>Bound</code> variables where they are. That's an argument for making (eg) <code>bindingSite.Body</code> public, and going under it in <code>GetChildren</code>/<code>SetChildren</code>. Although in practice that's the sort of thing which is hard to get right and can cause subtle (or not-so-subtle) bugs in your language implementation.</p>

]]></summary>
</entry>
<entry>
    <title>Building Prolog's Rules Engine</title>
    <link href="http://www.benjamin.pizza/posts/2019-12-22-building-prologs-rules-engine.html" />
    <id>http://www.benjamin.pizza/posts/2019-12-22-building-prologs-rules-engine.html</id>
    <published>2019-12-22T00:00:00Z</published>
    <updated>2019-12-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>This is part of a series of posts about implementing a miniature Prolog interpreter in C#.</p>
<nav>
    <ol>
        <li>[Introduction & Syntax](/posts/2019-12-01-write-you-a-prolog.html)</li>
        <li>[Parsing](/posts/2019-12-08-parsing-prolog-with-pidgin.html)</li>
        <li>[Unification](/posts/2019-12-15-generic-unification-with-sawmill.html)</li>
        <li>**The rules engine**</li>
    </ol>
</nav>
<img src="/images/2017-11-13-recursion-without-recursion/compiler.jpg" alt="Compiler overview" />
<p>Today's the day! We're going to turn last week's unification algorithm into an actual programming language by filling in the bottom-right part of the above diagram. Prolog's <em>rules engine</em> is the system which processes the predicates and facts in your program to answer queries.</p>
<h2 id="the-database"><a href="#the-database">The Database

</a></h2>
<p>When you load a program into your Prolog interpreter, it reads the collection of rules into an internal data structure. This data structure is known as Prolog's <em>database</em>. For simplicity we can represent the database as an array of rules (ie, the same structure which comes out of the parser), but a real Prolog interpreter would use a more specialised data structure to make querying more efficient.</p>
<pre><code class="language-csharp">class Engine
{
    private readonly ImmutableArray&lt;Rule&gt; _database;

    public Engine(ImmutableArray&lt;Rule&gt; database)
    {
        _database = database;
    }

    // ...
}
</code></pre>
<h2 id="a-first-attempt"><a href="#a-first-attempt">A First Attempt

</a></h2>
<p>Prolog's interactive terminal accepts <em>queries</em> (predicates) and outputs a <em>substitution</em> which satisfies the query, according to the rules in the database. So the signature for <code>Query</code> is</p>
<pre><code class="language-csharp">public ImmutableDictionary&lt;string, Term&gt; Query(Term goal)
</code></pre>
<p>If a goal can't be satisfied, <code>Query</code> will return <code>null</code>.</p>
<p>When you type a query, Prolog looks for the first rule in its database which matches the goal. In code, that means looping over <code>_database</code> and ignoring rules whose <code>Head</code> doesn't unify with the goal.</p>
<pre><code class="language-csharp">public ImmutableDictionary&lt;string, Term&gt; Query(Term goal)
{
    foreach (var rule in _database)
    {
        // call the overload below
        var result = Query(rule, goal);
        if (result != null)
        {
            return result;
        }
    }
    return null;
}
private ImmutableDictionary&lt;string, Term&gt; Query(Rule rule, Term goal)
{
    ImmutableDictionary&lt;string, Term&gt; subst;
    try
    {
        subst = rule.Head.Unify(goal);
    }
    catch (UnificationError)
    {
        return null;
    }
    // ...
}
</code></pre>
<p>If the rule's head does match the goal, Prolog needs to check the conditions on the right hand side of the rule. Each predicate on the right-hand side becomes a sub-goal; we'll recursively call <code>Query</code> for each of these sub-goals. These recursive <code>Query</code> calls will return a substitution, and the information from those substitutions needs to be propagated bi-directionally. It's just like when we were solving a system of equations in the last post: we'll propagate information forwards by <code>Apply</code>ing the current substitution to later sub-goals, and propagate it backwards by <code>Compose</code>-ing the substitutions from each recursive <code>Query</code> call into the current substitution.</p>
<pre><code class="language-csharp">public ImmutableDictionary&lt;string, Term&gt; Query(Rule rule, Term goal)
{
    // ... 
    foreach (var predicate in rule.Body)
    {
        var subst2 = Query(subst.Apply(predicate));
        if (subst2 == null)
        {
            return null;
        }
        subst = subst.Compose(subst2);
    }
    return subst;
}
</code></pre>
<p>If any of the recursive <code>Query</code> calls failed, that means one of the conditions on the right hand side was not satisfiable, so the whole rule fails (this function returns <code>null</code>) and we have to try the next one. Eventually we hope to find a rule whose head matches the goal <em>and</em> whose conditions are satisfied. This means the query was successful, so we return the <code>subst</code> corresponding to the rule.</p>
<p>That's a decent first stab at the code for <code>Query</code>. Now we have to fix the bugs.</p>
<h2 id="backtracking"><a href="#backtracking">Backtracking

</a></h2>
<p>Here's a trimmed-down version of the example database from the first post.</p>
<pre><code class="language-prolog">likes(benjamin, asparagus).
likes(benjamin, pizza).
likes(clio, pizza).

dinner(Food) :- likes(benjamin, Food), likes(clio, Food).
</code></pre>
<p>When we feed this database into our <code>Engine</code> and ask it what's for dinner,</p>
<pre><code class="language-csharp">var program = @&quot;
likes(benjamin, asparagus).
likes(benjamin, pizza).
likes(clio, pizza).

dinner(Food) :- likes(benjamin, Food), likes(clio, Food).
&quot;;
var ast = PrologParser.ParseProgram(program);
var engine = new Engine(ast);

var query = PrologParser.ParseQuery(&quot;dinner(Food)&quot;);
var result = engine.Query(query);

if (result == null)
{
    Console.WriteLine(&quot;no solution&quot;);
}
else
{
    Console.WriteLine(Write(result));
}
</code></pre>
<p>it replies <code>no solution</code>, even though there is a clear solution (<code>Food := pizza</code>). What did we do wrong?</p>
<p>Let's think about how our engine is going to execute this query.</p>
<ol>
<li>Find a rule whose head unifies with the goal. There's only one option: the <code>dinner(Food)</code> line.
</li>
<li>Query for the first predicate in the rule's body, namely <code>likes(benjamin, Food)</code>.
</li>
<li>Find a rule whose head unifies with the predicate. There are two options, <code>likes(benjamin, asparagus)</code> and <code>likes(benjamin, pizza)</code>. The engine chooses the first one, so <code>Food := asparagus</code>.
</li>
<li>Now query for the second predicate in the rule's body, which is <code>likes(clio, Food)</code>. Since <code>Food := asparagus</code>, we're looking for <code>likes(clio, asparagus)</code>
</li>
<li>Since there are no rules matching <code>likes(clio, asparagus)</code> the query fails.
</li>
</ol>
<p>The bug was in step 3, when the engine committed to the first rule which matched. It later realised that <code>Food</code> couldn't be <code>asparagus</code>, so it needs to be able to <em>backtrack</em> and undo that decision. (This is very similar to the backtracking which parsers do, as detailed in an earlier post.)</p>
<p>The reality is that each goal may have multiple substitutions which satisfy it. <code>likes(benjamin, Food)</code> has two solutions, <code>Food := asparagus</code> and <code>Food := pizza</code>, but we threw away the second one. So our <code>Query</code> signature really should've looked like this:</p>
<pre><code class="language-csharp">public IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Term goal)
</code></pre>
<p>When there's no solution, <code>Query</code> will return an empty list (rather than <code>null</code> as before). I'll update the implementation to work with a collection of substitutions.</p>
<pre><code class="language-csharp">public IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Term goal)
    =&gt; _database.SelectMany(rule =&gt; Query(rule, goal));

private IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Rule rule, Term goal)
{
    IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; substs;
    try
    {
        substs = new[] { rule.Head.Unify(goal) };
    }
    catch (UnificationError)
    {
        return new ImmutableDictionary&lt;string, Term&gt;[] { };
    }
        
    foreach (var predicate in rule.Body)
    {
        substs =
            from subst in substs
            from subst2 in Query(subst.Apply(predicate))
            select subst.Compose(subst2);
    }

    return substs;
}
</code></pre>
<p><code>SelectMany</code> (and <code>from...select</code>, which is the same thing under the hood) coming in clutch here. Each predicate in the body of a rule has potentially many solutions, so each recursive call to <code>Query</code> introduces potentially many possible branches to choose from. <code>SelectMany</code> lets us take all of them — but because <code>IEnumerable</code> is lazy, at runtime it doesn't actually explore more branches than necessary to find a single solution. It also handles the &quot;no solutions&quot; case gracefully: if <code>substs</code> is empty then the body of the <code>foreach</code> loop does nothing. (<code>SelectMany</code> always returns an empty collection when its input collection is empty.)</p>
<p>Working with immutable data is of utmost importance here. Combining lazy <code>IEnumerable</code>s with mutable data can have catastrophic consequences because it's hard to predict when your code will be run. The body of my loop computes a <em>new</em> <code>substs</code> to be used for the next iteration; it doesn't update a single collection of solutions. I don't know what would happen if you did write it like that but I know it would be wrong!</p>
<p>Feeding the <code>dinner(Food)</code> query into this version of the code correctly outputs <code>Food := pizza</code>.</p>
<h2 id="scope-management"><a href="#scope-management">Scope Management

</a></h2>
<h3 id="discharging">[Discharging](#discharging</h3>
<p>Here's another example program which reveals a bug in <code>Query</code>.</p>
<pre><code class="language-prolog">foo(X).
bar(baz).

test() :- foo(wibble), bar(X).
</code></pre>
<p><code>test()</code> is straightforwardly true, but our interpreter again prints out <code>no solution</code>. Once again, let's walk through the steps the engine will take.</p>
<ol>
<li>Query <code>foo(wibble)</code>. The only rule matching this goal is <code>foo(X).</code>, which matches when <code>X := wibble</code>. So the recursive call to <code>Query</code> returns the substitution <code>X := wibble</code>.
</li>
<li>Apply that substitution to the rest of the <code>test</code> rule.
</li>
<li>Query the second predicate in <code>test</code>, which is <code>bar(wibble)</code>. This goal fails because the only <code>bar</code>-related fact in the database is <code>bar(baz)</code>.
</li>
</ol>
<p>The bug is in step 1, when we returned the <code>X := wibble</code> substitution. <code>foo</code>'s <code>X</code> escaped its scope and collided with the <code>X</code> in <code>test</code>'s body. We need to adjust our code so that variables can't escape their scope by being returned in a substitution.</p>
<p>The fix is to <em>discharge</em> some of the equalities in a substitution at the end of running a rule (that is, on the last line of <code>Query</code>). We want to throw out equalities related to variables which were local to the rule, keeping only the ones which tell you something about the variables in the goal.</p>
<pre><code class="language-csharp">private IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Rule rule, Term goal)
{
    // ...
    return substs.Select(s =&gt; Discharge(s, goal));
}

private static ImmutableDictionary&lt;string, Term&gt; Discharge(ImmutableDictionary&lt;string, Term&gt; subst, Term goal)
    =&gt; subst
        .Where(kvp =&gt; goal.Variables().Contains(kvp.Key))
        .ToImmutableDictionary();
</code></pre>
<p>This fixes the bug. <code>test()</code> now correctly returns an empty substitution.</p>
<h3 id="freshening">[Freshening](#freshening</h3>
<p>There's one final bug to fix in the engine.</p>
<pre><code class="language-prolog">foo(a, X).
</code></pre>
<p>Running the query <code>foo(X, b)</code> should return <code>X := a</code>, but the interpreter prints <code>no solution</code>. This is of course another name collision issue. When unifying the goal with the left hand side, the interpreter has mistaken the two <code>X</code>s for being the <em>same</em> <code>X</code>. The <code>X</code> in <code>foo</code>'s left hand side should be local to <code>foo</code>.</p>
<p>To avoid unintentional collisions we need to <em>freshen</em> local variables. Whenever the interpreter enters a rule, it should rename the rule's local variables to new variables which aren't being used anywhere in the goal. An easy way to guarantee that a name isn't being used anywhere in the goal is to stick a number on the end of it, and increase that number by 1 each time. (I'm also going to put an <code>?</code> on the front of the auto-generated variable names, because variables beginning with <code>?</code> is not valid Prolog so it's guaranteed not to collide with a name typed by a programmer.)</p>
<pre><code class="language-csharp">private int _freshenCounter = 0;
private Rule Freshen(Rule rule)
{
    // find all of the rule's local variables.
    // NB: local variables may appear in the rule's body but not the head.
    var variables = rule.Body
        .Select(x =&gt; ((Term)x).Variables())
        .Aggregate(((Term)rule.Head).Variables(), (xs, ys) =&gt; xs.Union(ys));
    
    // build a substitution which maps each variable to a fresh version of that variable
    var subst = ImmutableDictionary&lt;string, Term&gt;.Empty;
    foreach (var variable in variables)
    {
        subst = subst.Add(variable, new Variable(&quot;?&quot; + variable + _freshenCounter));
        _freshenCounter++;
    }

    // apply the substitution everywhere in the rule
    return new Rule(
        (Predicate)subst.Apply(rule.Head),
        rule.Body.Select(subst.Apply).Cast&lt;Predicate&gt;().ToImmutableArray()
    );
}
</code></pre>
<p>Then at the start of <code>Query</code> we need to <code>Freshen</code> the rule we're about to enter.</p>
<pre><code class="language-csharp">private IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Rule rule, Term goal)
{
    rule = Freshen(rule);
    // ...
}
</code></pre>
<p>The <code>foo(X, b)</code> query now correctly returns <code>X := a</code>.</p>
<p>Here's the final implementation of <code>Query</code> in full.</p>
<pre><code class="language-csharp">public IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Term goal)
    =&gt; _database.SelectMany(rule =&gt; Query(rule, goal));

private IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; Query(Rule rule, Term goal)
{
    rule = Freshen(rule);

    IEnumerable&lt;ImmutableDictionary&lt;string, Term&gt;&gt; substs;
    try
    {
        substs = new[] { rule.Head.Unify(goal) };
    }
    catch (UnificationError)
    {
        return new ImmutableDictionary&lt;string, Term&gt;[] { };
    }
    
    foreach (var predicate in rule.Body)
    {
        substs =
            from subst in substs
            from subst2 in Query(subst.Apply(predicate))
            select subst.Compose(subst2);
    }

    return substs.Select(s =&gt; Discharge(s, goal));
}
</code></pre>
<p>Here are some exercises you could try:</p>
<ul>
<li>Because we implemented <code>_database</code> as an array, finding a matching rule is linear in the size of the program. Try optimising the <code>_database</code> data structure.
</li>
<li>Try optimising <code>Freshen</code> so that it doesn't rename more variables than it needs to.
<ul>
<li>It's actually possible to avoid freshening altogether by changing the way variables are represented.
</li>
</ul>
</li>
<li>Backtracking is enabled by default in Prolog (unlike in Pidgin where it's disabled by default). The full Prolog language includes the <code>!</code> operator (pronounced <em>cut</em>) to disable backtracking when necessary. When Prolog encounters <code>!</code> in the right-hand side of a rule, it commits to all of the choices it's made since (and including) selecting that rule from the database. Try implementing cut as an exercise. (You'll need to change the parser, the AST, and the rules engine.)
</li>
</ul>
<p>Today's code is available in <a href="https://github.com/benjamin-hodgson/Amateurlog">the example repo</a>, in the <a href="https://github.com/benjamin-hodgson/Amateurlog/blob/80a4ac12303d007295059ac4d9e36ce935a904c2/Engine.cs"><code>Engine.cs</code></a> file.</p>

]]></summary>
</entry>
<entry>
    <title>Generic Unification with Sawmill</title>
    <link href="http://www.benjamin.pizza/posts/2019-12-15-generic-unification-with-sawmill.html" />
    <id>http://www.benjamin.pizza/posts/2019-12-15-generic-unification-with-sawmill.html</id>
    <published>2019-12-15T00:00:00Z</published>
    <updated>2019-12-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>This is part of a series of posts about implementing a miniature Prolog interpreter in C#.</p>
<nav>
    <ol>
        <li>[Introduction & Syntax](/posts/2019-12-01-write-you-a-prolog.html)</li>
        <li>[Parsing](/posts/2019-12-08-parsing-prolog-with-pidgin.html)</li>
        <li>**Unification**</li>
        <li>[The rules engine](/posts/2019-12-22-building-prologs-rules-engine.html)</li>
    </ol>
</nav>
<p>We're getting on to the fun part of writing an interpreter: actually <em>interpreting</em> the input code. In this post we're going to implement a <em>unification</em> algorithm using my generic programming library <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a>. Unification slots somewhere in the middle or bottom-right of this diagram:</p>
<img src="/images/2017-11-13-recursion-without-recursion/compiler.jpg" alt="Compiler overview" />
<h2 id="unification"><a href="#unification">Unification

</a></h2>
<p>Unification is a process for solving equations, much like the equations you'd solve in algebra at school. It's all about plugging holes — given a pair of terms with variables inside them, what subterms can I plug in for the variables to make the terms equal? Unifying two terms produces a <em>substitution</em> — a mapping from variables to terms. If you plug the terms in for the variables (on both sides of the equation), then the equation should be satisfied.</p>
<p>The algorithm works by examining the syntactic structure of the two terms. It tries to match up each node in one term's syntax tree with a corresponding node in the other. When you find yourself matching a term against a variable, you know that the variable must equal the term. Unification is a purely syntactic process — you can't apply domain-specific rules such as &quot;add a number to both sides&quot; like you can in algebra.</p>
<p>Let's work through some examples to get a feel for it.</p>
<ul>
<li>
<p>Unify <code>f(a, X, Y)</code> with <code>f(a, b, g(x))</code>.</p>
<ul>
<li>Starting at the outermost part of the two terms' syntax, we see <code>f</code> applied to three arguments in each of them — a match. Now we can match up the arguments and try to unify each of them.
</li>
<li>The first argument in each is the atom <code>a</code> — so they match.
</li>
<li>Unifying the second arguments gives us <code>X ~ b</code>. <code>X</code> is a variable, and we now know it must be equal to <code>b</code>, so that gets added to the output substitution.
</li>
<li>Unifying the third arguments, we find <code>Y ~ g(x)</code>, so that also goes in the output. It's OK to match a variable to a composite term.
</li>
<li>The resulting substitution is <strong><code>X := b; Y := g(x)</code></strong>. You can try plugging this substitution back in to the two terms to verify that it does indeed make them equal.
<div style="display:flex; align-items:center"><img src="/images/2019-12-15-generic-unification-with-sawmill/example1-1.png" width="47%" style="margin-right:3%" /><img src="/images/2019-12-15-generic-unification-with-sawmill/example1-2.png" width="47%" style="margin-left:3%" /></div>
</li>
</ul>
</li>
<li>
<p>Unify <code>f(X, g(X))</code> with <code>f(m(b), g(m(b)))</code>. In this example, the <code>X</code> variable appears twice in the left hand argument; these are the <strong>same</strong> <code>X</code> and at the end of unification they have to agree.</p>
<ul>
<li>The outermost <code>f</code>s match, so we descend to the arguments.
</li>
<li>The first argument gives us <code>X ~ m(b)</code>.
</li>
<li>Looking at the second argument, you see <code>g</code> applied to one argument in both cases. This is a match, so we can once more continue to compare the arguments.
</li>
<li>Comparing the arguments gives us <code>X ~ m(b)</code>. This is the same constraint as the one we got from the first argument.
</li>
<li>The resulting substitution is <strong><code>X := m(b)</code></strong>.
<div style="display:flex; align-items:center"><img src="/images/2019-12-15-generic-unification-with-sawmill/example2-1.png" width="47%" style="margin-right:3%" /><img src="/images/2019-12-15-generic-unification-with-sawmill/example2-2.png" width="47%" style="margin-left:3%" /></div>
</li>
</ul>
</li>
<li>
<p>Unify <code>f(g(X), a)</code> with <code>f(g(Y), X)</code>. In this example we see variables appearing on both sides. We have to come up with a value for all of the variables in both terms; once again the <code>X</code> appearing on both sides is the same <code>X</code>.</p>
<ul>
<li>Once more, the outermost <code>f</code>s match.
</li>
<li>Matching up the first arguments, we get <code>g(X) ~ g(Y)</code>, so <code>X ~ Y</code>. It's OK to match variables to each other.
</li>
<li>Matching up the first arguments gives us <code>a ~ X</code>.
</li>
<li>The resulting substitution is <strong><code>X := a; Y := X</code></strong> (or, equivalently, <strong><code>X := a; Y := a</code></strong>).
<div style="display:flex; align-items:center"><img src="/images/2019-12-15-generic-unification-with-sawmill/example3-1.png" width="47%" style="margin-right:3%" /><img src="/images/2019-12-15-generic-unification-with-sawmill/example3-2.png" width="47%" style="margin-left:3%" /></div>
</li>
</ul>
</li>
<li>
<p>A couple of exercises:</p>
<ul>
<li>Does <code>f(g(X), a)</code> unify with <code>f(g(b), X)</code>?
</li>
<li>Does <code>f(X, Y)</code> unify with <code>f(g(Y), Z)</code>?
</li>
<li>Does <code>f(X, Y)</code> unify with <code>f(Y, g(X))</code>?
</li>
<li>Try coming up with some interesting examples of terms which don't unify.
</li>
</ul>
</li>
</ul>
<p>Let's sketch out an algorithm for unification.</p>
<ol>
<li>Look at the outermost node in the two terms' ASTs.
</li>
<li>If either one of them is a variable, bind the variable to the term in the output substitution. If it's already bound in the output substitution, make sure that this binding doesn't conflict.
</li>
<li>If both are leaf nodes, check that they match.
</li>
<li>If both are composite terms, check that they match and that they have the same number of children. If so, repeat steps 1-3 for each pair of children.
</li>
<li>Otherwise, the terms don't match and we should abort unification.
</li>
</ol>
<p>Hopefully you can see how I've applied these steps in the examples above. I've glossed over some important details here (particularly regarding what happens in step 2 when a variable is already in the substitution) but I'll be sure to fill them in when we get to writing code. (In fact we're going to design our code so that a variable will never already be in the substitution.)</p>
<h2 id="iunifiable"><a href="#iunifiable"><code>IUnifiable</code>

</a></h2>
<p>Unification is a central part of Prolog's programming model, but it has applications outside of Prolog as well (type inference, for example). Any time you have a syntactic structure with placeholder variables, the question &quot;what can I plug in for these variables?&quot; makes sense.</p>
<p>Let's quantify that. Reading back over the description of the algorithm, it seems to build on a few core concepts:</p>
<ul>
<li>Terms
</li>
<li>Terms which are variables
</li>
<li>Composite terms with children
</li>
<li>Leaf nodes with no children
</li>
<li>Matching a pair of terms
</li>
</ul>
<p>Abstracting over these concepts should allow us to implement unification once and for all, without depending on the specifics of Prolog's terms. Don't solve one problem. Solve them all!</p>
<p>Sawmill's core <code>IRewritable</code> interface is a generic way of looking at tree-shaped terms with children. The only concepts we need to add on top of <code>IRewritable</code> are <em>variables</em> and <em>matching</em>.</p>
<pre><code class="language-csharp">interface IUnifiable&lt;T&gt; : IRewritable&lt;T&gt; where T : IUnifiable&lt;T&gt;
{
    bool Match(T other);
    string AsVariable();
}
</code></pre>
<p><code>Match</code> tells you whether the current node matches another node. In Prolog's case, it tests whether the two nodes have the same name and number of children. <code>AsVariable</code> checks whether the current node is a variable, returning its name as a string (or <code>null</code> if it's not). Here's what it looks like implemented on <a href="/posts/2019-12-01-write-you-a-prolog.html">the <code>Term</code> syntax tree</a>:</p>
<pre><code class="language-csharp">abstract class Term : IUnifiable&lt;Term&gt;
{
    // ...
    public abstract bool Match(Term other);
    public abstract string AsVariable();
}
class Predicate : Term
{
    // ...
    public override bool Match(Term other)
        =&gt; other is Predicate p
        &amp;&amp; p.Name == Name
        &amp;&amp; p.Args.Length == Args.Length;
    public override string AsVariable() =&gt; null;
}
class Variable : Term
{
    // ...
    public override bool Match(Term other)
        =&gt; other is Variable v
        &amp;&amp; v.Name == Name;
    public override string AsVariable() =&gt; Name;
}
class Atom : Term
{
    // ...
    public override bool Match(Term other)
        =&gt; other is Atom a
        &amp;&amp; a.Value == Value;
    public override string AsVariable() =&gt; null;
}
</code></pre>
<p>For example, here's a LINQ query which finds all of the variables mentioned anywhere in a term. It uses <code>SelfAndDescendants</code> together with <code>AsVariable</code> to find the nodes which were in fact variables.</p>
<pre><code class="language-csharp">public static ImmutableHashSet&lt;string&gt; Variables&lt;T&gt;(this T value) where T : IUnifiable&lt;T&gt;
    =&gt; value
        .SelfAndDescendants()
        .Select(x =&gt; x.AsVariable())
        .Where(name =&gt; name != null)
        .Distinct()
        .ToImmutableHashSet();
</code></pre>
<h2 id="substitutions"><a href="#substitutions">Substitutions

</a></h2>
<p>I mentioned that the output of unification is a <em>substitution</em> — a mapping from variable names to terms. We'll model that using an ordinary dictionary of strings and <code>T</code>s. Our signature for <code>Unify</code> is going to look like this:</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Unify&lt;T&gt;(this T left, T right) where T : IUnifiable&lt;T&gt;
</code></pre>
<p>The main thing you can do with a substitution is <em>apply</em> it to a term. That means replacing all the variables in the term with the sub-terms they're mapped to by the substitution. This should eliminate any variables in the term which are bound by the substitution.</p>
<pre><code class="language-csharp">public static T Apply&lt;T&gt;(this ImmutableDictionary&lt;string, T&gt; subst, T value) where T : IUnifiable&lt;T&gt;
    =&gt; value.Rewrite(
        x =&gt;
        {
            var name = x.AsVariable();
            if (name != null)
            {
                return subst.GetValueOrDefault(name, x);
            }
            return x;
        }
    );
</code></pre>
<p>We're going to maintain the invariant that the terms in a substitution are never going to contain any variables which are bound by the substitution. (It's possible, and indeed desirable for efficiency, to relax this invariant, but today we're going to focus on simplicity.)</p>
<h2 id="binding-single-variables--the-occurs-check"><a href="#binding-single-variables--the-occurs-check">Binding Single Variables &amp; the Occurs Check

</a></h2>
<p>Time to fill in the body of <code>Unify</code>. Let's start with step 2 of the algorithm: what to do when either of the input terms is a variable.</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Unify&lt;T&gt;(this T left, T right) where T : IUnifiable&lt;T&gt;
{
    var leftName = left.AsVariable();
    if (leftName != null)  // then the left term was a variable
    {
        return Bind(leftName, right);
    }
    var rightName = right.AsVariable();
    if (rightName != null)  // then the right term was a variable
    {
        return Bind(rightName, left);
    }
    // ...
}
</code></pre>
<p><code>Bind(name, value)</code> returns a substitution with the variable <code>name</code> bound to the term <code>value</code>. However! There are some important edge cases to watch out for which could result in breaking the invariant I mentioned above.</p>
<ol>
<li>The term <code>value</code> could be the <em>same</em> variable as <code>name</code>. This would produce a substitution like <code>X := X</code> (that is, a do-nothing substitution), so we'll just return an empty substitution in that case.
</li>
<li>The term <code>value</code> could mention the variable <code>name</code> somewhere inside it. This would produce a substitution like <code>X := foo(X)</code>. If you try and find a concrete value for <code>X</code> you'd end up with an infinitely long chain of <code>foo</code>s! Some systems allow infinite terms like this, but others disallow it by checking whether a variable occurs in the term it's being bound to. This check is suitably called the <em>occurs check</em>.
</li>
</ol>
<pre><code class="language-csharp">private static ImmutableDictionary&lt;string, T&gt; Bind&lt;T&gt;(string name, T value) where T : IUnifiable&lt;T&gt;
{
    var valueName = value.AsVariable();
    if (valueName != null &amp;&amp; name == valueName)
    {
        return ImmutableDictionary&lt;string, T&gt;.Empty;
    }
    if (value.Variables().Contains(name))
    {
        throw new UnificationError(&quot;occurs check&quot;);
    }
    return ImmutableDictionary&lt;string, T&gt;.Empty.Add(name, value);
}
</code></pre>
<h2 id="handling-non-variables"><a href="#handling-non-variables">Handling Non-Variables

</a></h2>
<p>Let's continue implementing <code>Unify</code>. At this point we know that neither <code>left</code> nor <code>right</code> are variables, so they must be either composite terms or leaf nodes. In either case, we need to check whether they match.</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Unify&lt;T&gt;(this T left, T right) where T : IUnifiable&lt;T&gt;
{
    // ...
    if (!left.Match(right))
    {
        throw new UnificationError(&quot;values didn't match&quot;);
    }
    // ...
}
</code></pre>
<p>If they did match, it's time to try matching up their children. (If the two nodes were leaf nodes, their list of children is empty, so the following code does nothing.)</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Unify&lt;T&gt;(this T left, T right) where T : IUnifiable&lt;T&gt;
{
    // ...
    var leftChildren = left.GetChildren();
    var rightChildren = right.GetChildren();
    return Solve(leftChildren, rightChildren);
}
</code></pre>
<h2 id="unifying-children--systems-of-equations"><a href="#unifying-children--systems-of-equations">Unifying Children — Systems of Equations

</a></h2>
<p>Unifying the children means solving a <em>system</em> of equations. If more than one equation mentions the same variable, they have to agree on what that variable should be.</p>
<p>The plain is to pair up the children in the two lists and try to unify each with their partner, building up a substitution as we go.</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Solve&lt;T&gt;(T[] left, T[] right) where T : IUnifiable&lt;T&gt;
{
    if (left.Length != right.Length)
    {
        throw new UnificationError(&quot;values didn't match&quot;);
    }
    var subst = ImmutableDictionary&lt;string, T&gt;.Empty;
    foreach (var (l, r) in left.Zip(right))
    {
        // ...
    }
    return subst;
}
</code></pre>
<p>This loop requires some thinking. As you unify each pair of children, you learn something about the variables in the system of equations. These learnings need to be propagated to the other equations. So as we build our substitution we need to make sure to apply it to the remaining equations in the system. This propagates information from left to right, ensuring that you don't encounter variables which have already been bound by earlier equations.</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Solve&lt;T&gt;(T[] left, T[] right) where T : IUnifiable&lt;T&gt;
{
    // ...
    foreach (var (l, r) in left.Zip(right))
    {
        var newL = subst.Apply(l);
        var newR = subst.Apply(r);
        var newSubst = newL.Unify(newR);
        // ...
    }
    // ...
}
</code></pre>
<p>But we also need to propagate information from right to left. When you unify <code>f(X, Y)</code> with <code>f(Y, a)</code>, the first argument sets <code>X := Y</code>, but the second argument sets <code>Y := a</code>. Simply combining the two substitutions results in <code>X := Y; Y := a</code> which breaks our invariant that bound variables don't appear inside a substitution. When you combine two substitutions, you need to <code>Apply</code> the second substitution to the terms inside the first one.</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Solve&lt;T&gt;(T[] left, T[] right) where T : IUnifiable&lt;T&gt;
{
    // ...
    foreach (var (l, r) in left.Zip(right))
    {
        // ...
        subst = subst.Compose(newSubst);
    }
    // ...
}
public static ImmutableDictionary&lt;string, T&gt; Compose&lt;T&gt;(
    this ImmutableDictionary&lt;string, T&gt; left,
    ImmutableDictionary&lt;string, T&gt; right
) where T : IUnifiable&lt;T&gt;
    =&gt; left
        .Select(kvp =&gt; new KeyValuePair&lt;string, T&gt;(kvp.Key, right.Apply(kvp.Value)))
        .Where(kvp =&gt; !(kvp.Value is Variable v &amp;&amp; v.Name == kvp.Key))
        .Concat(right)
        .ToImmutableDictionary();
</code></pre>
<p>Note <code>right.Apply(kvp.Value)</code> inside the <code>Select</code>. This can sometimes result in substitution entries of the form <code>X := X</code> (for example when unifying <code>f(X, Y)</code> with <code>f(Y, X)</code>), so I'm just filtering them out with <code>Where</code>.</p>
<p>That's the whole algorithm! I'll repeat <code>Unify</code> and <code>Solve</code> in full:</p>
<pre><code class="language-csharp">public static ImmutableDictionary&lt;string, T&gt; Unify&lt;T&gt;(this T left, T right) where T : IUnifiable&lt;T&gt;
{
    var leftName = left.AsVariable();
    if (leftName != null)
    {
        return Bind(leftName, right);
    }
    var rightName = right.AsVariable();
    if (rightName != null)
    {
        return Bind(rightName, left);
    }
    if (!left.Match(right))
    {
        throw new UnificationError(&quot;values didn't match&quot;);
    }
    var leftChildren = left.GetChildren();
    var rightChildren = right.GetChildren();
    return Solve(leftChildren, rightChildren);
}
public static ImmutableDictionary&lt;string, T&gt; Solve&lt;T&gt;(T[] left, T[] right) where T : IUnifiable&lt;T&gt;
{
    if (left.Length != right.Length)
    {
        throw new UnificationError(&quot;values didn't match&quot;);
    }
    var subst = ImmutableDictionary&lt;string, T&gt;.Empty;
    foreach (var (l, r) in left.Zip(right))
    {
        var newL = subst.Apply(l);
        var newR = subst.Apply(r);
        var newSubst = newL.Unify(newR);
        subst = subst.Compose(newSubst);
    }
    return subst;
}
</code></pre>
<p>A quick test of one of our examples:</p>
<pre><code class="language-csharp">static void Main(string[] args)
{
    Term left = PrologParser.ParseQuery(&quot;f(g(X), a)&quot;);
    Term right = PrologParser.ParseQuery(&quot;f(g(Y), X)&quot;);

    var subst = left.Unify(right);

    Console.WriteLine(string.Join(&quot;\n&quot;, subst.Select(kvp =&gt; kvp.Key + &quot; := &quot; + kvp.Value)));
    // prints out:
    // X := a
    // Y := a

    var newLeft = subst.Apply(left);
    var newRight = subst.Apply(right);

    Console.WriteLine(newLeft);
    Console.WriteLine(newRight);
    // both print out f(g(a), a)
}
</code></pre>
<p>Some exercises you could try:</p>
<ul>
<li>This implementation does a lot of <code>Apply</code>ing substitutions to terms, specifically in <code>Solve</code> and <code>Compose</code>.
<ul>
<li>Since <code>Rewrite</code> is linear in the size of the tree (it visits every node), what's the complexity of <code>Unify</code>?
</li>
<li>Can you make it more efficient?
<ul>
<li>Hint 1: you need to relax the invariant that substitutions can't contain bound variables.
</li>
<li>Hint 2: you'll need to tweak <code>Bind</code> to avoid <a href="https://norvig.com/unify-bug.pdf">a common bug</a>.
</li>
<li>Hint 3: <code>Apply</code>ing a substitution to a term will no longer eliminate all the bound variables inside the term. Try using <a href="https://www.benjamin.pizza/Sawmill/v3.1.0/api/Sawmill.Rewritable.html#Sawmill_Rewritable_RewriteIter__1___0_System_Func___0___0__"><code>RewriteIter</code></a> to reestablish that invariant.
</li>
</ul>
</li>
</ul>
</li>
<li><code>IUnifiable</code> assumes that variables can always be represented by strings. Many practical programming language implementations use richer data structures than strings (such as variables tagged with their scope) to represent variables. Can you change this design to break the dependency on strings? What API usability issues do you encounter?
</li>
</ul>
<p>All of this code can be found in <a href="https://github.com/benjamin-hodgson/Amateurlog">the example repo</a>, in the <a href="https://github.com/benjamin-hodgson/Amateurlog/blob/master/Unification.cs"><code>Unification.cs</code></a> and <a href="https://github.com/benjamin-hodgson/Amateurlog/blob/master/Syntax.Unification.cs"><code>Syntax.Unification.cs</code></a> files. In the final part of this series, we'll apply unification to build Prolog's rules engine.</p>

]]></summary>
</entry>
<entry>
    <title>Parsing Prolog with Pidgin</title>
    <link href="http://www.benjamin.pizza/posts/2019-12-08-parsing-prolog-with-pidgin.html" />
    <id>http://www.benjamin.pizza/posts/2019-12-08-parsing-prolog-with-pidgin.html</id>
    <published>2019-12-08T00:00:00Z</published>
    <updated>2019-12-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>Happy birthday to my sister! This is part of a series of posts about implementing a miniature Prolog interpreter in C#.</p>
<nav>
    <ol>
        <li>[Introduction & Syntax](/posts/2019-12-01-write-you-a-prolog.html)</li>
        <li>**Parsing**</li>
        <li>[Unification](/posts/2019-12-15-generic-unification-with-sawmill.html)</li>
        <li>[The rules engine](/posts/2019-12-22-building-prologs-rules-engine.html)</li>
    </ol>
</nav>
<img src="/images/2017-11-13-recursion-without-recursion/compiler.jpg" alt="Compiler overview" />
<p>In this post I'm going to focus on the left-hand part of that diagram. We'll use my parsing library <a href="https://github.com/benjamin-hodgson/Pidgin">Pidgin</a> to convert Prolog source code into the abstract syntax classes I outlined in the previous post.</p>
<h2 id="about-pidgin"><a href="#about-pidgin">About Pidgin

</a></h2>
<p>Pidgin is a <em>parser combinator library</em>, meaning it consists of three things:</p>
<ol>
<li>A type <code>Parser&lt;TToken, T&gt;</code>, representing a process which consumes a sequence of <code>TToken</code>s and produces a <code>T</code>. (In our case, <code>TToken</code> will be <code>char</code> because we're parsing textual data from a string.)
</li>
<li>Methods to create simple <code>Parser</code>s, such as &quot;consume a single character&quot;.
</li>
<li>Methods to combine <code>Parser</code>s into more complex and interesting ones, such as &quot;run two parsers sequentially&quot;.
</li>
</ol>
<p>The power of parser combinators comes from the fact that parsers are treated as ordinary values. You can pass them around as parameters, return them, store them in fields... Programming with parser combinators is <em>just programming</em>. Contrast this with &quot;language workbench&quot; tools like Antlr, which are often more powerful than parser combinators but have a separate syntax, toolchain, etc. Parser combinators are great for medium-sized parsing tasks like this one.</p>
<p>It's worth elaborating a bit on what I mean by &quot;consume a sequence&quot;. A parser walks left to right along an input sequence, keeping track of its location in the input. Consuming a token means moving the &quot;current location&quot; pointer along to the next token. This is much like how the <code>Stream</code> classes work in .NET — calling a stream's <code>Read</code> method moves the stream's current position, so that successive calls to <code>Read</code> return different results. (Pidgin indeed lets you feed a <code>Stream</code> to a <code>Parser</code>.)</p>
<p>When a parser makes a decision about what to do next, that decision is based only on the current character and not on what comes next. If the parser moves further along the string and then realises that decision was wrong, it has to <em>backtrack</em> to the position where it made the decision before it can proceed with a different choice. Backtracking can be catastrophically costly (as described in <a href="https://vimeo.com/112065252">a short talk by my esteemed ex-colleague Balpha</a>), so you want to code your parser to minimise the amount of backtracking it might have to do. With Pidgin, backtracking is disabled by default; it's enabled by the <code>Try</code> function.</p>
<h2 id="handling-tokens-and-whitespace"><a href="#handling-tokens-and-whitespace">Handling Tokens and Whitespace

</a></h2>
<p>Let's start with the basics and build upwards. Prolog is a <em>whitespace-insensitive</em> language, so we need a way to skip over the whitespace in the source code. We'll use a convention where each component <code>Parser</code> will consume any whitespace <em>after</em> parsing any relevant non-whitespace characters.</p>
<p>Here's a method which creates a <code>Parser</code> which consumes a specific character, skips over any whitespace, and then returns the original character.</p>
<pre><code class="language-csharp">static Parser&lt;char, char&gt; Tok(char value)  // for &quot;token&quot;
    =&gt; Char(value).Before(SkipWhitespaces);
</code></pre>
<p>(<code>Char</code> comes from the <code>Pidgin.Parser</code> static class; all of today's code is in a file with <code>using static Pidgin.Parser</code> and <code>using static Pidgin.Parser&lt;char&gt;</code> at the top.) <code>Char</code> is a parser which matches a specific character; <code>SkipWhitespaces</code> greedily consumes and discards a sequence of whitespace characters; and <code>Before</code> glues two parsers together sequentially, keeping only the result of the first one. So <code>Tok('(')</code> will match the input strings <code>&quot;(&quot;</code> and <code>&quot;(    &quot;</code> but not <code>&quot; (&quot;</code> or <code>&quot;)&quot;</code>.</p>
<p>Here's an equivalent parser for a specific string:</p>
<pre><code class="language-csharp">static Parser&lt;char, string&gt; Tok(string value)
    =&gt; Try(String(value)).Before(SkipWhitespaces);
</code></pre>
<p>Why am I using the <em>backtracking</em> function <code>Try</code> here? Remember, a parser consumes its input one character at a time, and makes decisions based only on the current character. If the first character of the input looks like it matches <code>value</code>, Pidgin will commit to running the <code>String(value)</code> parser; if matching the string fails after the first character then the parser needs to return to where it was so that it can try any alternative ways to parse from that location.</p>
<p>For an example of why we need <code>Try</code>, consider the parser <code>String(&quot;prolog&quot;).Or(String(&quot;programming&quot;))</code>. <code>Or</code> is Pidgin's <em>choice</em> function — it attempts to run one parser, and falls back on the other one if the first parser failed without consuming any input. Here's our parser represented pictorally:</p>
<pre><code>             +--Or--+
             |      |
String(&quot;prolog&quot;)  String(&quot;programming&quot;)
</code></pre>
<p>Let's think about how this parser behaves when you feed it the string <code>&quot;programming&quot;</code>. Naïvely we might expect this to succeed, because <code>String(&quot;programming&quot;)</code> is one of <code>Or</code>'s options. But in fact this parser will fail to parse the string <code>&quot;programming&quot;</code>. Looking at it step by step reveals why:</p>
<p>At the start of the parsing process, the current character is the first letter, namely <code>p</code>.</p>
<pre>
<b>p</b> r o g r a m m i n g
</pre>
<p>Our <code>Or</code> parser will try to apply the <code>prolog</code> parser first. The <code>prolog</code> parser looks at the current character and says &quot;This looks like the start of <code>prolog</code> to me. I'll consume it.&quot;</p>
<pre>
p <b>r</b> o g r a m m i n g
</pre>
<p>This continues for a couple more characters.</p>
<pre>
p r <b>o</b> g r a m m i n g
p r o <b>g</b> r a m m i n g
</pre>
<p>Now that we're looking at a <code>g</code>, it's clear that the input doesn't match the string <code>prolog</code>. The <code>prolog</code> parser fails and yields control back to <code>Or</code>. Because the <code>prolog</code> parser consumed input and did not backtrack, <code>Or</code> will not attempt to apply the <code>programming</code> parser. (If it did try, it would fail anyway because we're no longer looking at a <code>p</code>.) If the <code>prolog</code> parser were wrapped in a <code>Try</code>, it would have backtracked to the <code>p</code> upon failing, allowing <code>Or</code> to fall back on the <code>programming</code> parser.</p>
<p>It's common to use <code>Try</code> for each word and symbol in your language's grammar; I'm going to use <code>Tok</code> for each of my low level component parsers.</p>
<p>Finally, we can generalise these <code>Tok</code> methods to run an arbitrary <code>Parser</code> with backtracking and whitespace.</p>
<pre><code class="language-csharp">static Parser&lt;char, T&gt; Tok&lt;T&gt;(Parser&lt;char, T&gt; p)
    =&gt; Try(p).Before(SkipWhitespaces);

static Parser&lt;char, char&gt; Tok(char value) =&gt; Tok(Char(value));
static Parser&lt;char, string&gt; Tok(string value) =&gt; Tok(String(value));
</code></pre>
<p>I have a half-baked plan to add this <code>Tok</code> parser, along with some other tools for handling whitespace, to the Pidgin library itself, so you don't have to write it yourself every time you write a parser.</p>
<p>Some useful token parsers for Prolog:</p>
<pre><code class="language-csharp">static readonly Parser&lt;char, char&gt; _comma = Tok(',');
static readonly Parser&lt;char, char&gt; _openParen = Tok('(');
static readonly Parser&lt;char, char&gt; _closeParen = Tok(')');
static readonly Parser&lt;char, char&gt; _dot = Tok('.');
static readonly Parser&lt;char, string&gt; _colonDash = Tok(&quot;:-&quot;);
</code></pre>
<h2 id="names"><a href="#names">Names

</a></h2>
<p>Prolog has two types of names — <em>atoms</em> start with a lowercase letter and <em>variables</em> start with an uppercase letter or an underscore.</p>
<pre><code class="language-csharp">static readonly Parser&lt;char, string&gt; _atomName = Tok(
    from first in Lowercase
    // OneOf is like Or, but it works with more than two parsers
    from rest in OneOf(Letter, Digit, Char('_')).ManyString()
    select first + rest
);
static readonly Parser&lt;char, string&gt; _variableName = Tok(
    from first in Uppercase.Or(Char('_'))
    from rest in OneOf(Letter, Digit, Char('_')).ManyString()
    select first + rest
);
</code></pre>
<p>One of the fun things about C#'s <code>from...select</code> syntax is that it's <a href="https://en.wikipedia.org/wiki/Duck_typing"><em>duck-typed</em></a>. The C# compiler allows you to use <code>from...select</code> with any object which has eligible <code>Select</code> and <code>SelectMany</code> methods, not just <code>IEnumerable</code>. Pidgin allows you to (ab)use this notation to sequence parsers — <code>from x in p1 from y in p2 select f(x, y)</code> is equivalent to <code>p1.Then(p2, (x,y) =&gt; f(x,y))</code>. So when you see a parser defined using <code>from...select</code> you should read it from top to bottom as a script.</p>
<p><code>ManyString</code> takes a parser and greedily runs it in a loop, then packs all of that parser's results into a string. <code>Uppercase</code>, <code>Lowercase</code>, <code>Letter</code> and <code>Digit</code> all consume and return a single character of their respective types.</p>
<p>So, taken as a whole, <code>_atomName</code> consumes and returns a string as long as it's a valid Prolog atom. Here's how it works:</p>
<ol>
<li>Consume a lowercase character and name it <code>first</code>
</li>
<li>Consume as many letters, digits and underscores as possible, naming the resulting string <code>rest</code>
</li>
<li>Skip any whitespace coming after the name (that's what <code>Tok</code> does)
</li>
<li>Return the string <code>first + rest</code>
</li>
</ol>
<p><code>_variableName</code> works just the same, except in step 1 it consumes an uppercase letter or an underscore. Let's de-duplicate them:</p>
<pre><code class="language-csharp">static Parser&lt;char, string&gt; Name(Parser&lt;char, char&gt; firstLetter)
    =&gt; Tok(
        from first in firstLetter
        from rest in OneOf(Letter, Digit, Char('_')).ManyString()
        select first + rest
    );
</code></pre>
<h2 id="terms"><a href="#terms">Terms

</a></h2>
<p>Terms in Prolog have a recursive structure, as <a href="/posts/2019-12-01-write-you-a-prolog.html">discussed previously</a>. A predicate's arguments can be any term, including another predicate. This circularity poses a problem for our parser code — the <code>_predicate</code> parser needs to call <code>_term</code>, but <code>_term</code> needs to call <code>_predicate</code>, so what order can you put the declarations in?</p>
<p>Pidgin's built-in <code>Rec</code> function enables forward references like this. The idea is to use a lambda to delay the access to the field until after it's been initialised.</p>
<pre><code class="language-csharp">static readonly Parser&lt;char, Term&gt; _term = Rec(() =&gt; OneOf(
    _variable,
    // upcast the Parser&lt;char, Predicate&gt; into a Parser&lt;char, Term&gt;
    _predicate.Cast&lt;Term&gt;(),
    _atom
)).Labelled(&quot;term&quot;);
</code></pre>
<p><code>_predicate</code> is textually below <code>_term</code>, so when <code>_term</code> is initialised <code>_predicate</code> will be <code>null</code>. By putting the <code>_predicate</code> reference inside a lambda and passing it to <code>Rec</code>, we can prevent <code>_predicate</code> from being accessed until after it's been initialised. (<code>Rec</code> returns a parser which calls the lambda the first time it's run.)</p>
<pre><code class="language-csharp">static readonly Parser&lt;char, Term&gt; _atom
    = Name(Lowercase)
        .Select(name =&gt; (Term)new Atom(name))
        .Labelled(&quot;atom&quot;);

static readonly Parser&lt;char, Term&gt; _variable
    = Name(Uppercase.Or(Char('_')))
        .Select(name =&gt; (Term)new Variable(name))
        .Labelled(&quot;variable&quot;);

static readonly Parser&lt;char, Predicate&gt; _predicate = (
    from name in Try(Name(Lowercase).Before(_openParen))
    from args in CommaSeparated(_term).Before(_closeParen)
    select new Predicate(name, args)
).Labelled(&quot;predicate&quot;);

static Parser&lt;char, ImmutableArray&lt;T&gt;&gt; CommaSeparated&lt;T&gt;(Parser&lt;char, T&gt; p)
    =&gt; p.Separated(_comma).Select(x =&gt; x.ToImmutableArray());
</code></pre>
<p>I'm using <code>Try</code> again in the <code>_predicate</code> parser. This is to disambiguate predicates from atoms. Both start in the same way (a lowercase letter) and you can't be sure you're parsing a predicate until you see a parenthesis. So our <code>_term</code> parser first attempts to read a predicate, but if it doesn't see a parenthesis then it backtracks and reads the name as an atom instead. The order of <code>OneOf</code>'s arguments matters!</p>
<h2 id="rules-queries-and-whole-programs"><a href="#rules-queries-and-whole-programs">Rules, Queries and Whole Programs

</a></h2>
<p>We're on the home straight. A Prolog program consists of a collection of top-level <em>rules</em> and <em>facts</em>. A rule consists of a predicate followed by the symbol <code>:-</code> and a comma-separated list of predicates, and ends with a <code>.</code> symbol. A fact is just a rule with no right-hand side.</p>
<pre><code class="language-csharp">static readonly Parser&lt;char, Rule&gt; _rule
    = Map(
        (head, body) =&gt; new Rule(head, body),
        _predicate,
        _colonDash
            .Then(CommaSeparatedAtLeastOnce(_predicate))
            .Or(Return(ImmutableArray&lt;Predicate&gt;.Empty))
    )
    .Before(_dot)
    .Labelled(&quot;rule&quot;);

static Parser&lt;char, ImmutableArray&lt;T&gt;&gt; CommaSeparatedAtLeastOnce&lt;T&gt;(Parser&lt;char, T&gt; p)
    =&gt; p.SeparatedAtLeastOnce(_comma).Select(x =&gt; x.ToImmutableArray());
</code></pre>
<p>(<code>Map</code> is another way of writing <code>Then</code>; <code>Return</code> is a parser which always returns the given value without touching the input string.) Note that <code>.Or(Return(...))</code> is applied to the result of <code>_colonDash.Then(...)</code>. If the parser encounters the <code>:-</code> symbol then it commits to reading at least one item in the rule's body. <code>name(args) :- .</code> is not valid Prolog; if a programmer wants a rule with no body then they have to omit the <code>:-</code>.</p>
<p>Finally, a Prolog program consists of a sequence of rules. We're also going to need a parser for individual queries typed at the interactive prompt.</p>
<pre><code class="language-csharp">static readonly Parser&lt;char, ImmutableArray&lt;Rule&gt;&gt; _program =
    from _ in SkipWhitespaces
    from rules in _rule.Many()
    select rules.ToImmutableArray();

static readonly Parser&lt;char, Predicate&gt; _query = SkipWhitespaces.Then(_predicate);

public static ImmutableArray&lt;Rule&gt; ParseProgram(string input) = _program.ParseOrThrow(input);
public static Predicate ParseQuery(string input) = _query.ParseOrThrow(input);
</code></pre>
<p>Since by convention each of our component parsers has been consuming whitespaces <em>after</em> the text they match, we need to remember to <code>SkipWhitespaces</code> at the start of the file.</p>
<p>That's our whole parser! Less than 80 lines of code is not bad, I think. A quick test of our <code>last</code> example from the previous post:</p>
<pre><code class="language-csharp">static void Main(string[] args)
{
    var program = PrologParser.ParseProgram(@&quot;
last(cons(X, nil), X).
last(cons(X, Xs), Y) :- last(Xs, Y).
&quot;);

    Console.WriteLine(string.Join(&quot;\n&quot;, program));
    // prints out:
    // last(cons(X, nil), X).
    // last(cons(X, Xs), Y) :- last(Xs, Y).
}
</code></pre>
<p>Here are a couple of exercises you might try:</p>
<ul>
<li>Extend this code to support numbers, building on the exercise from the end of the <a href="/posts/2019-12-01-write-you-a-prolog.html">last post</a>.
</li>
<li>Extend this code to support lists.
<ul>
<li>Prolog's lists are linked lists; <code>[]</code> is an empty list and cons cells look like <code>[head | tail]</code>. (<code>head</code> and <code>tail</code> are both arbitrary terms; though at runtime <code>tail</code> should be a list.)
</li>
<li>At first you can try pretending that lists are syntactic sugar — desugar <code>[]</code> to a <code>nil</code> atom and <code>[head | tail]</code> to a <code>cons(head, tail)</code> predicate.
</li>
<li>How can you avoid clashing with mentions of those names in user code?
<ul>
<li>You could try mangling the names to something which can't be typed by a user.
</li>
<li>You could try making them reserved words (adjust the parser to reject user-defined mentions of <code>cons</code> and <code>nil</code>).
</li>
<li>You could try adding lists directly to the <code>Term</code> AST.
</li>
</ul>
</li>
</ul>
</li>
<li>Try using <a href="https://www.benjamin.pizza/Pidgin/v2.2.0/api/Pidgin.Comment.CommentParser.html">Pidgin's <code>CommentParser</code> class</a> to handle Prolog code with comments in.
</li>
</ul>
<p>You can find this code in <a href="https://github.com/benjamin-hodgson/Amateurlog">the example repo</a>, in the file <a href="https://github.com/benjamin-hodgson/Prolog/blob/master/Parser.cs"><code>Parser.cs</code></a>. Next time we'll talk about <em>unification</em>, the core of Prolog's bi-directional programming model. We'll be using <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a> to implement unification <em>generically</em>, without making any assumptions about Prolog's syntax.</p>

]]></summary>
</entry>
<entry>
    <title>Write You a Prolog</title>
    <link href="http://www.benjamin.pizza/posts/2019-12-01-write-you-a-prolog.html" />
    <id>http://www.benjamin.pizza/posts/2019-12-01-write-you-a-prolog.html</id>
    <published>2019-12-01T00:00:00Z</published>
    <updated>2019-12-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>I figured it'd be useful to have some examples of my language tooling libraries <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a> and <a href="https://github.com/benjamin-hodgson/Pidgin">Pidgin</a> in action. I thought it could be fun to use them to write a miniature <a href="https://en.wikipedia.org/wiki/Prolog">Prolog</a> interpreter!</p>
<nav>
    <ol>
        <li>**Introduction & Syntax**</li>
        <li>[Parsing](/posts/2019-12-08-parsing-prolog-with-pidgin.html)</li>
        <li>[Unification](/posts/2019-12-15-generic-unification-with-sawmill.html)</li>
        <li>[The rules engine](/posts/2019-12-22-building-prologs-rules-engine.html)</li>
    </ol>
</nav>
<p>You'll find all the code for this series <a href="https://github.com/benjamin-hodgson/Amateurlog">on GitHub</a>.</p>
<h2 id="whistle-stop-introduction-to-prolog"><a href="#whistle-stop-introduction-to-prolog">Whistle-Stop Introduction to Prolog

</a></h2>
<p>Prolog is a <em>logic programming</em> language. Prolog code consists of a collection of <em>rules</em>. Each rule says &quot;X is true if Y (and Z and...) is true&quot;. A rule is a logical axiom, with a set of premises (on the right) and a conclusion you can draw from those premises (on the left).</p>
<p>As an example, you might say that a <code>Person</code> wants a certain <code>Food</code> if they're hungry and they like that food. And — I don't know about you — but I'll eat something I really love even if I'm not hungry.</p>
<pre><code class="language-prolog">wants(Person, Food) :- hungry(Person), likes(Person, Food).
wants(Person, Food) :- loves(Person, Food).

% If someone loves a given food, then they also like it.
likes(Person, Food) :- loves(Person, Food).

% If we're going to have dinner, we'd better agree on what to eat.
dinner(Person1, Person2, Food) :- wants(Person1, Food), wants(Person2, Food).
</code></pre>
<p>Variables in Prolog start with a capital letter. <code>:-</code> means &quot;if&quot; and <code>,</code> means &quot;and&quot;. You can give multiple alternative ways of satisfying the same predicate by just declaring it more than once. In other words, multiple declarations of the same predicate means &quot;or&quot;.</p>
<p>Next, we'll prime Prolog's database with a couple of <em>facts</em> about people and foods.</p>
<pre><code class="language-prolog">loves(benjamin, pizza).
loves(benjamin, asparagus).
likes(benjamin, soup).

loves(clio, salad).
likes(clio, pizza).
likes(clio, soup).
hungry(clio).
</code></pre>
<p><em>Atoms</em> in Prolog are somewhat like strings. They begin with a lower case letter.</p>
<p>Finally, we can issue a <em>query</em> to the interactive Prolog interpreter to find out whether we can have soup for dinner.</p>
<pre><code class="language-prolog">?- dinner(benjamin, clio, soup).
false
</code></pre>
<p>No soup for you. What <em>can</em> we eat for dinner? If we replace the atom <code>soup</code> — a specific food — with a variable <code>Food</code> (note the capital letter) — standing in for any food — Prolog will try to find a value for <code>Food</code> which satisfies the <code>dinner</code> predicate. (You can use as many variables as you like in a query. Prolog will try to find a value for all of them.)</p>
<pre><code class="language-prolog">?- dinner(benjamin, clio, Food).
Food = pizza
</code></pre>
<p>Even though predicates don't return anything per se — they either succeed or fail — you can use variables in this way to get information out of a predicate. Prolog's constraint solving system is <strong>bi-directional</strong> — a predicate's parameters can serve as both inputs and outputs.</p>
<h3 id="pattern-matching-and-recursion">[Pattern Matching and Recursion](#pattern-matching-and-recursion</h3>
<p>As well as putting <em>conditions</em> on the right-hand side of a rule, you can put <em>patterns</em> on the left. This is somewhat like pattern matching in functional languages — the right-hand side of a rule is only entered if its arguments match the pattern on the left.</p>
<p>Here's a recursive predicate <code>last(List, Item)</code> which succeeds when <code>Item</code> is the last element of <code>List</code>.</p>
<pre><code class="language-prolog">last(cons(X, nil), X).
last(cons(X, Xs), Y) :- last(Xs, Y).
</code></pre>
<p><code>last</code> is another example of bi-directionality. You can use the second parameter as an input by passing in a value (in which case <code>last</code> will test whether <code>Item</code> is the last element of <code>List</code>), or you can use it as an output by passing in a variable (in which case Prolog will find the last element of <code>List</code> and set <code>Item</code> equal to it).</p>
<p>When running a predicate, Prolog tries each of its clauses from top to bottom to see if any of them succeed. So let's review this code line by line.</p>
<p>The first rule has no right hand side, which means it succeeds as long as the predicate's arguments match the pattern on the left. (Rules with no right-hand side are called <em>facts</em>.) The first argument on the left hand side is the pattern <code>cons(X, nil)</code>. <code>cons</code> is a predicate whose two arguments are the head and tail of a list (<a href="https://en.wikipedia.org/wiki/Cons">the <code>cons</code> nomenclature</a> comes from Lisp); in this instance its second argument is the atom <code>nil</code> (representing an empty list) and its first argument is left indeterminate — <code>X</code> can stand in for any element. The second argument of <code>last</code> is <code>X</code>. This is the <em>same</em> <code>X</code> as appeared in <code>cons(X, nil)</code>. So, taken all together, this first line succeeds when its first argument is a single-element list and its second argument is that element.</p>
<p>The second line of this code is only entered when the first line fails — that is, when <code>Xs</code> is not <code>nil</code>. In the pattern on the left, we've replaced the concrete empty list <code>nil</code> with a variable <code>Xs</code> which can stand in for any list. The right-hand side of the rule recursively calls <code>last</code>. So this line says the last item of the list <code>cons(X, Xs)</code> is <code>Y</code> when the last item of <code>Xs</code> is <code>Y</code>.</p>
<p>Prolog's bi-directional pattern matching system works by <em>unification</em>. Unification is a process of making two terms equal by filling in their variables. When matching a goal like <code>last(cons(oranges, nil), Y)</code> to a rule head like <code>last(cons(X, nil), X)</code>, Prolog tries to find values to plug in for all the variables in scope so that the goal matches the rule. In this case, it'd determine that <code>X</code> and <code>Y</code> should both be <code>oranges</code>. I'll talk about unification in much more detail in a later post.</p>
<p>It's instructive to work through an example query: <code>last(cons(apples, cons(oranges, nil)), oranges)</code>.</p>
<ol>
<li>Prolog first tries the top clause. It tries to unify the goal with <code>last(cons(X, nil), X)</code>. This fails because there's no value for <code>X</code> which makes this match the goal. In other words, there's one too many elements in the list for this clause to match.
</li>
<li>Now it tries the second rule. Prolog tries to unify the goal with <code>last(cons(X, Xs), Y)</code>. This succeeds — the terms match when <code>X = apples</code>, <code>Xs = cons(oranges, nil)</code>, and <code>Y = oranges</code>. So now Prolog creates a sub-goal for each clause on the right, of which there's only one (<code>last(Xs, Y)</code>). Since <code>Xs = cons(oranges, nil)</code> and <code>Y = oranges</code>, the goal is <code>last(cons(oranges, nil), oranges)</code>. So now the code has to recursively call <code>last</code>.
</li>
<li>With this new goal we try the first clause again. Prolog tries to unify <code>last(cons(oranges, nil), oranges)</code> with <code>last(cons(X, nil), X)</code>. This succeeds when <code>X = oranges</code>. Since there are no conditions on the right hand side (this clause is the base case of the recursive function), we're done! The query succeeded; <code>oranges</code> is indeed the last element of the list.
</li>
</ol>
<h2 id="representing-prolog-syntax"><a href="#representing-prolog-syntax">Representing Prolog Syntax

</a></h2>
<p>Hopefully blasting through Prolog's core in only a few paragraphs was enough to get you excited about implementing it! The first step in interpreting a programming lanugage is to come up with a way to represent programs in that language. That means writing down some types representing the language's <em>abstract syntax tree</em>. The middle part of the diagram I drew for <a href="https://www.benjamin.pizza/posts/2017-11-13-recursion-without-recursion.html">my post announcing Sawmill</a>:</p>
<img src="/images/2017-11-13-recursion-without-recursion/compiler.jpg" alt="Compiler overview" />
<p>As the name suggests, the exercise is to come up with an abstract representation of Prolog's syntax. Think beyond the specifics of how the language is presented as text (such as where parentheses go and so on); we want to talk about the high-level grammatical constructs and how they relate to one another. This mode of thinking is akin to thinking about English at the level of sentence structure — subordinate clauses and so on — rather than spelling and punctuation.</p>
<p>I said a Prolog program was a collection of <em>rules</em>, so let's start there.</p>
<pre><code class="language-csharp">class Rule
{
    // ?
}
</code></pre>
<p>What constitutes a rule? Looking at our example from earlier,</p>
<pre><code class="language-prolog">wants(Person, Food) :- hungry(Person), likes(Person, Food).
</code></pre>
<p>you can see that a rule has two main parts, separated by the <code>:-</code> symbol. On the left is the <em>conclusion</em> of the logical statement, in the form of a pattern which the rule can match. On the right are the <em>premises</em> of the logical statement, in the form of a comma-separated list of calls to other predicates. We'll call these the <code>Head</code> and the <code>Body</code> of the rule. (A fact is just a rule with no predicates in the body.)</p>
<pre><code class="language-csharp">class Rule
{
    public ? Head { get; }
    public ImmutableArray&lt;?&gt; Body { get; }
}
</code></pre>
<p>(I'm omitting constructors for brevity.) What should the types of these properties be? A rule's head is always a name, followed by a comma-separated list of expressions inside parentheses. We'll call this a <code>Predicate</code>. A rule's body is also list of predicates.</p>
<pre><code class="language-csharp">class Rule
{
    public Predicate Head { get; }
    public ImmutableArray&lt;Predicate&gt; Body { get; }
}

class Predicate
{
    public string Name { get; }
    public ImmutableArray&lt;?&gt; Args { get; }
}
</code></pre>
<p>Now to fill in the type of <code>Args</code>. Looking at the example <code>last(cons(X, nil), X)</code>, each argument to a predicate can be one of:</p>
<ol>
<li>Another predicate applied to some arguments (<code>cons</code> in this example).
</li>
<li>A variable (<code>X</code>).
</li>
<li>An atom (<code>nil</code>).
</li>
</ol>
<p>We'll refer to all three of these syntactic forms as <em>terms</em>. We'll use subtyping to represent the fact that each argument could be any one of the three. <code>Predicate</code>, <code>Variable</code> and <code>Atom</code> are all subclasses of the <code>Term</code> base class; a <code>Predicate</code>'s arguments are a collection of <code>Term</code>s (but you don't know statically what kind of <code>Term</code> to expect).</p>
<pre><code class="language-csharp">abstract class Term {}
class Predicate : Term
{
    public string Name { get; }
    public ImmutableArray&lt;Term&gt; Args { get; }
}
class Variable : Term
{
    public string Name { get; }
}
class Atom : Term
{
    public string Value { get; }
}
</code></pre>
<p>That's our whole abstract syntax! Here's how our <code>last(cons(X, Xs), Y) :- last(Xs, Y)</code> example would be represented:</p>
<pre><code class="language-csharp">new Rule(
    head: new Predicate(
        name: &quot;last&quot;,
        args: new[]
        {
            new Predicate(&quot;cons&quot;, new[] { new Variable(&quot;X&quot;), new Variable(&quot;Xs&quot;) }),
            new Variable(&quot;Y&quot;)
        }
    ),
    body: new[]
    {
        new Predicate(&quot;last&quot;, new[] { new Variable(&quot;Xs&quot;), new Variable(&quot;Y&quot;) })
    }
)
</code></pre>
<img src="/images/2019-12-01-write-you-a-prolog/anatomy.png" alt="Anatomy of a Rule" />
<p>Prolog's rules engine is based entirely on manipulating terms, so these three classes will turn out to be quite important in our little interpreter.</p>
<h2 id="implementing-irewritable"><a href="#implementing-irewritable">Implementing <code>IRewritable</code>

</a></h2>
<p><code>Term</code> is an immutable type with a recursive tree-shaped structure — a predicate's arguments can be any <code>Term</code>, including more predicates. My generic programming library <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a> is filled with tools for working with immutable trees! As a Sawmill user, you implement its core <code>IRewritable</code> interface on your tree structure, and Sawmill takes care of much of the boilerplate of traversing the tree for you. (See <a href="https://www.benjamin.pizza/posts/2017-11-13-recursion-without-recursion.html">my earlier post</a> for an introduction to Sawmill.)</p>
<p><code>IRewritable</code> is all about addressing the immediate children of the current node in a tree. It has three methods, <code>CountChildren</code>, <code>GetChildren</code>, and <code>SetChildren</code>, which we have to implement for each subclass of our <code>Term</code> tree. Variables and atoms don't have child terms — only predicates.</p>
<pre><code class="language-csharp">abstract class Term : IRewritable&lt;Term&gt;
{
    public abstract int CountChildren();
    public abstract void GetChildren(Span&lt;Term&gt; childrenReceiver);
    public abstract Term SetChildren(ReadOnlySpan&lt;Term&gt; newChildren);
}
class Predicate : Term
{
    // ...
    public override int CountChildren() =&gt; Args.Length;
    public override void GetChildren(Span&lt;Term&gt; childrenReceiver)
    {
        Args.CopyTo(childrenReceiver);
    }
    public override Term SetChildren(ReadOnlySpan&lt;Term&gt; newChildren)
        =&gt; new Predicate(Name, newChildren.ToImmutableArray());
}
class Variable : Term
{
    // ...
    public override int CountChildren() =&gt; 0;
    public override void GetChildren(Span&lt;Term&gt; childrenReceiver) { }
    public override Term SetChildren(ReadOnlySpan&lt;Term&gt; newChildren) =&gt; this;
}
class Atom : Term
{
    // ...
    public override int CountChildren() =&gt; 0;
    public override void GetChildren(Span&lt;Term&gt; childrenReceiver) { }
    public override Term SetChildren(ReadOnlySpan&lt;Term&gt; newChildren) =&gt; this;
}
</code></pre>
<p>We can now use Sawmill's stock traversals like <code>SelfAndDescendants</code> and <code>Rewrite</code> to work with terms. For example, here's a method which finds all of the variables mentioned in a term.</p>
<pre><code class="language-csharp">public static IEnumerable&lt;string&gt; Variables(this Term term)
    =&gt; term
        .SelfAndDescendants()
        .OfType&lt;Variable&gt;()
        .Select(v =&gt; v.Name)
        .Distinct();
</code></pre>
<p>Here's a method which writes out a term as a string (and a corresponding one for <code>Rule</code>s).</p>
<pre><code class="language-csharp">abstract class Term
{
    // ...
    public override string ToString()
        =&gt; this.Fold&lt;Term, string&gt;((childStrings, x) =&gt;
        {
            switch (x)
            {
                case Predicate p:
                    return p.Name + &quot;(&quot; + string.Join(&quot;, &quot;, childStrings.ToArray()) + &quot;)&quot;;
                case Variable v:
                    return v.Name;
                case Atom a:
                    return a.Value;
                default:
                    throw new Exception(&quot;unknown term&quot;);
            }
        });
}
class Rule
{
    // ...
    public override string ToString()
        =&gt; Head + (
            Body.Length == 0
                ? &quot;&quot;
                : &quot; :- &quot; + string.Join(&quot;, &quot;, Body.Select(x =&gt; x.ToString()))
        ) + &quot;.&quot;;
}
</code></pre>
<p>As an exercise, you could try extending this abstract syntax (and <code>ToString</code>) to support numbers.</p>
<p>And a quick test of our <code>last</code> example:</p>
<pre><code class="language-csharp">static void Main(string[] args)
{
    var rule = new Rule(
        head: new Predicate(
            name: &quot;last&quot;,
            args: new[]
            {
                new Predicate(&quot;cons&quot;, new[] { new Variable(&quot;X&quot;), new Variable(&quot;Xs&quot;) }),
                new Variable(&quot;Y&quot;)
            }
        ),
        body: new[]
        {
            new Predicate(&quot;last&quot;, new[] { new Variable(&quot;Xs&quot;), new Variable(&quot;Y&quot;) })
        }
    );

    Console.WriteLine(rule.ToString());
    // prints out last(cons(X, Xs), Y) :- last(Xs, Y).
}
</code></pre>
<p>You can find these AST classes in <a href="https://github.com/benjamin-hodgson/Amateurlog">the example repo</a>, in the file <a href="https://github.com/benjamin-hodgson/Prolog/blob/master/Syntax.cs"><code>Syntax.cs</code></a>. <a href="/posts/2019-12-08-parsing-prolog-with-pidgin.html">Next time</a> we'll write a parser!</p>

]]></summary>
</entry>
<entry>
    <title>Rewriting IRewritable</title>
    <link href="http://www.benjamin.pizza/posts/2019-10-05-rewriting-irewritable.html" />
    <id>http://www.benjamin.pizza/posts/2019-10-05-rewriting-irewritable.html</id>
    <published>2019-10-05T00:00:00Z</published>
    <updated>2019-10-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<blockquote>
<p><em>I got married yesterday! This post is about my C# generic programming library <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a>. Have a read of <a href="/posts/2017-11-13-recursion-without-recursion.html">my earlier post</a> for an introduction.</em></p>
</blockquote>
<h2 id="how-things-were">[How things were](#how-things-were</h2>
<p>I recently made a substantial change to Sawmill's core <code>IRewritable</code> interface. Here's what it used to look like:</p>
<pre><code class="language-csharp">interface IRewritable&lt;T&gt; where T : IRewritable&lt;T&gt;
{
    Children&lt;T&gt; GetChildren();
    T SetChildren(Children&lt;T&gt; newChildren);
    T RewriteChildren(Func&lt;T, T&gt; transformer);
}
</code></pre>
<p>It's a pleasingly simple interface with two core operations — <code>GetChildren</code> returns the current object's immediate children and <code>SetChildren</code> replaces them. But there are also some warts.</p>
<h3 id="wart-1-childrent">[Wart 1: <code>Children&lt;T&gt;</code>](#wart-1-childrent</h3>
<p><code>Children&lt;T&gt;</code> is a custom struct, not a standard collection type:</p>
<pre><code class="language-csharp">struct Children&lt;T&gt;
{
    public NumberOfChildren NumberOfChildren { get; }
    public T First { get; }
    public T Second { get; }
    public ImmutableList&lt;T&gt; Many { get; }
}
enum NumberOfChildren
{
    None, One, Two, Many
}
</code></pre>
<p>I wanted to avoid boxing when the object has a small number of children (which is fairly common in practice). So <code>GetChildren</code> returns a <code>Children&lt;T&gt;</code>, passing up to two children on the stack and the rest in an <code>ImmutableList</code>. The <code>NumberOfChildren</code> property tells the library how many of the struct's fields are filled in.</p>
<p>This custom collection type is an extra hurdle to understanding <code>IRewritable</code>'s API. It also makes certain parts of Sawmill's implementation more complex — many internal methods have to <code>switch</code> on the <code>NumberOfChildren</code> they're working with and do the same work in four different ways. It's also relatively large for a <code>struct</code> (at least 16 bytes and probably more, depending on your processor architecture) so there is a marginal performance cost associated with copying it around.</p>
<h3 id="wart-2-collections">[Wart 2: Collections](#wart-2-collections</h3>
<p>In fact, having <code>GetChildren</code> return a collection at all is problematic, because it forces implementations to allocate memory. If your object has three or more children (and they're not already stored in an <code>ImmutableList</code>) then you have to create a new <code>ImmutableList</code> whenever <code>GetChildren</code> is called:</p>
<pre><code class="language-csharp">class ThreeChildren : IRewritable&lt;ThreeChildren&gt;
{
    private readonly ThreeChildren _child1;
    private readonly ThreeChildren _child2;
    private readonly ThreeChildren _child3;

    public Children&lt;ThreeChildren&gt; GetChildren()
        =&gt; new Children&lt;ThreeChildren&gt;(
            Children.Many,
            ImmutableList.Create(_child1, _child2, _child3)
        );
    public ThreeChildren SetChildren(Children&lt;ThreeChildren&gt; newChildren)
        =&gt; new ThreeChildren(
            newChildren.Many[0],
            newChildren.Many[1],
            newChildren.Many[2]
        );
}
</code></pre>
<p>When Sawmill traverses a tree it typically calls <code>GetChildren()</code> for every node in the tree. That's a lot of throwaway <code>ImmutableList</code>s!</p>
<p>One way to avoid creating all this garbage might be to redesign <code>IRewritable</code> to be buffer-oriented.</p>
<pre><code class="language-csharp">interface IRewritable&lt;T&gt; where T : IRewritable&lt;T&gt;
{
    int CountChildren();
    void GetChildren(T[] buffer);
    T SetChildren(T[] newChildren);
}
</code></pre>
<p>With this design, Sawmill passes an array into <code>GetChildren</code> (after calling <code>CountChildren</code> to find out how big the array needs to be) and asks the object to copy its children into the array. Implementations of <code>IRewritable</code> no longer have to allocate memory for their return value. The memory is allocated (and hopefully reused) by the library. But this API is less safe — if an <code>IRewritable</code> implementation stores a reference to the buffer then it could get unexpectedly mutated.</p>
<pre><code class="language-csharp">class Bad : IRewritable&lt;Bad&gt;
{
    private readonly Bad[] _children;

    public int CountChildren() =&gt; _children.Length;
    public void GetChildren(Bad[] buffer) =&gt; _children.CopyTo(buffer);
    public Bad SetChildren(Bad[] newChildren) =&gt; new Bad(newChildren);  // BUG
}
</code></pre>
<p>This is not a &quot;pit of success&quot; API — <code>SetChildren</code> looks sensible but could go wrong if anyone else has a reference to the array. Sawmill would have to allocate a new array for each <code>GetChildren</code>/<code>SetChildren</code> call in order to be safe. So this design would end up allocating just as much as the <code>ImmutableList</code> version.</p>
<p>(Keep the buffer idea in your head, though, because we'll be coming back to it in a minute.)</p>
<h3 id="wart-3-rewritechildren">[Wart 3: <code>RewriteChildren</code>](#wart-3-rewritechildren</h3>
<p><code>RewriteChildren</code>, which applies a transformation function to the object's immediate children (in other words, a one-level <code>Rewrite</code>), has a sensible implementation in terms of the other two methods: get the children, transform them, and put them back.</p>
<pre><code class="language-csharp">public static T DefaultRewriteChildren&lt;T&gt;(this T value, Func&lt;T, T&gt; transformer)
    where T : IRewritable&lt;T&gt;
{
    var children = value.GetChildren();
    var newChildren = children.Select(transformer);
    return value.SetChildren(newChildren);
}
</code></pre>
<p>(<a href="https://github.com/benjamin-hodgson/Sawmill/blob/87aea1e5757360b99457c8e2e6a7993fc2176f23/Sawmill/Rewriter.DefaultRewriteChildren.cs">The real <code>DefaultRewriteChildren</code></a> was a bit more complex than this, because it tried to avoid calling <code>SetChildren</code> if <code>transformer</code> didn't actually change anything.)</p>
<p>In fact, I expect that most <code>IRewritable</code>s would just delegate <code>RewriteChildren</code> to <code>DefaultRewriteChildren</code>. So why did I put <code>RewriteChildren</code> on the interface? It's because of Wart 2 — <code>GetChildren</code> can be expensive because it might have to create an <code>ImmutableList</code>. (And applying <code>transformer</code> to that <code>ImmutableList</code> using eg <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.immutable.immutablelist-1.convertall?view=netcore-3.0"><code>ConvertAll</code></a> would have to create another one.) So <code>DefaultRewriteChildren</code> is slow; the idea of <code>RewriteChildren</code> was for objects to transform their children directly, without going via an <code>ImmutableList</code>.</p>
<pre><code class="language-csharp">class ThreeChildren : IRewritable&lt;ThreeChildren&gt;
{
    // ...
    public ThreeChildren RewriteChildren(Func&lt;ThreeChildren, ThreeChildren&gt; f)
        =&gt; new ThreeChildren(f(_child1), f(_child2), f(_child3));  // no ImmutableList
}
</code></pre>
<p>The extra method makes <code>IRewritable</code> harder to understand and harder to implement. Implementing <code>RewriteChildren</code> (without using <code>DefaultRewriteChildren</code>) can be tricky; if the transformer function doesn't change anything you should avoid recreating the object.</p>
<h2 id="the-new-way">[The new way](#the-new-way</h2>
<p>I'm pleased to say that <code>IRewritable</code>'s new design addresses all three of these warts. Remember the &quot;buffer&quot; idea from earlier?</p>
<pre><code class="language-csharp">interface IRewritable&lt;T&gt; where T : IRewritable&lt;T&gt;
{
    int CountChildren();
    void GetChildren(Span&lt;T&gt; buffer);
    T SetChildren(ReadOnlySpan&lt;T&gt; newChildren);
}
</code></pre>
<p>This design is almost identical to the one I outlined earlier — it uses <a href="https://docs.microsoft.com/en-us/dotnet/api/system.span-1?view=netcore-3.0"><code>Span</code></a>s instead of arrays. (For the uninitiated, a <code>Span</code> is basically a &quot;slice&quot; of an array. It can also be backed by <em>unmanaged</em> memory, though, making it more flexible than <code>ArraySegment</code>.)</p>
<ul>
<li><code>CountChildren</code> tells Sawmill how much space is needed to copy the children.
</li>
<li><code>GetChildren</code> copies the current object's immediate children into <code>buffer</code>.
</li>
<li><code>SetChildren</code> creates a copy of the current object with its immediate children replaced.
</li>
</ul>
<p>Here's the important difference. Unlike an array, a <code>Span</code> <em>can't be stored on the heap</em>. <code>Span</code> is defined using <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/ref?view=netcore-3.0#ref-struct-types">the <code>ref</code> keyword</a>, which tells the compiler to check that <code>Span</code>s are confined to the stack. If you pass a <code>Span</code> into a method as an argument, you can be confident that the <code>Span</code> won't leave the scope of that method's stack frame (just like <code>ref</code>).</p>
<p>So <code>Span</code> thoroughly solves the safety issue with the array-oriented API. I don't have to worry about mutating a <code>Span</code> which got stored inside a client object, because the <code>Span</code> can't be stored! This allows Sawmill to safely reuse the memory behind a <code>Span</code>, rather than allocating a new array for each <code>GetChildren</code>/<code>SetChildren</code> call.</p>
<p>So Sawmill's methods like <code>RewriteChildren</code> can be implemented without allocating memory. In this example I'm using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.buffers.arraypool-1?view=netcore-3.0"><code>ArrayPool</code></a> to avoid creating a new array for every <code>RewriteChildren</code> call.</p>
<pre><code class="language-csharp">public static T RewriteChildren&lt;T&gt;(this T value, Func&lt;T, T&gt; transformer)
    where T : IRewritable&lt;T&gt;
{
    var count = value.CountChildren();
    var array = ArrayPool&lt;T&gt;.Shared.Rent(count);
    // ArrayPool can return arrays bigger than you asked for
    var span = array.AsSpan().Slice(count);

    value.GetChildren(span);
    
    for (var i = 0; i &lt; span.Length; i++)
    {
        span[i] = transformer(span[i]);
    }

    var result = value.SetChildren(newChildren);
    ArrayPool&lt;T&gt;.Shared.Return(array);
    return result;
}
</code></pre>
<p>The main reason for having <code>RewriteChildren</code> be a method on the interface was to avoid allocating memory (for the <code>GetChildren</code> calls). So we don't need it on the interface any more — this extension method serves as a single universal implementation. Likewise, <code>Children&lt;T&gt;</code>'s purpose was also to avoid allocating memory, so we can do away with it too.</p>
<h2 id="relieving-arraypool-pressure">[Relieving <code>ArrayPool</code> pressure](#relieving-arraypool-pressure</h2>
<p>There's one operational problem with this implementation: it can end up renting a large number of arrays from the pool. <code>Rewrite</code>, which applies a transformation function to every node in a tree (not just one layer), is implemented something like this:</p>
<pre><code class="language-csharp">public static T Rewrite(this T value, Func&lt;T, T&gt; transformer) where T : IRewritable&lt;T&gt;
    =&gt; transformer(t.RewriteChildren(child =&gt; child.Rewrite(transformer)));
</code></pre>
<p>The lambda which is passed to <code>RewriteChildren</code> contains a recursive call to <code>Rewrite</code>. Let's think through the operational behaviour of <code>Rewrite</code>:</p>
<ol>
<li><code>Rewrite</code> calls <code>RewriteChildren</code>
</li>
<li><code>RewriteChildren</code> rents an array from the array pool and calls <code>GetChildren</code>
</li>
<li><code>RewriteChildren</code> calls the <code>child =&gt; child.Rewrite(transformer)</code> lambda function for each child
</li>
<li>The lambda function recursively calls <code>Rewrite</code>; steps 1-3 are repeated until you encounter a node with no children
</li>
<li><code>RewriteChildren</code> calls <code>SetChildren</code> and returns its array to the array pool
</li>
<li><code>RewriteChildren</code> returns and step 5 is repeated as you return up the call stack
</li>
</ol>
<p>Since steps 1-3 are repeated before step 5 happens, you can end up renting many arrays (a number equal to the height of the tree) before returning any of them to the pool. So the array pool could run out of arrays!</p>
<p>To fix this problem, we want to rent a small number of large arrays from the array pool, rather than a large number of small ones. We can lean on the fact that each array only lives as long as a single method — the array is <code>Rent</code>ed at the start of <code>RewriteChildren</code> and then <code>Return</code>ed at the end. The memory usage is stack-shaped.</p>
<p>So here's the plan. We're going to rent a large array from the pool at <code>Rewrite</code>'s beginning, and <code>RewriteChildren</code> will take a chunk from that array each time it's called. Each chunk will be freed up before any previously-allocated chunks are freed.</p>
<pre><code class="language-csharp">public static T Rewrite&lt;T&gt;(this T value, Func&lt;T, T&gt; transformer)
    where T : IRewritable&lt;T&gt;
{
    using (var chunks = new ChunkStack&lt;T&gt;())
    {
        T Go(T x)
            =&gt; transformer(t.RewriteChildrenInternal(Go, chunks));
        return Go(value);
    }
}

public static T RewriteChildren&lt;T&gt;(this T value, Func&lt;T, T&gt; transformer)
    where T : IRewritable&lt;T&gt;
{
    using (var chunks = new ChunkStack&lt;T&gt;())
    {
        return RewriteChildrenInternal(t, transformer, chunks);
    }
}

private static T RewriteChildrenInternal&lt;T&gt;(
    this T value,
    Func&lt;T, T&gt; transformer,
    ChunkStack&lt;T&gt; chunks
) where T : IRewritable&lt;T&gt;
{
    var count = value.CountChildren();
    var span = chunks.Allocate(count);

    value.GetChildren(span);
    
    for (var i = 0; i &lt; span.Length; i++)
    {
        span[i] = transformer(span[i]);
    }

    var result = value.SetChildren(newChildren);
    chunks.Free(span);
    return result;
}
</code></pre>
<p><code>ChunkStack</code> contains an array and a count of how much of that array is in use. <code>Allocate</code> and <code>Free</code> increase and decrease that count.</p>
<pre><code class="language-csharp">class ChunkStack&lt;T&gt; : IDisposable
{
    private T[] _array = ArrayPool&lt;T&gt;.Shared.Rent(512);
    private int _used = 0;

    public Span&lt;T&gt; Allocate(int count)
    {
        var span = _array.Slice(_used, count);
        _used += count;
        return span;
    }
    public void Free(Span&lt;T&gt; span)
    {
        _used -= span.Length;
    }
    public void Dispose()
    {
        if (_array != null)
        {
            ArrayPool&lt;T&gt;.Shared.Return(_array);
            _array = null;
        }
    }
}
</code></pre>
<p>I've glossed over an important part of <code>ChunkStack</code>'s implementation: what happens when the array fills up? The <a href="https://github.com/benjamin-hodgson/Sawmill/blob/bf652359023a76d3ea395d7d743db1a8d6559ec2/Sawmill/ChunkStack.cs">real implementation</a> manages a collection of &quot;regions&quot;, taking a new region from the array pool when existing regions fill up.</p>
<p>In the pathological case of a <em>very</em> large tree, this version of <code>Rewrite</code> can still exhaust the array pool, but it'll happen much less quickly.</p>
<h2 id="hackalloc">[Hackalloc](#hackalloc</h2>
<p>I mentioned earlier that a <code>Span</code> is not necessarily backed by an array. <code>Span</code> represents <em>a contiguous block of memory</em>, with no assumptions about where that memory is or how it's managed. A <code>Span</code> could be a slice of an array, or it could be a chunk of unmanaged memory, or it could be an area of the stack. Under the hood it's <em>just a pointer</em>; <code>Span</code> doesn't care exactly where the pointer points.</p>
<p>In fact, C# has built in support for that last case. The <code>stackalloc</code> keyword works like C's <code>alloca</code>: it carves out a chunk of memory directly in the current stack frame, which becomes invalid when the current method returns. Until recently, <code>stackalloc</code> was only available in an <code>unsafe</code> context, but today it's available in safe code thanks to <code>Span</code>.</p>
<pre><code class="language-csharp">public void StackallocExample()
{
    // allocate space for three ints in the current stack frame
    Span&lt;int&gt; myInts = stackalloc int[3];
    myInts[0] = 123;
    myInts[1] = 456;
    myInts[2] = myInts[0] + myInts[1];
    Console.WriteLine(myInts[2]);  // prints 579
}
</code></pre>
<p>I want to use <code>stackalloc</code> to avoid taking memory from the <code>ChunkStack</code> when an object has a small number (say, 4) of children. This is fairly common in practice. Stack memory tends to be marginally faster than heap memory because it's more likely to be in the processor cache, so this may have a modest performance benefit as well as relieving pressure on the array pool. If there are more than 4 children we can just fall back on the <code>ChunkStack</code>.</p>
<p>Here's the new <code>RewriteChildrenInternal</code>:</p>
<pre><code class="language-csharp">private static T RewriteChildrenInternal&lt;T&gt;(
    this T value,
    Func&lt;T, T&gt; transformer,
    ChunkStack&lt;T&gt; chunks
) where T : IRewritable&lt;T&gt;
{
    var count = value.CountChildren();

    Span&lt;T&gt; span = stackalloc T[4];
    if (count &gt; 4)
    {
        span = chunks.Allocate(count);
    }

    value.GetChildren(span);
    
    for (var i = 0; i &lt; span.Length; i++)
    {
        span[i] = transformer(span[i]);
    }

    var result = value.SetChildren(newChildren);

    if (count &gt; 4)
    {
        chunks.Free(span);
    }
    return result;
}
</code></pre>
<p>Sadly this doesn't work. The compiler complains about the <code>stackalloc T</code> line: &quot;Cannot take the address of, get the size of, or declare a pointer to a managed type ('T')&quot;. Basically, the CLR doesn't support <code>stackalloc</code> with reference types — you can only use <code>stackalloc</code> with primitives or structs containing primitives. (A type parameter <code>T</code> <em>might</em> be a reference type, so you still can't use it with <code>stackalloc</code>.) Under the hood, <code>stackalloc</code> is untyped; the garbage collector doesn't know how to follow pointers that are stored in <code>stackalloc</code>ed memory because it doesn't even know there are pointers there.</p>
<p>I still think the idea's a good one, though. Can we unsafely hack it up?</p>
<p>I'm going to use the following <code>struct</code> as a &quot;poor man's <code>stackalloc[4]</code>&quot;:</p>
<pre><code class="language-csharp">struct Four&lt;T&gt;
{
    public T First;
    public T Second;
    public T Third;
    public T Fourth;
}
</code></pre>
<p>A variable of type <code>Four&lt;T&gt;</code> has enough room for four <code>T</code>s — so when the variable is a local variable (in an ordinary method) it's functionally equivalent to a <code>stackalloc T[4]</code>. We won't be using the <code>First</code>, <code>Second</code>, <code>Third</code> and <code>Fourth</code> properties directly — we'll be (unsafely) addressing them relative to the start of the struct. In this example I'm using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe?view=netcore-3.0"><code>System.Runtime.CompilerServices.Unsafe</code></a> to address <code>Third</code> by looking 2 elements beyond <code>First</code>:</p>
<pre><code class="language-csharp">var four = new Four&lt;T&gt;();
ref T third = ref Unsafe.Add(ref four.First, 2);
Assert.True(Unsafe.AreSame(ref four.Third, ref third));
</code></pre>
<p>The plan is to create a <code>Span</code> whose pointer refers to the start of a <code>Four&lt;T&gt;</code> on the stack. <code>span[0]</code> will address <code>four.First</code>, <code>span[1]</code> will address <code>Second</code>, and so on. My first idea to implement this was to use <code>System.Runtime.CompilerServices.Unsafe</code> to coerce a <code>ref Four&lt;T&gt;</code> to an unmanaged pointer, and then put that in a <code>Span</code>:</p>
<pre><code class="language-csharp">unsafe
{
    var four = new Four&lt;T&gt;();
    void* ptr = Unsafe.AsPointer(ref four);
    var span = new Span&lt;T&gt;(ptr, 4);
}
</code></pre>
<p>Sadly the <code>Span</code> constructor throws an exception when <code>T</code> is a reference type. At this point I went for a poke around in the .NET source code. I wanted to know how <code>array.AsSpan()</code> works. I found <a href="https://github.com/dotnet/corefx/blob/7e9a177824cbefaee8985a9b517ebb0ea2e17a81/src/Common/src/CoreLib/System/Span.Fast.cs#L123">an internal constructor</a> which takes a <code>ref T</code>. We can illictly call that constructor using reflection, although of course we want to avoid the performance costs of reflection. So the actual plan is to use runtime code generation to call the internal <code>Span</code> constructor. Ordinarily I'd use <code>Expression</code> to do this runtime code generation, but <code>Expression</code> doesn't support <code>ref</code> parameters, so we have to write the IL by hand.</p>
<pre><code class="language-csharp">private static class SpanFactory&lt;T&gt;
{
    private delegate Span&lt;T&gt; SpanCtor(ref T value, int length);
    private static readonly SpanCtor _spanCtor;

    static SpanFactory()
    {
        var ctor = typeof(Span&lt;T&gt;)
            .GetConstructors(BindingFlags.NonPublic | BindingFlags.Instance)
            .Single(c =&gt;
                c.GetParameters().Length == 2
                &amp;&amp; c.GetParameters()[0].ParameterType.IsByRef
            );

        var method = new DynamicMethod(
            &quot;&quot;,
            typeof(Span&lt;T&gt;),
            new[] { typeof(T).MakeByRefType(), typeof(int) }
        );

        var il = method.GetILGenerator();
        il.Emit(OpCodes.Ldarg_0);
        il.Emit(OpCodes.Ldarg_1);
        il.Emit(OpCodes.Newobj, ctor);
        il.Emit(OpCodes.Ret);

        _spanCtor = (SpanCtor)method.CreateDelegate(typeof(SpanCtor));
    }

    [MethodImpl(MethodImplOptions.AggressiveInlining)]
    public static Span&lt;T&gt; Create(ref T value, int length)
        =&gt; _spanCtor(ref value, length);
}
</code></pre>
<p>Obviously relying on BCL internals like this is risky. The internal constructor could be removed, or changed to work differently, in which case my code could stop working or even segfault. That said, I think the likelihood of the internal constructor changing is quite low in this case.</p>
<blockquote>
<p><strong>Update</strong>: On .NET Core, there is an officially-supported API to do this: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.memorymarshal.createspan?view=netcore-3.0"><code>MemoryMarshal.CreateSpan</code></a>. I didn't know it existed at the time that I wrote this.</p>
</blockquote>
<p>There are also risks associated with mixing pointers and references like this. You have to be very careful that the <code>Span</code> doesn't live longer than the <code>Four</code> it points to. That means the <code>Four</code> has to be discarded at the end of the method along with the <code>Span</code>, and it has to be stored in a &quot;real&quot; local variable, not in temporary storage on the evaluation stack. I'll address that by mentioning the variable (as a parameter to a non-inlined &quot;keep-alive&quot; method) at the end of the method.</p>
<p>You also need to be certain that the <code>Four</code> is stored on the stack and not the heap. Data stored on the heap is liable to get moved by the garbage collector, which would invalidate the pointer inside the <code>Span</code>. Beware that local variables are not always safe from being moved! Methods containing <code>await</code>s, <code>yield</code>s, and lambdas are liable to store their local variables on the heap, so if <code>RewriteChildrenInternal</code> were not an ordinary method this hack would not be safe.</p>
<p>Here's the final implementation of <code>RewriteChildrenInternal</code>. <code>var four = new Four&lt;T&gt;();</code> allocates space for four <code>T</code>s in <code>RewriteChildrenInternal</code>'s stack frame. Then, when I call <code>GetSpan</code>, I'm passing in the address of the start of that <code>Four</code> using the <code>ref</code> keyword. <code>GetSpan</code> returns a <code>Span</code> which either points at the start of <code>four</code> or at a chunk taken from the <code>ChunkStack</code>, depending on how many children we need to store. <code>ReleaseSpan</code> returns the <code>Span</code> to the <code>ChunkStack</code> if it came from there, and the <code>KeepAlive</code> call ensures the <code>four</code> isn't deallocated too early.</p>
<pre><code class="language-csharp">private static T RewriteChildrenInternal&lt;T&gt;(
    this T value,
    Func&lt;T, T&gt; transformer,
    ChunkStack&lt;T&gt; chunks
) where T : IRewritable&lt;T&gt;
{
    var count = value.CountChildren();

    var four = new Four&lt;T&gt;();
    var span = GetSpan(count, chunks, ref four);

    value.GetChildren(span);
    
    for (var i = 0; i &lt; span.Length; i++)
    {
        span[i] = transformer(span[i]);
    }

    var result = value.SetChildren(newChildren);

    ReleaseSpan(span, chunks);
    KeepAlive(ref four);
    return result;
}
private static Span&lt;T&gt; GetSpan&lt;T&gt;(int count, ChunkStack&lt;T&gt; chunks, ref Four&lt;T&gt; four)
{
    if (count == 0)
    {
        return new Span&lt;T&gt;();
    }
    else if (count &lt;= 4)
    {
        return SpanFactory&lt;T&gt;.Create(ref four.First, count);
    }
    else
    {
        return chunks.Allocate(count);
    }
}
private static void ReleaseSpan&lt;T&gt;(Span&lt;T&gt; span, ChunkStack&lt;T&gt; chunks)
{
    if (span.Length &gt; 4)
    {
        chunks.Free(span);
    }
}
[MethodImpl(MethodImplOptions.NoInlining)]
private static void KeepAlive&lt;T&gt;(ref Four&lt;T&gt; four)
{
}
</code></pre>
<p>As far as I know, the designers of <code>Span</code> were thinking primarily about applications such as serialisation and parsing — the sort of low-level code you'd find in a <a href="https://github.com/aspnet/AspNetCore">high performance web server</a>. But <code>Span</code> also really shines in this high-level library of recursion patterns. Its guarantees about storage proved crucial to the safety of my <code>IRewritable</code> abstraction, but I'm also leaning on its flexibility to implement that abstraction as efficiently as possible.</p>
<p>Sawmill version 3.0 is now available <a href="https://www.nuget.org/packages/Sawmill">on Nuget</a>, and you can read all of this code in <a href="https://github.com/benjamin-hodgson/Sawmill">the GitHub repo</a>.</p>

]]></summary>
</entry>
<entry>
    <title>Announcing Pidgin v2.0</title>
    <link href="http://www.benjamin.pizza/posts/2019-01-26-announcing-pidgin-v2.0.html" />
    <id>http://www.benjamin.pizza/posts/2019-01-26-announcing-pidgin-v2.0.html</id>
    <published>2019-01-26T00:00:00Z</published>
    <updated>2019-01-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>Happy birthday to me! I'm pleased to announce that version 2.0 of my functional parsing library, <a href="https://github.com/benjamin-hodgson/Pidgin">Pidgin</a>, is now available <a href="https://www.nuget.org/packages/Pidgin">on Nuget</a>. In this release I've focused on error messages, performance, and <code>Span</code> support.</p>
<h2 id="parser-combinators"><a href="#parser-combinators">Parser Combinators

</a></h2>
<p>I haven't written about Pidgin before, so allow me to briefly introduce it. Pidgin is a <a href="https://en.wikipedia.org/wiki/Parser_combinator"><em>parser combinator</em></a> library, meaning that it consists of three main concepts:</p>
<ul>
<li>A type <code>Parser</code> which models a <em>parsing process</em>
</li>
<li>A collection of <em>primitive</em> <code>Parser</code> objects which perform some simple individual parsing task
</li>
<li>A collection of <em>combinator</em> functions which can build complex <code>Parser</code>s out of simpler ones
</li>
</ul>
<p>Taken together, we have an object model allowing you to write code resembling a high-level description of a parsing process. As a brief taste, here is a simple example parser which parses an identifier in a typical programming language.</p>
<pre><code class="language-csharp">Parser&lt;char, string&gt; Identifier =
    Letter.Then(
        LetterOrDigit.ManyString(),
        (first, rest) =&gt; first + rest
    );

Assert.Equal(&quot;abc1&quot;, Identifier.ParseOrThrow(&quot;abc1&quot;));
</code></pre>
<ul>
<li><code>Identifier</code> is a <code>Parser&lt;char, string&gt;</code>, meaning it's a process which consumes a sequence of <code>char</code>s and produces a <code>string</code>.
</li>
<li><code>Letter</code> is a primitive parser which consumes and returns a single character from the input stream, moving it on to the next character. (If the character is not a letter, the parser fails and doesn't change the state of the input stream.)
</li>
<li><code>LetterOrDigit</code> is like <code>Letter</code> but for alphanumeric characters.
</li>
<li><code>ManyString</code> is a combinator method which runs a parser in a loop until it fails. It takes all of the smaller parser's results from the loop and packs them into a <code>string</code>. So <code>LetterOrDigit.ManyString()</code> is a parser which consumes and returns a sequence of alphanumeric characters.
</li>
<li><code>Then</code> is another combinator which runs two parsers in sequence and applies a function to the result. So, reading the parser as a whole, we can see that an <code>Identifier</code> consists of a single letter followed by a sequence of letters or digits.
</li>
</ul>
<p>Parser combinators' power comes from their composability. The library comprises a small number of building blocks, which you can put together in rich and varied ways to build a parser which does what you need. The library's level of abstraction is a good fit for small-to-medium sized parsing tasks: it's not as high-level as a full-blown parser generator like Antlr, but it's much simpler to integrate. Rewriting our hand-written <a href="https://www.benjamin.pizza/posts/2017-11-13-recursion-without-recursion.html">JQL</a> parser in Pidgin took around four times less code.</p>
<p>That's the overview — I won't dive into a full tutorial on parser combinators because there are already <a href="http://www.lihaoyi.com/post/EasyParsingwithParserCombinators.html">plenty</a> <a href="http://webdoc.sub.gwdg.de/ebook/serien/ah/UU-CS/2008-044.pdf">of</a> <a href="https://news.ycombinator.com/item?id=14600079">those</a> a mere Google away.</p>
<h3 id="how-it-works">[How it works](#how-it-works</h3>
<p>Pidgin's <code>Parser&lt;TToken, T&gt;</code> type represents a process which pulls <code>TToken</code>s one at a time from a (stateful) input stream, and either successfully returns a <code>T</code> or fails with an error message.</p>
<pre><code class="language-csharp">public abstract class Parser&lt;TToken, T&gt;
{
    internal abstract Result&lt;T&gt; Parse(IParseState&lt;TToken&gt; input);
}
internal interface IParseState&lt;TToken&gt;
{
    TToken? Peek();  // null if at end of file
    void Advance();

    // ... plus a few methods to facilitate backtracking etc
}
internal struct Result&lt;T&gt;
{
    // if Success is true, Value contains the parsed value and Error is null
    // if Success is false, Value is null and Error contains error info
    public bool Success { get; }
    public T Value { get; }
    public ParseError Error { get; }
}
</code></pre>
<p><code>IParseState</code> represents an iterator over the parser's input (a sequence of <code>TToken</code>s). <code>Peek</code> returns the token at the current location in the input (or <code>null</code> if the parser has reached the end of the input) and <code>Advance</code> moves the stream on to the next token.</p>
<p>Primitive parsers typically manipulate the <code>IParseState</code> directly. This parser matches a specific string:</p>
<pre><code class="language-csharp">class StringParser : Parser&lt;char, string&gt;
{
    private readonly string _expected;

    internal override Result&lt;string&gt; Parse(IParseState&lt;char&gt; input)
    {
        foreach (var c in _expected)
        {
            var token = input.Peek();
            if (!token.HasValue)
            {
                return Result.Error(ParseError.EndOfFile);
            }
            if (token.Value != c)
            {
                return Result.Error(ParseError.UnexpectedToken);
            }
            input.Advance();
        }
        return Result.Success(_expected);
    }
}
</code></pre>
<p>Higher-order parsers typically compose one or more smaller parsers, delegating to their <code>Parse</code> methods in some useful way. <code>Then</code> returns a parser which sequences two parsers and applies a mapping function to their results:</p>
<pre><code class="language-csharp">class ThenParser&lt;TToken, T, U, R&gt; : Parser&lt;TToken, R&gt;
{
    private readonly Parser&lt;TToken, T&gt; _first;
    private readonly Parser&lt;TToken, U&gt; _second;
    private readonly Func&lt;T, U, R&gt; _resultSelector;

    internal override Result&lt;R&gt; Parse(IParseState&lt;TToken&gt; input)
    {
        var result1 = _first.Parse(input);
        if (!result1.Success)
        {
            return Result.Error(result1.Error);
        }
        var result2 = _second.Parse(input);
        if (!result2.Success)
        {
            return Result.Error(result2.Error);
        }
        return Result.Success(_resultSelector(result1.Value, result2.Value));
    }
}
</code></pre>
<h2 id="error-messages"><a href="#error-messages">Error messages

</a></h2>
<p>Part of Pidgin's job is to report when you gave a parser bad input. For example, the top level of a C# file must contain <code>class</code>es and <code>namespace</code>s, so feeding a C# parser the text <code>Console.WriteLine(&quot;foo&quot;);</code> (without an enclosing <code>class</code>) should fail with some information about the problem:</p>
<pre><code>Parse error.
    unexpected 'C'
    expected &quot;class&quot; or &quot;namespace&quot;
    at line 1, col 1
</code></pre>
<p>A mistake I made early on in Pidgin's development nearly two years ago(!) was trying to pre-compute the content of these error messages. Under the assumption that parsers are typically built once and then run repeatedly, I wrote some code to examine your parser upon construction and try to predict the ways it could fail on unexpected input, in order to avoid constructing error messages at runtime. This code calculated a set of &quot;expected&quot; input strings, accounting for <code>Then</code>s and <code>Or</code>s by concatenating the strings in the set and by unioning the sets respectively. (The idea was that a parser like <code>Keyword(&quot;public&quot;).Optional().Then(Keyword(&quot;class&quot;))</code> would report that it expected <code>&quot;class&quot; or &quot;public class&quot;</code>.)</p>
<p>This went catastrophically wrong for complex parsers. Here's a sketch of some code which parses left-associative mathematical operators with precedence, so <code>3^2 + 4 * 3^5 * 5</code> is parsed as <code>(3^2) + ((4 * (3^5)) * 5)</code>:</p>
<pre><code class="language-csharp">var topPrecedence = Number.Then(Char('^').Then(Number).Many());
var midPrecedence = topPrecedence.Then(Char('*').Then(topPrecedence).Many());
var lowPrecedence = midPrecedence.Then(Char('+').Then(midPrecedence).Many());
</code></pre>
<p><code>topPrecedence</code> expects a number, or a number followed by a sequence of exponents — two possibilities. <code>midPrecedence</code> therefore expects a number, or a number followed by a sequence of exponents, or a number followed by a sequence of multiplications, or a number followed by a sequence of exponents followed by a sequence of multiplications — four possibilities. <code>lowPrecedence</code> has eight possible expected inputs. The set of expected inputs blows up exponentially! Calculating the error messages in advance means you need to explore all of the (exponentially large number of) expected inputs in advance, which led to cosmologically long parser build times. And in any case, reporting a huge number of expected inputs does not make for a very good error message.</p>
<p>I made some attempts to optimise this by using more efficient data structures, but I realised this was a losing proposition and decided to throw it out altogether in v2.0. Error messages are now computed at runtime, when the error actually occurs. This means I can be more precise about what the parser was expecting at that particular point in the input: once a parsing process has committed to a branch I can report expected inputs <em>from that branch</em>, rather than reporting all possible expected inputs.</p>
<p>Implementing this efficiently was a challenge. Parse errors actually occur quite frequently in a parser combinator library, even in the happy path, because of the way the <em>prioritised choice</em> operator <code>Or</code> works — <code>String(&quot;foo&quot;).Or(String(&quot;bar&quot;))</code> only tries <code>bar</code> if an attempt to parse <code>foo</code> failed. So I tried to implement this change without allocating heap memory every time a parser fails. When a parser fails, it saves its expected inputs in a stack implemented on top of pooled memory (using <code>ArrayPool</code>), which are then popped if the error gets discarded. (The way that certain parsers manipulate their children's error messages adds some interesting complications here, which I've described in a <a href="https://github.com/benjamin-hodgson/Pidgin/blob/60c7734393719d11714158b201c99976ec48ffb9/Pidgin/ParseState.Error.cs#L36-L74">long comment</a>.)</p>
<h2 id="span"><a href="#span"><code>Span</code>

</a></h2>
<blockquote>
<p><strong>Note</strong>: You can watch <a href="https://twitter.com/g3rv4">my esteemed colleague Gervasio</a> and me carrying out the work I describe here in <a href="https://www.youtube.com/watch?v=O23OLkQtiS4">a live-stream on YouTube</a>. It was pretty fun!</p>
</blockquote>
<p>I wanted to add support for parsing input stored in a <code>Span</code>. (<code>Span</code> is a new part of the BCL representing a reference to a contiguous block of memory such as a chunk of an array.) I already had functions which applied a <code>Parser</code> to a <code>string</code>, a <code>T[]</code>, a <code>Stream</code>, etc; I'd abstracted over these various input types using the aforementioned <code>IParseState</code> interface.</p>
<pre><code class="language-csharp">internal interface IParseState&lt;TToken&gt;
{
    TToken? Peek();
    void Advance();
}
</code></pre>
<p>So adding <code>Span</code> support basically means implementing <code>IParseState</code> on top of a <code>Span</code>. In theory this should be quite straightforward — a class which has a <code>Span</code> and keeps track of its current position:</p>
<pre><code class="language-csharp">class SpanTokenStream&lt;TToken&gt; : ITokenStream&lt;TToken&gt;
{
    private readonly Span&lt;TToken&gt; _span;
    private int _current;

    TToken? Peek() =&gt; _current &gt;= _span.Length ? null : _span[_current];
    
    void Advance()
    {
        _current++;
    }
}
</code></pre>
<p>But if you try and write this class, you'll find that the compiler turns you away. <code>Span</code> is a <code>ref struct</code>, which means that it can only be stored on the stack. You can't use a <code>Span</code> as a field of a <code>class</code>, or put it in an array, or box it (by upcasting it to an <code>interface</code> or <code>object</code>), or use it as a local variable in an <code>async</code> or <code>yield</code> method (because behind the scenes such methods copy their stack frame to the heap). (There are good reasons for this restriction, pertaining to memory safety.)</p>
<p>How to implement <code>IParseState</code> without storing a <code>Span</code> on the heap? Because <code>IParseState</code> is an internal interface, I can make certain guarantees about its usage. Instances of <code>IParseState</code> have a limited life-span — each <code>IParseState</code> instance becomes garbage before the call to <code>Parse</code> returns, and <code>IParseState</code> instances are never accessed from multiple threads. So I can store the <code>Span</code> in <code>Parse</code>'s stack frame and put a pointer to that stack frame in the <code>IParseState</code>.</p>
<p>This idea is complicated by the fact that <code>Span</code> is a <a href="https://stackoverflow.com/questions/42154908">managed type</a>, so you're not allowed to declare a <code>Span&lt;T&gt;*</code>. (This restriction ensures that a managed object can't accidentally become garbage while it's still being referred to by an unmanaged pointer; this needn't trouble us here as the <code>Span</code> is guaranteed to be reachable because it's on the stack.)</p>
<p>Fortunately, <a href="https://www.nuget.org/packages/System.Runtime.CompilerServices.Unsafe/">the <code>System.Runtime.CompilerServices.Unsafe</code> package</a> contains some dangerous tools to get around this memory safety restriction: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe.aspointer"><code>Unsafe.AsPointer</code></a> coerces a <code>ref</code> to an untyped <code>void*</code> and <a href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe.asref"><code>Unsafe.AsRef</code></a> coerces it back. So my plan was to store a <code>ref Span&lt;T&gt;</code> as a <code>void*</code> on the heap, and coerce it back to a <code>ref Span&lt;T&gt;</code> using  when I need to address it as a <code>Span</code>. (This is a similar hack to one that I used in <a href="/posts/2018-03-16-eighty.html">my HTML generation library</a>.)</p>
<p>Unfortunately this doesn't work either, because <code>Unsafe.AsPointer</code> is generic and you can't use a <code>ref struct</code> as a type parameter! The compiler can't be sure that <code>Unsafe.AsPointer</code> doesn't box its argument, which of course is forbidden when the argument is a <code>Span</code>.</p>
<p>I wound up implementing a type-specialised copy of those two <code>Unsafe</code> methods:</p>
<pre><code class="language-csharp">static class Unsafe
{
    public static void* AsPointer&lt;T&gt;(ref Span&lt;T&gt; span);
    public static ref Span&lt;T&gt; AsRef(void* ptr);
}
</code></pre>
<p><code>AsPointer</code> and <code>AsRef</code> are not expressible in C# (even <code>unsafe</code> C#); <code>System.Runtime.CompilerServices.Unsafe</code> is implemented in raw IL. My copies of those methods have stubs in C# which are filled in by rewriting the DLL's IL in a post-compile step.</p>
<h2 id="assorted-performance-improvements"><a href="#assorted-performance-improvements">Assorted Performance Improvements

</a></h2>
<p>Performance has always been one of Pidgin's priorities — I'm proud that Pidgin is C#'s fastest parser combinator library (that I know of!) — but there's always room for improvement. In my tests Pidgin still runs somewhat slower than F#'s FParsec library, for example. In this release I made some architectural changes to <code>IParseState</code> to help close that gap.</p>
<h3 id="buffering-uniformly">[Buffering Uniformly](#buffering-uniformly</h3>
<p>As I mentioned earlier, Pidgin has several internal implementations of <code>IParseState</code>, each of which implements a streaming abstraction on top of a different type of input. Parsers may need to <em>backtrack</em> on failure (using the <code>Try</code> combinator), so you can't always discard a token as soon as you've seen it. Some <code>IParseState</code> implementations — specifically the ones that are built on top of streaming storage (like <code>Stream</code>) — therefore <em>buffer</em> their input into an array. The ones that are backed by in-memory storage like <code>string</code> don't need to buffer because their data is already in memory.</p>
<p>I decided to move the buffering logic to a shared part of the code. Now <em>all</em> <code>IParseState</code> implementations buffer their input, even the in-memory ones. On its own this should make the code slower (it's more expensive to copy a <code>string</code> into an array than not to!), but it enables all of the optimisations I'm about to describe.</p>
<h3 id="de-virtualisation--inlining">[De-Virtualisation &amp; Inlining](#de-virtualisation--inlining</h3>
<p>Virtual method calls are comparatively expensive in .NET, and they happen frequently in Pidgin's implementation because of <code>IParseState</code>'s design: a parser which consumes <em>n</em> tokens makes a minimum of <em>n</em> virtual calls to <code>Advance</code>.</p>
<p>With the buffering code extracted into a single place, there was no longer any need to keep it behind an interface. I moved all the buffering and error handling code into a mutable struct which is stored on the stack and passed to the <code>Parser</code>s by reference. Now <code>Advance</code> and <code>Peek</code> are non-virtual.</p>
<pre><code class="language-csharp">struct ParseState&lt;TToken&gt;
{
    private TToken[] _buffer;
    private int _currentPosition;
    private ITokenStream&lt;TToken&gt; _stream;

    public TToken? Peek() =&gt;
        _currentPosition &lt; 0 || _currentPosition &gt;= _buffer.Length
            ? null
            : _buffer[_currentPosition];
    
    public void Advance()
    {
        _currentPosition++;

        if (_currentPosition &gt;= _buffer.Length)
        {
            ReadStreamIntoBuffer();
        }
    }
}

public abstract class Parser&lt;TToken, T&gt;
{
    internal abstract Result&lt;T&gt; Parse(ref ParseState&lt;TToken&gt; state);
}
</code></pre>
<p>That's a fairly general C# performance trick: put shared state in a mutable struct, store it on the stack, and pass it around as a <code>ref</code> parameter. This both reduces GC pressure and improves the CPU cache's hit rate, because the execution stack is likely to be in cache.</p>
<p>I also noticed that <code>Peek</code> was not <em>inlining</em> well. Inlining is an important optimisation carried out by the JIT compiler: if you call a method that's short and simple enough, the compiler will just copy the method's body into the current method instead of emitting code to call it. This can often enable further optimisations such as erasing array bounds checks.</p>
<p>I split <code>Peek</code> up into a pair of properties:</p>
<pre><code class="language-csharp">struct ParseState&lt;TToken&gt;
{
    private TToken[] _buffer;
    private int _currentPosition;

    public bool HasCurrent =&gt; _currentPos &gt;= 0 &amp;&amp; _currentPos &lt; _buffer.Length;
    public TToken Current =&gt; _buffer[_currentPosition];
}
</code></pre>
<p>These properties are short and non-virtual, so they are good inlining candidates. After inlining them, the JIT sees simple array-manipulating code, which it can easily optimise in the context of the containing method. This simple change resulted in a 15-20% performance improvement across the board.</p>
<h3 id="chunking">[Chunking](#chunking</h3>
<p>The <code>Stream</code> version of <code>Advance</code> called <code>stream.Read()</code>, which reads and returns a single byte from the stream. This is not as efficient as <code>stream.Read(byte[])</code>, which reads a chunk of bytes from the stream and copies them into an array. (The former requires <em>n</em> virtual method calls to read <em>n</em> bytes, whereas the latter requires only <em>n / <code>array.Length</code></em>.)</p>
<p>I replaced <code>ITokenStream</code>'s <code>Advance</code> and <code>Peek</code> methods with a method which reads a chunk of tokens into the <code>ParseState</code>'s buffer.</p>
<pre><code class="language-csharp">interface ITokenStream&lt;TToken&gt;
{
    void ReadInto(TToken[] buffer, int start, int count);
}
</code></pre>
<p>Why not design this signature as <code>void ReadInto(Span&lt;TToken&gt; span)</code>? I'd love to! Sadly that would preclude some implementations of <code>ITokenStream</code>. Methods like <code>stream.Read(Span&lt;byte&gt; span)</code> are available only on .NET Core, and as far as I'm aware Microsoft has no plans to backport them, so for compatibility with the desktop framework I'm stuck using arrays. This is irksome, as <a href="/posts/2018-12-06-zooming-in-on-field-accessors.html">I've said before</a> — what's the point of designing a feature for library authors if you're not going to support it properly?</p>
<p><code>ReadInto</code> allows the <code>ParseState</code> to fill its buffer in chunks. But <code>ParseState</code>'s interface can also be chunk-ified — some <code>Parser</code>s (like <code>String</code>) can predict how many characters they'll pull from the input. I added a <code>Peek</code> method to <code>ParseState</code> which returns a view into the <code>ParseState</code>'s buffer. (This makes <code>Peek</code> a little tricky to use correctly — you have to be careful not to continue using the <code>Span</code> after a call to <code>Advance</code>, which may mutate the buffer.)</p>
<pre><code class="language-csharp">public ReadOnlySpan&lt;TToken&gt; Peek(int count);
</code></pre>
<p>Using <code>Peek</code>, a parser like <code>String(&quot;foo&quot;)</code> can now look at three characters from the input to see if they match <code>foo</code>, rather than one at a time.</p>
<pre><code class="language-csharp">class StringParser : Parser&lt;char, string&gt;
{
    private readonly string _expected;
    internal override Result&lt;string&gt; Parse(ref ParseState&lt;char&gt; state)
    {
        var span = state.Peek(_expected.Length);
        if (!_expected.AsSpan().SequenceEqual(span))
        {
            return Result.Failure();
        }
        return Result.Success(_expected);
    }
}
</code></pre>
<p>This runs about 20% faster for long strings.</p>
<h2 id="make-your-own-opportunities"><a href="#make-your-own-opportunities">Make Your Own Opportunities

</a></h2>
<p>Indulge me for a moment while I dispense some unsolicited career advice. For a long time, performance engineering was something that <a href="https://twitter.com/marcgravell/">other</a>, <a href="https://twitter.com/davidfowl">cleverer</a> <a href="https://mattwarren.org/">people</a> did while I observed from a distance with awe. Why is ASP.NET fast? Because the guys that wrote it are Actual Wizards.</p>
<p>I've been lucky enough to work with some of those clever people for a few years now, and actually they're not Actual Wizards. They just have more experience than me — they know how to use and interpret profiles and benchmarks, they've seen enough to know what does and doesn't work, and they've had practice coming up with ideas to improve performance.</p>
<p>The good news is that performance optimisation usually doesn't involve fiddly low-level programming like this — simply being aware of when your code performs IO makes a big difference — and it's never too early or late to start building your experience level, no matter where you are in your career. For me, that meant setting myself the goal of making my open source libraries as fast as I could. For you, it could mean picking a slow part of your codebase at work and trying to make it faster, or pair-programming with a colleague, or thoroughly reading through some unfamiliar code. Making your own opportunities to learn new things is the best way to get better.</p>

]]></summary>
</entry>
<entry>
    <title>The Fourth Type of Variance</title>
    <link href="http://www.benjamin.pizza/posts/2019-01-11-the-fourth-type-of-variance.html" />
    <id>http://www.benjamin.pizza/posts/2019-01-11-the-fourth-type-of-variance.html</id>
    <published>2019-01-11T00:00:00Z</published>
    <updated>2019-01-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>Given a polymorphic type, like <code>List</code>, what can we say about the relationship between different usages of that type? If <code>A</code> and <code>B</code> are related, is <code>List[A]</code> related to <code>List[B]</code>? <em>Variance</em> is the word for this type of relationship, and it turns out there are a few different answers to that question, depending on the type you're asking about.</p>
<h2 id="covariance"><a href="#covariance">Covariance

</a></h2>
<p>Probably the most familiar situation is when the parameterised types are related in the same way as the parameter. This is the type of variance exhibited by most &quot;container&quot; types, like <code>List</code>.</p>
<pre><code class="language-scala">sealed abstract class List[+A]

val cats : List[Cat] = List(Cat(&quot;Tilly&quot;), Cat(&quot;Charlie&quot;), Cat(&quot;Izzy&quot;))
val animals : List[Animal] = cats
</code></pre>
<p><code>List</code>'s parameter <code>A</code> is annotated with <code>+</code>, so it'll be treated as covariant. This allows you to use a <code>List[Cat]</code> any time you need a <code>List[Animal]</code>. A list of <code>Cat</code>s is a list of <code>Animal</code>s, because every <code>Cat</code> is an <code>Animal</code>. The subtype relationship of the container goes in the same direction as the subtype relationship of the elements. (In C# <code>+</code> is pronounced <code>out</code>, as in <code>IEnumerable&lt;out T&gt;</code>.)</p>
<p>Variance is visible even in non-subtyping-based languages. Haskellers'll be familiar with <em>covariant functors</em>. It's the type of functor exhibited the the standard <code>Functor</code> class.</p>
<pre><code class="language-haskell">class Functor f where
    fmap :: (a -&gt; b) -&gt; f a -&gt; f b

instance Functor [] where
    fmap f xs = [f x | x &lt;- xs]
</code></pre>
<p>This chimes with the intuition that a functor <code>f</code> is a container of sorts. If you can write a function to convert each <code>a</code> in the container into a <code>b</code>, then <code>fmap</code> can convert a container of <code>a</code>s into a container of <code>b</code>s by converting each item in the container.</p>
<p>In general, a type is covariant if its parameter is used as an output. An object which produces <code>Cat</code>s can be used to produce <code>Animal</code>s. All you have to do is ignore the cattiness of the animals you get out of the producer.</p>
<h2 id="contravariance"><a href="#contravariance">Contravariance

</a></h2>
<p><em>Contravariance</em>, covariance's evil twin, is the word for when the parameterised types are related in the opposite way as the parameters. Scala's <code>Ordering</code>, which determines which way round to put two objects (like C#'s <code>IComparer</code>), is an example of a contravariant type.</p>
<pre><code class="language-scala">trait Ordering[-A] {
    def apply(x : A, y : A) : Int
}
val animalOrdering : Ordering[Animal] = Ordering.by[Animal, Int](x =&gt; x.cuteness)
val catOrdering : Ordering[Cat] = animalOrdering
</code></pre>
<p>The <code>-</code> symbol denotes a contravariant parameter, allowing us to use an <code>Ordering[Animal]</code> as an <code>Ordering[Cat]</code>. (C#ers say <code>in</code>, as in <code>IComparer&lt;in T&gt;</code>.) If you know how to compare two <code>Animal</code>s (perhaps by comparing their cuteness), you can certainly compare two <code>Cat</code>s. The subtype relationship of the comparers goes in the opposite direction to that of the parameters.</p>
<p>The class of contravariant functors in Haskell is just like <code>Functor</code> but with the direction of one of the arrows flipped.</p>
<pre><code class="language-haskell">class Contravariant f where
    contramap :: (b -&gt; a) -&gt; f a -&gt; f b

newtype Comparer a = Comparer (a -&gt; a -&gt; Ord)

instance Contravariant Comparer where
    contramap f (Comparer p) = Comparer (\x y -&gt; p (f x) (f y))
</code></pre>
<p>If you can turn <code>b</code>s into <code>a</code>s, then you can turn a comparer of <code>a</code>s into a comparer of <code>b</code>s by converting the <code>b</code>s into <code>a</code>s before they go into the comparer. Note how <code>f</code> is applied to <code>p</code>'s inputs in the implementation of <code>contramap</code>.</p>
<p>In general, a type is contravariant if its parameter appears as an input. An object which consumes <code>Animals</code> can be used to consume <code>Cat</code>s. All you have to do is forget about the cattiness of the animals before you put them into the consumer.</p>
<p>Julie Moronuki has <a href="https://typeclasses.com/contravariance">the best explanation of contravariance</a> that I know of.</p>
<h2 id="invariance"><a href="#invariance">Invariance

</a></h2>
<p><em>Invariance</em> is the word for when there's no relationship at all between different usages of a parameterised type.</p>
<p>In Scala a type parameter unadorned with a sign is invariant. The following mutable set type is invariant:</p>
<pre><code class="language-scala">trait Set[A] {
    // A appears as both an input and an output
    def add(item: A): Unit
    def remove(item: A): Unit
    def contains(item: A): Boolean
    def items(): Iterable[A]
}
</code></pre>
<p>In general, a type is invariant if its parameter appears as both an input and an output. You can't use <code>Set</code> covariantly, because <code>A</code> appears as an input to <code>contains</code>, and you can't use it contravariantly because <code>A</code> appears in <code>items</code>'s output. There's no subtyping relationship between the parameter and the type. A <code>Set[Cat]</code> is not a <code>Set[Animal]</code>. If it was, you'd be allowed to upcast it and then call <code>add</code> with a <code>Dog</code>:</p>
<pre><code class="language-scala">val catSet = new Set[Cat](Cat(&quot;Tilly&quot;))
val animalSet: Set[Animal] = catSet

animalSet.add(Dog(&quot;Richard&quot;))
for (cat: Cat &lt;- catSet.items()) {}  // uh oh, one of the cats will actually be a dog!
</code></pre>
<p>The same logic applies to the opposite situation. A <code>Set[Animal]</code> is not a <code>Set[Cat]</code>.</p>
<p>Here's a Haskell class defining <code>Invariant</code> functors.</p>
<pre><code class="language-haskell">class Invariant f where
    invmap :: (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b
</code></pre>
<p>You have to be able to map <code>a</code>s and <code>b</code>s in both directions to convert an invariant functor. This implies that the functor both consumes and produces <code>a</code>s: you map items on the way out and on the way in.</p>
<pre><code class="language-haskell">newtype Operation a = Operation (a -&gt; a -&gt; a)

instance Invariant Operation where
    invmap f g (Operation op) = Operation (\x y -&gt; f (g x `op` g y))
</code></pre>
<p>Note how we use <code>f</code> on the output of <code>op</code> and <code>g</code> on the inputs.</p>
<p>The only time I've actually seen this class used is in <a href="http://comonad.com/reader/2008/rotten-bananas/">Ed Kmett's old article about attempting to represent higher-order abstract syntax generically</a>.</p>
<p>Let me spell out the similarity between <code>Invariant</code> functors and Scala's subtype invariance. For <code>Operation a</code> to be convertible to <code>Operation b</code>, <code>a</code> must be convertible to <code>b</code> <em>and</em> <code>b</code> must be convertible to <code>a</code>. For <code>Set[A]</code> to be a subtype of <code>Set[B]</code>, <code>A</code> must be a subtype of <code>B</code> <em>and</em> <code>B</code> must be a subtype of <code>A</code> (that is, they must be the same type).</p>
<p>Note that variance is a property of the type parameter (<code>A</code>), not the type constructor (<code>List</code>/<code>Ordering</code>). A given type constructor may have multiple parameters with different variances. <code>Function1[-A, +B]</code>, for example.</p>
<img src="/images/2019-01-11-the-fourth-type-of-variance/hierarchy.png" />
<h2 id="combining-variances"><a href="#combining-variances">Combining Variances

</a></h2>
<p>An object which produces a producer of <code>A</code>s effectively produces <code>A</code>s. A type with a covariant type as an output is itself covariant.</p>
<pre><code class="language-scala">// Container returns a covariant type, so Container is covariant
trait Container[+A] {
    def toList(): List[A]
}
</code></pre>
<p>Consuming a producer of <code>A</code>s is basically the same as consuming <code>A</code>s. A type which has a covariant type as an input is contravariant.</p>
<pre><code class="language-scala">// Printer consumes a covariant type, so it's contravariant
trait Printer[-A] {
    def printAll(items: List[A]): Unit
}
</code></pre>
<p>Producing a consumer of <code>A</code>s is like consuming <code>A</code>s. A type with a contravariant type as an output is contravariant.</p>
<pre><code class="language-scala">// Produces a contravariant type, so contravariant
trait OrderingFactory[-A] {
    def getOrdering(): Ordering[A]
}
</code></pre>
<p>A consumer of consumers is itself a producer. (You have to be able to produce <code>A</code>s in order to feed them to the consumer.) A type with a contravariant type as an input is covariant.</p>
<pre><code class="language-scala">// Consumes a contravariant type, so covariant
trait Sortable[+A] {  
    def sortBy(ordering: Ordering[A]): Unit
}
</code></pre>
<p>Mnemonically, you can think of input parameters as meaning &quot;times -1&quot;. <code>Ordering</code> takes <code>A</code>s as its inputs, so <code>Ordering</code> is negative. <code>Sortable</code> takes a (negative) <code>Ordering</code> as an input, so it's positive (-1 * -1 = 1). Printer takes a (positive) <code>List</code> as input, so it's negative. This explains Scala's choice of <code>+</code> and <code>-</code> as the syntax for its variance annotations.</p>
<h2 id="the-semilattice-of-variances"><a href="#the-semilattice-of-variances">The Semilattice of Variances

</a></h2>
<p>Now, it turns out that these three types of variance have a relationship to each other. Invariance generalises both covariance and contravariance. Covariant things are also invariant, and contravariant things are also also invariant.</p>
<pre><code class="language-haskell">defaultInvmapCo :: Functor f =&gt; (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b
defaultInvmapCo f _ x = fmap f x

defaultInvmapContra :: Contravariant f =&gt; (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b
defaultInvmapContra _ g x = contramap g x
</code></pre>
<p>If I was in the business of redesigning Haskell's libraries, I'd even consider making <code>Invariant</code> a superclass of <code>Functor</code> and <code>Contravariant</code>.</p>
<pre><code class="language-haskell">class Invariant f where {- ... -}

class Invariant f =&gt; Functor f where {- ... -}

class Invariant f =&gt; Contravariant f where {- ... -}
</code></pre>
<img src="/images/2019-01-11-the-fourth-type-of-variance/semilattice.jpg" />
<p>So there's this interesting relationship between the three types of variance. They form a little semilattice, of which <code>Invariant</code> is the supremum.</p>
<p>But, hmm, the picture seems asymmetric. Is variance really only a semilattice? Or is there something lurking at the bottom of that picture?</p>
<h2 id="phantom-variance"><a href="#phantom-variance">Phantom Variance

</a></h2>
<p>Looking at the code above, it appears that <code>Functor</code> and <code>Contravariant</code> both specialise <code>Invariant</code> by ignoring one of <code>Invariant</code>'s function parameters. What if we ignored both of them?</p>
<pre><code class="language-haskell">class (Functor f, Contravariant f) =&gt; Phantom f where
    pmap :: f a -&gt; f b
</code></pre>
<p>This strange class says that you can map an <code>f a</code> to an <code>f b</code> without needing to map <code>a</code>s or <code>b</code>s at all! Intuitively, you can only convert <code>f a</code> to <code>f b</code> for free when <code>f</code> doesn't mention <code>a</code> anywhere in its body.</p>
<p>A functor is <code>Invariant</code> when it has <code>a</code>s both as inputs and outputs. <code>Functor</code> specialises <code>Invariant</code> by promising that <code>f</code> doesn't have any input <code>a</code>s, so all you need to do is map the outputs. <code>Contravariant</code> specialises <code>Invariant</code> by promising that there are no output <code>a</code>s and all you need to do is map the inputs. <code>Phantom</code>, being a special case of both covariance and contravariance, guarantees that there are no <code>a</code>s at all in the <code>f</code>.</p>
<p>So the four types of variance form a nice lattice.</p>
<img src="/images/2019-01-11-the-fourth-type-of-variance/lattice.jpg" />
<p>For completeness, here's the proof that the superclass constraints make sense:</p>
<pre><code class="language-haskell">defaultFmap :: Phantom f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
defaultFmap _ = pmap

defaultContramap :: Phantom f =&gt; (b -&gt; a) -&gt; f a -&gt; f b
defaultContramap _ = pmap
</code></pre>
<p>Phantom types show up every now and then in Haskell. They're used to decorate ordinary values with additional type-level information, either to layer on additional type safety or to give GHC a hint for type inference.</p>
<pre><code class="language-haskell">data Proxy a = Proxy  -- from Data.Proxy
instance Phantom Proxy where
    pmap _ = Proxy
</code></pre>
<p>Haskell is the only language I know of with proper support for phantom types, in its <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#roles">role system</a>. (<code>Phantom</code> roughly means <code>forall a b. Coercible (f a) (f b)</code>.) Scala doesn't support it, but it'd mean that a type is always a subtype of any other instantiation of that type, even if the type arguments have no relationship.</p>
<pre><code class="language-scala">case class Proxy[±A]  // fantasy syntax

val catProxy = Proxy[Cat]()
val dogProxy : Proxy[Dog] = catProxy
</code></pre>
<p><code>Proxy[A]</code> is always a subtype of <code>Proxy[B]</code> (and vice versa!), even when <code>A</code> and <code>B</code> are nothing to do with each other. To a certain extent this defeats the purpose of phantom types. It also breaks antisymmetry — two different types can both be a subtype of each other — so subtyping is no longer a partial order. As a language feature, phantom variance probably isn't actually all that desirable.</p>

]]></summary>
</entry>
<entry>
    <title>Zooming In on Field Accessors</title>
    <link href="http://www.benjamin.pizza/posts/2018-12-06-zooming-in-on-field-accessors.html" />
    <id>http://www.benjamin.pizza/posts/2018-12-06-zooming-in-on-field-accessors.html</id>
    <published>2018-12-06T00:00:00Z</published>
    <updated>2018-12-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>It's common in functional languages — and increasingly in hybrid languages like C# — to work with complex systems of immutable datatypes. For a contrived example, suppose you're working on a billing application:</p>
<pre><code class="language-csharp">class Order
{
    public Customer Customer { get; }
    public ImmutableList&lt;Product&gt; Products { get; }
}
class Customer
{
    public string Name { get; }
    public Address Address { get; }
}
class Address
{
    public string Line1 { get; }
    public string Line2 { get; }
    public string Postcode { get; }
}
class Product
{
    public string Title { get; }
    public decimal Price { get; }
}
</code></pre>
<img src="/images/2018-12-06-zooming-in-on-field-accessors/model.jpg" />
<p>(I've omitted the constructors; you can imagine your own.) These objects are immutable, meaning you can't modify them directly. The way to update an immutable object is to make a copy of that object with the relevant properties changed. This turns out to be surprisingly tedious when you're working inside a deeply nested structure:</p>
<pre><code class="language-csharp">Order UpdatePostcode(Order order/*👨🏻‍⚖️*/, string newPostcode)
    =&gt; new Order(
        new Customer(
            order.Customer.Name,
            new Address(
                order.Customer.Address.Line1,
                order.Customer.Address.Line2,
                newPostcode
            )
        ),
        order.Products
    );
</code></pre>
<p>F#'s <code>with</code>-syntax helps a little, but not a lot. You still have to write each name multiple times.</p>
<pre><code class="language-fsharp">let updatePostcode(order : Order, newPostcode : string) : Order = {
  order with customer = {
    order.customer with address = {
      order.customer.address with postcode = newPostcode
    }
  }
}
</code></pre>
<p>This pain is caused by immutability. In an imperative setting you can say <code>order.Customer.Address.Postcode = newPostcode;</code>, although of course mutable data is less reliable and harder to work with overall. Rather than give up on immutability, functional programmers have invented a remarkable family of composable tools called <em>optics</em> for poking around inside complex datatypes. Optics are a way of describing &quot;paths&quot; through structures: you can compose paths together, and read and write the values at the end of those paths. I'm here today to demonstrate that C#8's upcoming <a href="https://github.com/dotnet/csharplang/issues/52"><em>default interface methods</em></a> are great at modelling optics.</p>
<p>Let's start with <em>lenses</em>, the family member that gave the family its name.</p>
<h2 id="lenses"><a href="#lenses">Lenses

</a></h2>
<p>A <em>lens</em> is a first-class property for an immutable object. It's an object with a pair of methods, a <em>getter</em> which retrives the value of a property and a <em>setter</em> which updates it. Remember, we're working with immutable data, so the setter returns a new copy of the object.</p>
<pre><code class="language-csharp">interface ILens&lt;T, TProp&gt;
{
    TProp Get(T obj);
    T Set(T oldObj, TProp newVal);
}
</code></pre>
<p>For example, here's a lens which focuses on an <code>Order</code>'s <code>Customer</code>.</p>
<pre><code class="language-csharp">class CustomerL : ILens&lt;Order, Customer&gt;
{
    public Customer Get(Order o) =&gt; o.Customer;

    public Order Set(Order o, Customer c) =&gt; new Order(c, o.Products);
}
</code></pre>
<img src="/images/2018-12-06-zooming-in-on-field-accessors/customer.jpg" />
<p>So a lens picks out a single property inside a given object.</p>
<p>The power of lenses comes from their composability. Given a lens identifying a <code>T2</code> inside a <code>T1</code> (<code>ILens&lt;T1, T2&gt;</code>) and a lens identifying a <code>T3</code> inside a <code>T2</code> (<code>ILens&lt;T2, T3&gt;</code>), you can compose those lenses together to focus all the way from the <code>T1</code> to the <code>T3</code>.</p>
<p>You can traverse any relationship in your data model by composing together a small number of individual lenses. Composing lenses is so important that I've given it the shortest name I can think of: <code>_</code>. (Readers of <a href="2018-03-16-eighty.html">an earlier post of mine</a> will know of my fondness for <code>_</code>.)</p>
<img src="/images/2018-12-06-zooming-in-on-field-accessors/postcode.jpg" />
<p>Compare this terse, declarative code with the tedious version of <code>UpdatePostcode</code> from the beginning:</p>
<pre><code class="language-csharp">Order UpdatePostcode(Order order/*👨🏻‍⚖️*/, string newPostcode)
{
    ILens&lt;Order, string&gt; l = new CustomerL()
        ._(new AddressL())
        ._(new PostcodeL());
    
    return l.Set(order, newPostcode);
}
</code></pre>
<p>Lenses work without reference to any particular instance. (This is called ~~<em>pointless</em>~~ <a href="https://stackoverflow.com/questions/944446/what-is-point-free-style-in-functional-programming"><em>point-free</em></a> programming.) <code>order.Customer.Address.Postcode</code> becomes <code>new CustomerL()._(new AddressL())._(new PostcodeL())</code> — the path of properties is sort of detached from the object itself. Treating a path through a datatype as a first class value is the big idea behind lenses.</p>
<p>Here's how <code>_</code> is implemented. It returns a new <code>ILens</code> (an instance of a private class) which delegates to the two smaller lenses.</p>
<pre><code class="language-csharp">static class LensExtensions
{
    public static ILens&lt;T1, T3&gt; _&lt;T1, T2, T3&gt;(this ILens&lt;T1, T2&gt; l1, ILens&lt;T2, T3&gt; l2)
        =&gt; new ComposedLens&lt;T1, T2, T3&gt;(l1, l2);

    private class ComposedLens&lt;T1, T2, T3&gt; : ILens&lt;T1, T3&gt;
    {
        private readonly ILens&lt;T1, T2&gt; _l1;
        private readonly ILens&lt;T2, T3&gt; _l2;

        public T3 Get(T1 obj) =&gt; _l2.Get(_l1.Get(obj));

        public T1 Set(T1 oldObj, T3 newVal)
            =&gt; _l1.Set(oldObj, _l2.Set(_l1.Get(oldObj), newVal));
    }
}
</code></pre>
<p>To summarise, a lens is a way to focus on a small part of a big immutable structure. They're like a first-class version of the <code>.</code> and <code>=</code> operators: you can compose lenses together to focus deeper, and upon focusing on a location you can get and set the value at that location.</p>
<h3 id="mapping-under-a-lens">[Mapping under a lens](#mapping-under-a-lens</h3>
<p>A common pattern is to get a value from a lens, apply some sort of transformation to it, and then put it back where it was. The <code>Map</code> helper function wraps up this pattern:</p>
<pre><code class="language-csharp">interface ILens&lt;T, TProp&gt;
{
    TProp Get(T obj);
    T Set(T oldObj, TProp newVal);

    T Map(T oldObj, Func&lt;TProp, TProp&gt; transformer)
        =&gt; Set(oldObj, transformer(Get(oldObj))).
}
</code></pre>
<p>Here's an early taste of a default interface implementation. The default, <em><code>Get</code>-then-<code>Set</code></em>, works correctly, but when you're working with deeply stacked lenses it can be inefficient to walk the whole data structure twice. (This is especially true of <em>multi-lenses</em> — of which more later — which build and discard a large number of intermediate enumerables.) If <code>Map</code> were an extension method, it would be impossible for users to override it and provide a more efficient implementation.</p>
<hr />
<p>Powerful as they are, these lenses don't quite scale up to cover all of the important ways to access data. Specifically, they don't support computed properties or lists.</p>
<h2 id="getters"><a href="#getters">Getters

</a></h2>
<p>How would you write a lens which focuses on a list's <code>Count</code>? You can't set <code>Count</code> directly - it measures the number of times you've added or removed something from the list. The only way to change the <code>Count</code> is to add or remove an item!</p>
<pre><code class="language-csharp">class CountL&lt;T&gt; : ILens&lt;ImmutableList&lt;T&gt;, int&gt;
{
    public int Get(ImmutableList&lt;T&gt; l) =&gt; l.Count;
    public ImmutableList&lt;T&gt; Set(ImmutableList&lt;T&gt; l, int count) =&gt; /* ??? */;
}
</code></pre>
<p>Clearly we need to separate the &quot;getting&quot; and &quot;setting&quot; reponsibilities of <code>ILens</code>.</p>
<pre><code class="language-csharp">interface IGetter&lt;T, TProp&gt;
{
    TProp Get(T obj);
}
interface ILens&lt;T, TProp&gt; : IGetter&lt;T, TProp&gt;
{
    T Set(T oldObj, TProp newVal);
}
</code></pre>
<p>We don't lose composability by doing this. You can still compose two getters to get a getter.</p>
<pre><code class="language-csharp">static class LensExtensions
{
    public static IGetter&lt;T1, T3&gt; _(
        this IGetter&lt;T1, T2&gt; g1,
        IGetter&lt;T2, T3&gt; g2
    ) =&gt; new ComposedGetter&lt;T1, T2, T3&gt;(g1, g2);

    private class ComposedGetter&lt;T1, T2, T3&gt; : IGetter&lt;T1, T3&gt;
    {
        private readonly IGetter&lt;T1, T2&gt; _g1;
        private readonly IGetter&lt;T2, T3&gt; _g2;

        public T3 Get(T1 obj) =&gt; _g2.Get(_g1.Get(obj));
    }
}
</code></pre>
<p>If you compose a lens with a getter, you get a getter. This makes sense: if any part of a given path through a data structure is read-only, then the whole path must be read-only. It Just Works™ because <code>ILens</code> is a subtype of <code>IGetter</code>. Overload resolution takes care of it: you type <code>_</code> and the compiler picks the right return type based on the types of <code>_</code>'s arguments.</p>
<h2 id="multi-lenses"><a href="#multi-lenses">Multi-lenses

</a></h2>
<p><code>ILens</code> focuses on a single part of a structure. Its <code>Get</code> method returns a single <code>TProp</code> and its <code>Set</code> method takes a single <code>TProp</code>. This means you can't use lenses to, for example, update the price of all the products in an order.</p>
<p>Enter <em>multi-lenses</em>, also known as <em>traversals</em>.</p>
<pre><code class="language-csharp">interface IMultiLens&lt;T, TProp&gt;
{
    IEnumerable&lt;TProp&gt; MultiGet(T obj);
    // newVals should be the same length as the list returned by MultiGet
    T MultiSet(T oldObj, IEnumerable&lt;TProp&gt; newVals);

    T Map(T oldObj, Func&lt;TProp, TProp&gt; transformer)
        =&gt; MultiSet(oldObj, MultiGet(oldObj).Select(transformer)).
}
</code></pre>
<p>(Readers of <a href="2017-11-13-recursion-without-recursion.html">an earlier post</a> might recognise <code>IMultiLens</code> as a generalisation of <code>IRewriter</code>.) A multi-lens is like a lens which can hit more than one target. While a lens focuses on <em>exactly one</em> <code>TProp</code> inside a <code>T</code>, a multi-lens relaxes that restriction, focusing on <em>zero-or-many</em> <code>TProps</code> at once.</p>
<p>Here's an example multi-lens which focuses on all of the <code>Product</code>s in an <code>Order</code>.</p>
<pre><code class="language-csharp">class ProductsL : IMultiLens&lt;Order, Product&gt;
{
    IEnumerable&lt;Product&gt; MultiGet(Order order) =&gt; order.Products;

    Order MultiSet(Order order, IEnumerable&lt;Product&gt; newProducts)
        =&gt; new Order(order.Customer, newProducts);
}
</code></pre>
<img src="/images/2018-12-06-zooming-in-on-field-accessors/products.jpg" />
<p>You can compose multi-lenses, too. If you have a multi-lens which finds $n$ <code>T2</code>s inside a <code>T1</code>, and a second multi-lens which finds $m$ <code>T3</code>s inside a <code>T2</code>, you can build a multi-lens which finds $nm$ <code>T3</code>s inside a <code>T1</code>. This works by looking through the second multi-lens at all $n$ of the first multi-lens's targets.</p>
<pre><code class="language-csharp">static class LensExtensions
{
    public static IMultiLens&lt;T1, T3&gt; _(
        this IMultiLens&lt;T1, T2&gt; m1,
        IMultiLens&lt;T2, T3&gt; m2
    ) =&gt; new ComposedMultiLens&lt;T1, T2, T3&gt;(m1, m2);

    private class ComposedMultiLens&lt;T1, T2, T3&gt; : IMultiLens&lt;T1, T3&gt;
    {
        private readonly IMultiLens&lt;T1, T2&gt; _m1;
        private readonly IMultiLens&lt;T2, T3&gt; _m2;

        public IEnumerable&lt;T3&gt; MultiGet(T1 obj)
            =&gt; _m1.MultiGet(obj).SelectMany(_m2.MultiGet);

        public T1 MultiSet(T1 oldObj, IEnumerable&lt;T3&gt; newVals)
        {
            IEnumerable&lt;T2&gt; NewT2s()
            {
                foreach (var x in _m1.MultiGet(oldObj))
                {
                    var chunkLength = _m2.MultiGet(x).Count();
                    yield return _m2.MultiSet(x, newVals.Take(chunkLength));
                    newVals = newVals.Skip(chunkLength);
                }
            }
            return _m1.MultiSet(oldObj, NewT2s());
        }
    }
}
</code></pre>
<p><code>MultiSet</code> chops <code>newVals</code> into chunks that are the length of each group of descendants. This is safe as long as a user never calls <code>MultiSet</code> with a different number of elements than was returned by <code>MultiGet</code>.</p>
<p>So far we can compose multi-lenses on their own, but they don't yet interoperate well with lenses. But note multi-lenses generalise lenses by relaxing the requirement that there should be exactly one substructure. Every lens is also a multi-lens by forgetting that there's a single <code>TProp</code>. (Once again we're relying on the assumption that the list does not change length in between <code>MultiGet</code> and <code>MultiSet</code> calls.)</p>
<pre><code class="language-csharp">interface ILens&lt;T, TProp&gt; : IMultiLens&lt;T, TProp&gt;
{
    TProp Get(T obj);
    T Set(T oldObj, TProp newVal);

    IEnumerable&lt;TProp&gt; MultiGet(T obj)
        =&gt; new[] { Get(obj) };
    T MultiSet(T oldObj, IEnumerable&lt;TProp&gt; newVals)
        =&gt; Set(oldObj, newVals.Single());
}
</code></pre>
<p>Inheriting from <code>IMultiLens</code> like this is just the same trick as inheriting from <code>IGetter</code>. It allows you to compose a lens with a multi-lens using <code>_</code>; the result will be a multi-lens.</p>
<p>If lenses are like a first-class <code>.</code>, then multi-lenses are like a first-class <code>Select</code>. Composing a lens onto the end of a multi-lens is like <code>Select</code>ing a field from each element of a list, with the added power of being able to write new values to the list. Like lenses, multi-lenses are point-free: you compose a multi-lens describing a path through a datatype, then apply that multi-lens to a specific instance of the datatype.</p>
<pre><code class="language-csharp">Order TwentyPercentOff(Order order)
{
    IMultiLens&lt;Order, decimal&gt; l = new ProductsL()._(new PriceL());
    return l.Map(order, x =&gt; x * 0.8);
}
</code></pre>
<img src="/images/2018-12-06-zooming-in-on-field-accessors/product-and-price.jpg" />
<p>Incorporating the earlier <code>IGetter</code> fix, and extending <code>IMultiLens</code> upwards in parallel, leaves us with the following hierarchy.</p>
<pre><code class="language-csharp">interface IMultiGetter&lt;T, TProp&gt;
{
    IEnumerable&lt;TProp&gt; MultiGet(T obj);
}
interface IMultiLens&lt;T, TProp&gt; : IMultiGetter&lt;T, TProp&gt;
{
    T MultiSet(T oldObj, IEnumerable&lt;TProp&gt; newVals);

    T Map(T oldObj, Func&lt;TProp, TProp&gt; transformer)
        =&gt; MultiSet(oldObj, MultiGet(oldObj).Select(transformer));
}
interface IGetter&lt;T, TProp&gt; : IMultiGetter&lt;T, TProp&gt;
{
    TProp Get(T obj);

    IEnumerable&lt;TProp&gt; MultiGet(T obj)
        =&gt; new[] { Get(obj) };
}
interface ILens&lt;T, TProp&gt; : IGetter&lt;T, TProp&gt;, IMultiLens&lt;T, TProp&gt;
{
    T Set(T oldObj, TProp newVal);

    T MultiSet(T oldObj, IEnumerable&lt;TProp&gt; newVals)
        =&gt; Set(oldObj, newVals.Single());
}
</code></pre>
<img src="/images/2018-12-06-zooming-in-on-field-accessors/hierarchy.jpg" />
<h2 id="default-interface-implementations"><a href="#default-interface-implementations">Default Interface Implementations

</a></h2>
<p>The code above makes central use of default interface implementations, so it's probably time to talk about what they are.</p>
<p>In C#8, interfaces won't just be type declarations any more. You'll be allowed to write code in an interface method, to function as the default implementation of that method. Typically it'll be written in terms of the other methods on the interface, like an extension method. They differ from extension methods, however, in that they are virtual. If an implementing class has a better (faster, typically) way of implementing the operation, it's free to override it.</p>
<p>Here's an example. How would LINQ's design look different if default interface methods had been around at the time? Today's <a href="https://docs.microsoft.com/en-us/dotnet/api/system.linq.enumerable.count"><code>Count</code></a> method, an extension method, works in linear time by counting up all of the elements of the input <code>IEnumerable</code>.</p>
<pre><code class="language-csharp">public static int Count&lt;T&gt;(this IEnumerable&lt;T&gt; source)
{
    var count = 0;
    foreach (var _ in source)
    {
        count++;
    }
    return count;
}
</code></pre>
<p>However, there are certain implementations of <code>IEnumerable</code> which can count themselves much faster than that:</p>
<pre><code class="language-csharp">class List&lt;T&gt; : IEnumerable&lt;T&gt;
{
    private T[] _array;
    private int _count;

    // ...

    public int Count =&gt; _count;
}
</code></pre>
<p><a href="https://github.com/dotnet/corefx/blob/master/src/System.Linq/src/System/Linq/Count.cs">The real <code>Count</code> extension method</a> takes a fast path when its argument happens to be an <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.icollection-1?view=netframework-4.7.2"><code>ICollection</code></a>, but that doesn't scale well. Not every <code>IEnumerable</code> which admits a fast <code>Count</code> can also implement <code>ICollection</code> — for example, an immutable collection can't implement the <code>void Add(T item)</code> method.</p>
<p>If LINQ had been designed not as a collection of extension methods but as a collection of default interface methods, it'd be possible to override <code>Count</code> in an extensible way:</p>
<pre><code class="language-csharp">interface IEnumerable&lt;T&gt;
{
    IEnumerator&lt;T&gt; GetEnumerator();

    int Count()
    {
        var count = 0;
        foreach (var _ in source)
        {
            count++;
        }
        return count;
    }
    // other methods like Select etc
}

class List&lt;T&gt; : IEnumerable&lt;T&gt;
{
    private T[] _array;
    private int _count;
    
    // ...

    // override the default version from IEnumerable
    public int Count() =&gt; _count;
}
</code></pre>
<p>Interfaces with default methods are somewhat like abstract classes with virtual methods. The main difference is that a class can implement multiple interfaces, while it can only derive from one class. So default interface implementations amount to a form of multiple inheritance! (This provoked much whingeing in <a href="https://github.com/dotnet/csharplang/issues/288">the discussion on GitHub</a>.)</p>
<p>The optics library I've outlined above makes central use of multiple inheritance. <code>ILens</code> inherits its <code>MultiGet</code> implementation from <code>IGetter</code> and its <code>Map</code> implementation from <code>IMultiLens</code>. So it wouldn't work with abstract classes; before C#8 we wouldn't have been able to write this program. Default interface implementations add new expressive power to the language.</p>
<h2 id="production-worthiness"><a href="#production-worthiness">Production-Worthiness

</a></h2>
<p>Lenses are very useful in functional languages, but I would not recommend you use them in practical C#, even after the release of C#8. When you stand it next to <a href="http://hackage.haskell.org/package/lens">Haskell's <code>lens</code> library</a>, the API I outlined above has a number of significant shortcomings.</p>
<ul>
<li>
<p><strong>Performance</strong>. The big one! Accessing a field is such a common operation that it'd better be fast. The <code>.</code> operator (<code>order.Customer.Address.Postcode</code>) is very fast on the CLR — just a pointer hop. Composed lenses, on the other hand, are tree-shaped objects, and calling <code>Get</code> means traversing that tree with a interface method call at each level. I ran some rudimentary benchmarks and found deeply nested lenses to be orders of magnitude slower than equivalent lensless code.</p>
<p>The <code>lens</code> library sidesteps this performance issue by leaning on Haskell's optimising compiler. <code>lens</code> has been carefully designed to be easy for GHC to optimise, and the result is that GHC generally produces identical machine code for equivalent lensy and lensless functions.</p>
</li>
<li>
<p><strong>Code generation</strong>. Almost all of the atomic lens classes you'd write for a business system are pure boilerplate — exactly the sort of thing you'd expect a machine to write. You should be able to define an object, perhaps mark it up using an attribute, and get on with using lenses into that object in the rest of your program, with Intellisense support. You shouldn't ever need to see a lens's source code. Roslyn, the C# compiler, has no facilities for compile-time code injection like this. A lens library could bundle a source code generator, perhaps using Roslyn's API, which users run ahead-of-time — many ORMs do this — but that's a much less compelling user experience.</p>
<p>This may be a good use case for F#'s type providers. (In any case F# places more emphasis on immutability than C#, making lenses a more natural fit in the first place.) Presently you can't use a type provider to generate code based on another type (though <a href="https://github.com/fsharp/fslang-design/issues/125">it appears to be planned</a>), and <a href="https://github.com/fsharp/fslang-suggestions/issues/679#issuecomment-399411192">there don't seem to be plans</a> to support multiple inheritance in the F# source language. In principle one could implement the hierarchy in C# and consume it from an F# type provider.</p>
</li>
<li>
<p>Two ergonomic complaints regarding C#'s support for <strong>generics</strong>:</p>
<ul>
<li>
<p><strong>Type inference</strong>. C# has only minimal support for type inference. This makes generic lenses unpleasant to use. The following lens picks out a <code>KeyValuePair</code>'s <code>Value</code>:</p>
<pre><code class="language-csharp">class ValueL&lt;K, V&gt; : ILens&lt;KeyValuePair&lt;K, V&gt;, V&gt;
{
    public V Get(KeyValuePair&lt;K, V&gt; kvp) =&gt; kvp.Value;
    public KeyValuePair&lt;K, V&gt; Set(KeyValuePair&lt;K, V&gt; kvp, V val)
        =&gt; new KeyValuePair&lt;K, V&gt;(kvp.Key, val);
}
</code></pre>
<p>You can't use <code>ValueL</code> without explicitly mentioning the concrete type parameters at which you're using it:</p>
<pre><code class="language-csharp">new ValueL&lt;string, int&gt;().Set(new KeyValuePair&lt;string, int&gt;(&quot;foo&quot;, 3), 7);
</code></pre>
<p>Ideally the compiler would be able to deduce the <code>&lt;string, int&gt;</code> part by noticing that we're using it on a <code>KeyValuePair&lt;string, int&gt;</code>. This is difficult to implement in a subtyping-based language, though.</p>
</li>
<li>
<p><strong>Generic type aliases</strong>. <a href="http://comonad.com/reader/2012/mirrored-lenses/">The most general formulation of lenses</a> actually has <em>four</em> type parameters: <code>ILens&lt;in S, out T, out A, in B&gt;</code>! This is to support lenses into generic types, allowing you to change the type of the resulting structure by writing a different type into the lens. (<code>new ValueL().Set(new KeyValuePair&lt;string, string&gt;(&quot;foo&quot;, &quot;bar&quot;), 3)</code> should return a <code>KeyValuePair&lt;string, int&gt;</code> — that is, a new <code>KeyValuePair</code> with a different type to the original.)</p>
<p>The old <code>ILens&lt;S, A&gt;</code> is then equivalent to <code>ILens&lt;S, S, A, A&gt;</code>. Ideally we'd be able to define a <em>type alias</em>, so that you can type <code>ILens&lt;S, A&gt;</code> for <code>ILens&lt;S, S, A, A&gt;</code>, but C# doesn't support this. (<a href="https://github.com/dotnet/roslyn/issues/3993">A modest proposed extension to <code>using</code></a> would largely service this complaint, reducing the noise to a few lines of boilerplate at the top of each file.)</p>
</li>
</ul>
</li>
<li>
<p><strong>Noisy syntax</strong>. Haskell allows you to define custom symbolic operators, and <code>lens</code> ships a large collection of operators to debigulate your code. <code>new CustomerL()._(new AddressL())._(new PostcodeL()).Get(order)</code> is clunky in comparison to Haskell's cute OO-style <code>order^.customer.address.postcode</code>.</p>
<ul>
<li>Related to this concern is <strong>namespacing</strong>. Above I've used a convention of appending the letter <code>L</code> to lens classes (<code>CustomerL</code>), but that starts to break down when you have more than one property with the same name in your system. One option might be to nest the lenses inside the objects themselves and import them with <code>using static</code>.
</li>
</ul>
</li>
<li>
<p><strong>Platform compatibility</strong>. According to <a href="https://blogs.msdn.microsoft.com/dotnet/2018/11/12/building-c-8-0/">a recent announcement</a>, default interface implementations are only going to be available on .NET Core, and won't be in the desktop framework for the foreseeable future. As I understand it, the desktop CLR's stringent backwards compatibility requirements make testing a wide-reaching CLR feature like this difficult and expensive. But to me, a library author, this is a very disappointing development: libraries <em>must</em> support the desktop CLR if they expect to have any users, so locking down features designed for library authors seems like a misfire. I'd prefer it if Microsoft just said directly that the desktop framework is being sunsetted — that way I'd at least have some ammunition for GitHub issues.</p>
</li>
</ul>
<p>All that said, there are a couple of things which I find preferable about this design when compared to <code>lens</code>. Haskell doesn't feature subtyping directly, so <code>lens</code> uses a clever function-based encoding of its type hierarchy, using the type class solver to handle the various subtype relationships. Encoding lenses as functions is partly why <code>lens</code> is so fast, but it does make for a steep learning curve and notoriously confusing type errors. Using a more direct representation of subtyping means the model is clearer, and it's easier to see how one would slot (eg) prisms or indexed lenses into the system I outlined above. What's more, the four-parameter version of <code>ILens</code> I mentioned above is variously co- and contra-variant in its parameters, meaning it interoperates well with the rest of C#'s type hierarchy. In some sense these lenses are <em>more</em> composable than <code>lens</code>'s lenses.</p>
<p>I'd love to tell you I've written this up as a published library, but the shortfalls I noted above make this formulation of lenses impractical for real-world use. I'd love to hear your ideas on how to improve the situation! In the meantime, I bagsie the name <code>OptiCS</code>.</p>

]]></summary>
</entry>
<entry>
    <title>Live-streaming</title>
    <link href="http://www.benjamin.pizza/posts/2018-07-16-live-streaming.html" />
    <id>http://www.benjamin.pizza/posts/2018-07-16-live-streaming.html</id>
    <published>2018-07-16T00:00:00Z</published>
    <updated>2018-07-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>My esteemed colleague <a href="https://twitter.com/g3rv4?lang=en">Gervasio</a> and I have arranged to live-stream some programming ~~this Friday, the 20th of July~~ <strong>Update</strong>: we've decided to move it to next <strong>Tuesday, the 24th</strong>, at 2PM BST. We're going to be working on adding some basic <code>Span</code> support to <a href="https://github.com/benjamin-hodgson/Pidgin">my parsing library</a>, and it's going to involve <code>unsafe</code> and custom IL, which should be a bit of fun.</p>
<p>The stream will be <a href="https://www.youtube.com/watch?v=O23OLkQtiS4">on YouTube</a> and we're planning to start at 2PM BST. Hope to see you there with your questions!</p>

]]></summary>
</entry>
<entry>
    <title>Eighty</title>
    <link href="http://www.benjamin.pizza/posts/2018-03-16-eighty.html" />
    <id>http://www.benjamin.pizza/posts/2018-03-16-eighty.html</id>
    <published>2018-03-16T00:00:00Z</published>
    <updated>2018-03-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>HTML templating systems are great but they sure are complex. ASP.NET's Razor, for example, is a whole new programming language! While Razor does happen to have a large chunk of C# embedded within it, and it works by generating and then compiling C# code, it's still a separate language with a separate syntax, separate abstraction techniques, separate compiler tooling, a separate file type, and separate (and usually inferior) editor support. All this for a task as simple and common as generating HTML!</p>
<p>This overhead can be worth it if you're building a complex web application, but for simple tools such as report generators or email batch mailers Razor is unwieldy. Many people in these situations resort to generating their own HTML, either by building strings manually or by imperatively building tags using .NET's supplied XML manipulation APIs. But there's a whole world of possible designs out there, and there's a lot of space in between &quot;complex templating language&quot; and &quot;build strings by hand&quot;.</p>
<h2 id="eighty"><a href="#eighty">Eighty

</a></h2>
<p><a href="https://github.com/benjamin-hodgson/Eighty">Eighty</a> (as in <em>eigh-ty-M-L</em>) is my attempt at striking a balance between these two extremes: not so abstract as to constitute a separate programming language, but not so concrete that you have to manipulate XML tags or strings manually. It's a simple embedded domain-specific language which piggybacks on C#'s syntax, enabling you to write code resembling the HTML you're generating. Rather than embedding C# into an HTML generator, Eighty embeds an HTML generator into C#.</p>
<p>Here's an example from <a href="https://github.com/benjamin-hodgson/Eighty/blob/master/README.md">the readme</a>:</p>
<pre><code class="language-csharp">var html = article(@class: &quot;readme&quot;)._(
    h1(id: &quot;Eighty&quot;)._(&quot;Eighty&quot;),
    p_(
        &quot;Eighty (as in &quot;,
        i_(&quot;eigh-ty-M-L&quot;),
        &quot;) is a simple HTML generation library.&quot;
    )
);
</code></pre>
<p>Eighty is organised around <a href="https://www.benjamin.pizza/Eighty/v1.2.0/api/Eighty.Html.html">the <code>Html</code> class</a>, being an immutable chunk of HTML which knows how to render itself using <a href="https://www.benjamin.pizza/Eighty/v1.2.0/api/Eighty.Html.html#Eighty_Html_Write_System_IO_TextWriter_">its <code>Write(TextWriter)</code> method</a>. <code>Html</code> defines a large collection of static methods (designed to be imported with <code>using static</code>), with names like <a href="https://www.benjamin.pizza/Eighty/v1.2.0/api/Eighty.Html.html#Eighty_Html_h1_System_String_System_String_System_String_System_String_System_String_System_String_"><code>h1</code></a> and <a href="https://www.benjamin.pizza/Eighty/v1.2.0/api/Eighty.Html.html#Eighty_Html_p_System_String_System_String_System_String_System_String_System_String_System_String_"><code>p</code></a>, which create <code>Html</code> values representing their respective tags, with a collection of children which are smaller <code>Html</code> values.</p>
<p>Eighty adopts some simple conventions for its HTML-esque domain-specific language:</p>
<ul>
<li>Tags are created using (lower-case) methods like <code>p()</code> and <code>i()</code>.
</li>
<li>Attributes are passed as optional named arguments: <code>a(href: &quot;benjamin.pizza&quot;, @class: &quot;website-link&quot;)</code>. I can't force you to name your arguments — you could pass them positionally — but that's not a good idea.
</li>
<li>A tag's children are introduced using the <code>_</code> character, which can appear at the end of a method name or as a method name all by itself. <code>a(href: &quot;benjamin.pizza&quot;)._(&quot;Visit my website&quot;)</code> creates an <code>a</code> tag with an <code>href</code> attribute and some text inside it; <code>p_(&quot;a paragraph of text&quot;)</code> represents a <code>p</code> tag with some text but no attributes. I chose <code>_</code> because it's the least noisy character that can be used as an identifier in C#.
</li>
<li>Strings can be implicitly converted to <code>Html</code> and are interpreted as HTML text. Text is HTML-encoded by default. You can opt out of this using the <code>Raw</code> method.
</li>
</ul>
<h2 id="eighty-vs-razor"><a href="#eighty-vs-razor">Eighty vs Razor

</a></h2>
<p>Of course, C# code will only ever look <em>a bit</em> like HTML. Razor code looks much more like HTML than this! This can be a drawback when you're working with designers who want to read and write HTML — I'm planning to write a tool to convert HTML text into an Eighty expression to partially ease this pain point. But Eighty has two big advantages which make it simpler and easier than Razor to program with:</p>
<ol>
<li>It plugs into your existing system. You don't require any extra tools to work with Eighty: if you can compile C#, you can use Eighty.
</li>
<li>Programming with Eighty is <em>just programming</em>. <code>Html</code> instances are plain old immutable CLR objects, so you can use all your favourite techniques for abstraction and code reuse.
</li>
</ol>
<p>To illustrate the second point, here are some examples of how you might emulate some of Razor's programming constructs using Eighty. In many of these cases Eighty does a better job than Razor of allowing abstraction and code reuse, because Eighty is embedded within C# rather than layered on top of C#.</p>
<h3 id="models">[Models](#models</h3>
<p>In Razor, each view file you write declares a <em>model type</em> — the type of object it expects you to pass in to direct the generation of HTML. You use the <code>@model</code> directive at the top of your file, and then you can access members of the model in your Razor code.</p>
<pre><code class="language-cshtml">@model ExampleModel

&lt;h1&gt;@Model.Title&lt;/h1&gt;
</code></pre>
<p>One important disadvantage of Razor's <code>@model</code> construct is that it is dynamically checked. The controller's <code>View</code> method takes an <code>object</code> for the <code>model</code> parameter. You get a runtime error, without any feedback from the compiler, if you pass in a model whose type doesn't match the view's expected model type.</p>
<p>Since Eighty is embedded within C#, there's no special syntax to declare the type of data a function depends on. You can just use a plain old parameter.</p>
<pre><code class="language-csharp">Html Example(ExampleModel model)
    =&gt; h1_(model.Title);
</code></pre>
<p>Since a template is a regular C# method, it's much easier to run in a unit test harness than Razor. You can just call the method and make assertions about the generated HTML, either by looking at the string directly or by parsing it and traversing the resultant DOM.</p>
<p>Eighty includes <a href="https://www.benjamin.pizza/Eighty/v1.2.0/api/Eighty.IHtmlRenderer-1.html">an <code>IHtmlRenderer&lt;TModel&gt;</code> interface</a>, which captures this pattern of parameterising a chunk of HTML by a model, but its use is optional — it's used primarily by Eighty's ASP.NET integration packages.</p>
<h3 id="control-flow">[Control flow](#control-flow</h3>
<p>Razor allows you to mix markup with C#'s control flow constructs such as <code>foreach</code> and <code>if</code>. Here's a simple example of populating a <code>ul</code> based on a list of values:</p>
<pre><code class="language-cshtml">&lt;ul&gt;
    @foreach (var item in Model.Items)
    {
        if (item.Visible)
        {
            &lt;li&gt;@item.Value&lt;/li&gt;
        }
    }
&lt;/ul&gt;
</code></pre>
<p>With Eighty, it's a question of building different <code>Html</code> values. You can use LINQ's high-level functional looping constructs:</p>
<pre><code class="language-csharp">return ul_(
    model.Items
        .Where(item =&gt; item.Visible)
        .Select(item =&gt; li_(item.Value))
);
</code></pre>
<p>Or you can write your own loop and build a list:</p>
<pre><code class="language-csharp">var lis = new List&lt;Html&gt;();
foreach (var item in model.Items)
{
    if (item.Visible)
    {
        lis.Add(li_(item.Value));
    }
}
return ul_(lis);
</code></pre>
<p>Mixing markup with C# is not a problem, because markup <em>is</em> C#.</p>
<h3 id="partials-and-helpers">[Partials and Helpers](#partials-and-helpers</h3>
<p>Razor's two main tools for code reuse are <em>partial views</em> and <em>helpers</em>. For the purposes of this article, they're roughly equivalent. Partial views can be returned directly from a controller but their model type is checked at runtime, whereas helpers' parameters are checked by the compiler but they can only be invoked from within a Razor view.</p>
<p>Eighty handles both of these uses in the simplest of ways: <em>calling a function</em>. If I want to include an HTML snippet in more than one place, I can just extract it into a method returning an <code>Html</code> object. Transliterating an example from <a href="https://docs.microsoft.com/en-us/aspnet/web-pages/overview/ui-layouts-and-themes/creating-and-using-a-helper-in-an-aspnet-web-pages-site">the MVC documentation</a>:</p>
<pre><code class="language-csharp">Html MakeNote(string content)
    =&gt; div(@class: &quot;note&quot;)._(
        p_(
            strong_(&quot;Note&quot;),
            Raw(&quot;&amp;nbsp;&amp;nbsp; &quot;),
            content
        )
    );

Html SomeHtmlContainingANote()
    =&gt; article_(
        p_(&quot;This is some opening paragraph text&quot;),
        MakeNote(&quot;My test note content&quot;),
        p_(&quot;This is some following text&quot;)
    );
</code></pre>
<p>This is the best of both worlds: types are checked by the compiler as usual, but the returned <code>Html</code> value is a perfectly good standalone chunk of HTML, and can be rendered separately if necessary.</p>
<p><code>Html</code> values being ordinary C# values, Eighty actually supports more types of reuse than Razor does. For example, you can pass a chunk of HTML as an argument, which is not easy to do with Razor:</p>
<pre><code class="language-csharp">Html RepeatFiveTimes(Html html)
    =&gt; _(Enumerable.Repeat(html, 5));
</code></pre>
<p>Since <code>Html</code> values are immutable, you can safely share them between different HTML documents, across different threads, etc. Sharing parts of your HTML document that don't change can be an important optimisation.</p>
<h3 id="layouts">[Layouts](#layouts</h3>
<p>Razor lets you define a shared <em>layout</em> page, which acts as a template for the other pages in your application. For example, you might put the <code>html</code> and <code>body</code> tags in a layout page, and use the built in <code>RenderBody</code> helper to render the concrete page's body inside the <code>body</code> tag. This is also where global navs and the like are defined.</p>
<p>One way to handle global layouts and sections in Eighty would be to define an abstract base class. Each section becomes an abstract method, allowing individual pages to fill in their own HTML for those sections.</p>
<pre><code class="language-csharp">abstract class Layout
{
    public Html GetHtml()
        =&gt; doctypeHtml_(
            head(
                link(
                    rel: &quot;stylesheet&quot;,
                    type: &quot;text/css&quot;,
                    href: &quot;default.css&quot;
                ),
                Css(),
                script(
                    type: &quot;text/javascript&quot;,
                    src: &quot;jquery-3.3.1.min.js&quot;
                ),
                Js()
            ),
            body(
                Body()
            )
        );

    protected abstract Html Css();
    protected abstract Html Js();
    protected abstract Html Body();
}
</code></pre>
<p>Then, inheriting a layout is as easy as inheriting a class.</p>
<pre><code class="language-csharp">class DashboardPage : Layout
{
    private DashboardModel _model;

    public Dashboard(DashboardModel model)
    {
        _model = model;
    }

    protected override Html Css()
        =&gt; /* Dashboard-specific CSS */;

    protected override Html Js()
        =&gt; /* Dashboard-specific scripts */;

    protected override Html Body()
        =&gt; /* The body of the dashboard page */;
}
</code></pre>
<h2 id="twenty"><a href="#twenty">Twenty

</a></h2>
<p>Eighty comes bundled with a second HTML generation library called Twenty. Twenty is harder to use correctly than Eighty, and its API is more verbose, but it's faster.</p>
<p>HTML tags have to be balanced: every opening tag has to have a matching closing tag and vice versa. While an <code>Html</code> value is being written to a <code>TextWriter</code>, Eighty manages the stack of currently-open tags using the call stack. Each tag writes its opening tag, tells its children to write themselves, and then writes its closing tag. This is possible because <code>Html</code> is an ordinary reference type; the objects you build with methods like <code>p()</code> and <code>h1()</code> are tree-shaped objects representing a DOM of statically-unknown size.</p>
<p>Twenty instead takes an imperative view of HTML generation. Each tag method writes an opening tag to the <code>TextWriter</code> immediately, and returns an <code>IDisposable</code> which writes out the closing tag when it's disposed. You, the programmer, use C#'s <code>using</code> statement to ensure that the <code>Dispose</code> method is called as soon as the children have been written. The structure of your HTML document is still visible in the code, but it's present in the nesting of <code>using</code> statements, rather than by the structure of a tree-shaped object.</p>
<pre><code class="language-csharp">class MyHtmlBuilder : HtmlBuilder
{
    protected override void Build()
    {
        using (article(@class: &quot;readme&quot;))
        {
            using (h1(id: &quot;Eighty&quot;))
                Text(&quot;Eighty&quot;);
            using (p())
            {
                Text(&quot;Eighty (as in &quot;);
                using (i())
                    Text(&quot;eigh-ty-M-L&quot;);
                Text(&quot;) is a simple HTML generation library.&quot;);
            }
        }
    }
}
</code></pre>
<p>Perhaps this is a bit of an abuse of <code>IDisposable</code>, and the <code>using</code> syntax is comparatively noisy, but this trick allows Twenty to operate quickly and without generating any garbage while still making for a reasonable DSL. Compared to Eighty, Twenty does lose out on some flexibility and safety:</p>
<ul>
<li>You mustn't forget a <code>using</code> statement, or call <code>Dispose</code> more than once, or Twenty will output malformed HTML. Eighty, on the other hand, will never generate bad HTML (notwithstanding the use of <code>Raw</code>).
</li>
<li>There's no <code>Html</code> object — you can't pass around chunks of HTML as first class values. This makes code reuse and abstraction somewhat more difficult.
</li>
<li><code>HtmlBuilder</code> is not re-entrant. You can't use the same <code>HtmlBuilder</code> from multiple threads.
</li>
<li>There's no <code>async</code> API, because there's no way to call <code>Dispose</code> asynchronously.
</li>
</ul>
<p>Given Twenty's limitations, my advice is to write your markup using <code>Html</code>, and convert it to <code>HtmlBuilder</code> if you see that building <code>Html</code> values is a performance bottleneck.</p>
<h2 id="performance"><a href="#performance">Performance

</a></h2>
<p>Eighty is pretty fast. I wrote a benchmark testing how long it takes to spit out around 30kB of HTML (with some encoding characters thrown in for good measure) while running in an in-memory hosted MVC application. Eighty's synchronous code path does this around three times faster than Razor, and Twenty runs about 30% faster than that — so, four times faster than Razor.</p>
<p>What have I done to make Eighty fast? Honestly, not a huge amount. There are a only few interesting optimisations in Eighty's codebase.</p>
<ul>
<li>Each call to <code>TextWriter</code>'s <code>Write</code> method is comparatively expensive, so rather than write individual snippets of HTML into the <code>TextWriter</code> directly, Eighty builds up a 4kB buffer and empties it out into the <code>TextWriter</code> when it fills up. The code to fill this buffer is a little fiddly, because you don't know how long your input string is going to be after HTML-encoding it, so the HTML encoder has to write the encoded HTML in chunks. I toyed with a hand-written encoder, but I wanted to interoperate with ASP.NET's pluggable <code>HtmlEncoder</code>, so I ended up calling that class's low-level API.
<ul>
<li>The buffer is managed by <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/HtmlEncodingTextWriter.cs">a mutable struct</a> which is <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/Html.cs#L61-L63">stored on the stack and passed by reference</a> because mutable structs must never be copied. However, <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/AsyncHtmlEncodingTextWriter.cs">the async version</a> <em>cannot</em> be a struct because <code>async</code> methods copy their <code>this</code> variable into a field behind the scenes. My first version of the code used the same mutable struct for both paths, which caused me some head-scratching when the <code>async</code> version didn't work!
</li>
<li>There's <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/Twenty/HtmlEncodingTextWriterReference.cs">a fun and dangerous hack</a> in Twenty's codebase to allow storing a reference to one of these stack-allocated structs <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/Twenty/HtmlBuilder.cs">in a field</a>. This is safe as long as the reference in the field doesn't live longer than the stack location to which it refers, but you don't get any compile-time feedback about this (I just have to program carefully and hope I don't make a mistake). This hack makes critical use of C# 7's &quot;<code>ref</code> return types&quot;, so it wouldn't have been possible a couple of years ago.
</li>
</ul>
</li>
<li>Calling an <code>async</code> method is comparatively expensive, even if it never goes async, because of the way <code>async</code> methods are translated by the compiler into code which builds and then executes a state machine. In the case of Eighty's frequently-called <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/AsyncHtmlEncodingTextWriter.cs#L149"><code>WriteRawImpl</code> method</a>, it's predictable whether a call will complete synchronously (that is, without calling the underlying <code>TextWriter</code>'s <code>WriteAsync</code> method). <a href="https://github.com/benjamin-hodgson/Eighty/commit/1d6b5f45919363c978de05a5a849835cec6a773b#diff-c3aed398c4361803494b9d59237185e2">I split the <code>async</code> method into two parts</a> — a fast wrapper which synchronously returns a <code>Task</code> and an <code>async</code> method which is only called when necessary — and got a ~15% speedup in my end-to-end benchmarks.
</li>
<li><code>Html</code> values make use of <code>ImmutableArray</code>s to store their children. <code>ImmutableArray</code> is a thin wrapper over a regular array, so if you have a <code>T[]</code> you should be able to turn it into an <code>ImmutableArray</code> in-place without copying the contents, as long as you're careful never to modify the original array after freezing it. There are several places in Eighty where this is a safe optimisation, but <code>ImmutableArray</code> doesn't have a public API to do this. However, since <code>ImmutableArray&lt;T&gt;</code> is a struct with a single private <code>T[]</code> field, its runtime representation is the same as <code>T[]</code>'s. This makes it possible to <a href="https://github.com/benjamin-hodgson/Eighty/blob/3c431c13200022bb34ee3de635cce305384abef5/Eighty/ImmutableArrayFactory.cs#L45">unsafely coerce a <code>T[]</code> to an <code>ImmutableArray&lt;T&gt;</code></a> with no runtime cost.
<ul>
<li>I've opened <a href="https://github.com/dotnet/corefx/issues/28064">an issue in the <code>corefx</code> repo</a> proposing an officially-supported API for this use case.
</li>
</ul>
</li>
</ul>
<p>I'm not sure exactly why Razor is slower by comparison. My guess is that Razor's template compiler just tends to generate comparatively slow C# code — so there's probably some room for improvement — but I would like to investigate this more.</p>
<hr />
<p>HTML generators are an example of a problem where the spectrum of possible solutions is very broad indeed. Just within the C# ecosystem there exists a menagerie of different templating languages, as well as imperative object-oriented APIs like <code>TagBuilder</code> and streaming APIs like <code>XmlWriter</code>. Even Eighty and Twenty, two implementations of the same idea, are substantially different. You can often find yourself somewhere quite interesting if you appreach a common problem from a different direction than the established solutions. What parts of the library ecosystem do you think you could do with a fresh perspective?</p>
<p>Eighty is available <a href="https://www.nuget.org/packages/Eighty">on Nuget</a>, along with some helpers to integrate Eighty with <a href="https://www.nuget.org/packages/Eighty.AspNet.Mvc">MVC</a> and <a href="https://www.nuget.org/packages/Eighty.AspNetCore">ASP.NET Core</a>. API docs are hosted <a href="https://www.benjamin.pizza/Eighty/v1.2.0/api/Eighty.html">on this very domain</a>, and the code's all <a href="https://github.com/benjamin-hodgson/Eighty">on GitHub</a> where contributions and bug reports are very welcome!</p>

]]></summary>
</entry>
<entry>
    <title>Zip-Folding</title>
    <link href="http://www.benjamin.pizza/posts/2018-01-10-zip-folding.html" />
    <id>http://www.benjamin.pizza/posts/2018-01-10-zip-folding.html</id>
    <published>2018-01-10T00:00:00Z</published>
    <updated>2018-01-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>One of my favourite little gems of functional programming is the following implementation of the dot product:</p>
<pre><code class="language-haskell">dot :: [Double] -&gt; [Double] -&gt; Double
xs `dot` ys = sum (zipWith (*) xs ys)
</code></pre>
<p><code>dot</code> zips two lists of numbers, multiplying each pair of elements using <code>(*)</code>, and then aggregates the results with <code>sum</code>. It's like a <em>map-reduce</em> program, but it processes two collections, not one. It generalises rather beautifully to any zippily <code>Applicative</code> <code>Foldable</code> container whose elements form a <code>Semiring</code>:</p>
<pre><code class="language-haskell">dot :: (Semiring a, Applicative t, Foldable t) =&gt; t a -&gt; t a -&gt; a
xs `dot` ys = foldl' (&lt;+&gt;) zero (liftA2 (&lt;.&gt;) xs ys)
</code></pre>
<p>I think I'm particularly taken with this example because it combines three different abstractions in a totally natural way to produce a concise and generic implementation of a well-known program. It's a beautiful demonstration of how these mathematical tools fit together. It also happens to be an example of a programming pattern that I call <em>zip-folding</em>.</p>
<hr />
<p>Until recently I felt rather embarrassed that my C# generic programming library <a href="https://github.com/benjamin-hodgson/Sawmill">Sawmill</a> didn't have a good story for consuming more than one tree at a time. I had lots of tools for <a href="https://github.com/benjamin-hodgson/Sawmill/blob/b87687e67185ddc299ad67455bd7c79f97e066b2/Sawmill/Rewriter.SelfAndDescendants.cs">querying</a>, <a href="https://github.com/benjamin-hodgson/Sawmill/blob/b87687e67185ddc299ad67455bd7c79f97e066b2/Sawmill/Rewriter.Rewrite.cs">editing</a>, and <a href="https://github.com/benjamin-hodgson/Sawmill/blob/b87687e67185ddc299ad67455bd7c79f97e066b2/Sawmill/Rewriter.Fold.cs">tearing down</a> single trees, but nothing that could help you process two trees at once. This is a very common requirement - for example, if you're unit testing a parser or a transformation pass, you need to compare the output tree to the one that you expected.</p>
<p>I got to thinking about what it means to zip two trees together - an operation which should make sense if you think of a tree as a container of subtrees. Pairing up nodes in a tree is straightforward, even if the two trees are unevenly shaped. You just pair up the children of each pair of nodes, ignoring those which don't have a partner (the grey-coloured ones in the drawing):</p>
<img src="/images/2018-01-10-zip-folding/zip.jpg" alt="Pairing up nodes" />
<p>But I got stuck on how to plug those paired nodes back into a single tree representing the zipped trees. Nodes typically have space for a fixed number of children, but pairing up children will typically change that number. That is, a binary operator has precisely two children, but when zipping two binary operators together you need to do something with four children.</p>
<p>And, more generally, what would it mean to zip trees recursively? You can imagine a scheme wherein each child of a node is replaced with a tuple of two children. But each child is really a subtree, with its own children, so the two subtrees need to be zipped - but that ought to produce a single tree, not a pair of trees. It's contradictory! The intuitive idea that a node in a tree is a container of subtrees fails when you consider zipping.</p>
<hr />
<p>Guess where this is going: you can't <em>zip</em> trees to produce a new tree, but you can <em>zip-fold</em> trees to produce a value. The idea is to take pairs of nodes in a tree and combine them with the results of zipping their children.</p>
<p>Let's start by looking at (an abbreviated version of) Sawmill's existing <code>Fold</code>. <code>Fold</code> says <em>if you give me a way to combine a node with the results of folding its children, I can recursively fold the entire tree to produce a single summary value</em>.</p>
<pre><code class="language-csharp">public static U Fold&lt;T, U&gt;(
    this T value,
    Func&lt;T, Children&lt;U&gt;, U&gt; func,
) where T : IRewritable&lt;T&gt;
    =&gt; func(
        value,
        value.GetChildren()
            .Select(child =&gt; child.Fold(func))
    );
</code></pre>
<p>Revisiting <a href="https://www.benjamin.pizza/posts/2017-11-13-recursion-without-recursion.html">the JQL example</a>, <code>Fold</code> will take an input tree like <code>[c#] and (not [javascript] or salary:50000gbp)</code> and compute the expression:</p>
<pre><code class="language-csharp">func(
    new AndNode(/* ... */),
    Children.Two(
        func(new TagNode(&quot;c#&quot;), Children.None&lt;U&gt;()),
        func(
            new OrNode(/* ... */),
            Children.Two(
                func(
                    new NotNode(/* ... */),
                    Children.One(
                        func(
                            new TagNode(&quot;javascript&quot;),
                            Children.None&lt;U&gt;()
                        )
                    )
                ),
                func(new SalaryNode(50000, &quot;gbp&quot;), Children.None&lt;U&gt;())
            )
        )
    )
)
</code></pre>
<p><code>Fold</code> traverses a tree from bottom to top, applying <code>func</code> to each subtree and the current set of intermediate results.</p>
<p><code>ZipFold</code> works by analogy to <code>Fold</code>. It says <em>if you give me a way to combine two nodes with the results of zip-folding their children, I can recursively zip the two entire trees to produce a single summary value</em>. <code>ZipFold</code> pairs up the children of the two input nodes using the standard <code>Enumerable.Zip</code>, recursively zip-folds each pair, and then feeds the results to <code>func</code>. Note that the length of the <code>IEnumerable</code> that's passed to <code>func</code> is the length of the smaller of the two nodes' collections of children.</p>
<pre><code class="language-csharp">public static U ZipFold&lt;T, U&gt;(
    this T value1,
    T value2
    Func&lt;T, T, IEnumerable&lt;U&gt;, U&gt; zipFunc,
) where T : IRewritable&lt;T&gt;
    =&gt; zipFunc(
        value1,
        value2,
        value1.GetChildren().Zip(
            value2.GetChildren(),
            (child1, child2) =&gt; child1.ZipFold(child2, zipFunc)
        )
    );
</code></pre>
<p>The two trees are zipped together and torn down in a single pass.</p>
<p>Here's how it looks in Haskell, using the <a href="https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Plated.html"><code>Control.Lens.Plated</code></a> API. Haskellers like to use tongue-in-cheek Greek names for recursion schemes. Apparently the Greek word for &quot;zip&quot; is &quot;fermouár&quot;, so I'm calling this a <em>fermomorphism</em>.</p>
<pre><code class="language-haskell">fermo :: Plated a =&gt; (a -&gt; a -&gt; [r] -&gt; r) -&gt; a -&gt; a -&gt; r
fermo f x y = f x y $
    zipWith (fermo f) (toListOf plate x) (toListOf plate y)
</code></pre>
<p>As an example: <code>ZipFold</code> allows you to concisely test a pair of trees for equality, by looking only at one pair of nodes at a time.</p>
<pre><code class="language-csharp">public static bool Equal(JqlNode j1, JqlNode j2)
    =&gt; j1.ZipFold&lt;JqlNode, bool&gt;(
        j2,
        (n1, n2, childrenEqual) =&gt;
        {
            switch (n1)
            {
                case SalaryNode s1 when n2 is SalaryNode s2:
                    return s1.Currency == s2.Currency
                        &amp;&amp; s1.Amount == s2.Amount;
                case TagNode t1 when n2 is TagNode t2:
                    return t1.Tag == t2.Tag;
                case AndNode a1 when n2 is AndNode a2:
                case OrNode o1 when n2 is OrNode o2:
                case NotNode a1 when n2 is NotNode a2:
                    return childrenEqual.All(c =&gt; c);
                default:
                    return false;
            }
        }
    );
</code></pre>
<p>The <code>ZipFold</code> that you'll find in Sawmill is actually an <em>n</em>-ary zip-fold. Instead of taking two <code>T</code>s, and passing two <code>T</code>s to <code>func</code>, it works with an arbitrary number of <code>T</code>s. Here's the code:</p>
<pre><code class="language-csharp">public static U ZipFold&lt;T, U&gt;(
    this T[] values,
    Func&lt;T[], IEnumerable&lt;U&gt;, U&gt; func,
) where T : IRewritable&lt;T&gt;
    =&gt; func(values, xs.ZipChildren(children =&gt; children.ZipFold(func)));

private static IEnumerable&lt;U&gt; ZipChildren&lt;T, U&gt;(
    this T[] input,
    Func&lt;T[], U&gt; zipFunc
) where T : IRewritable&lt;T&gt;
{
    var enumerators = input
        .Select(x =&gt; x.GetChildren().GetEnumerator())
        .ToArray();

    while (enumerators.All(e =&gt; e.MoveNext()))
    {
        yield return zipFunc(
            enumerators.Select(e =&gt; e.Current).ToArray()
        );
    }
}
</code></pre>
<p>Sadly, the invariant that <code>zipFunc</code> receives the same number of <code>T</code>s as were passed to <code>ZipFold</code> is not expressible in C#'s type system. So as a consumer of <code>ZipFold</code>, you just have to trust that <code>zipFunc</code>'s argument is of a certain size. In the <code>Equal</code> example, that size is two, because we're consuming two trees:</p>
<pre><code class="language-csharp">public static bool Equal(JqlNode j1, JqlNode j2)
    =&gt; new[] { j1, j2 }.ZipFold&lt;JqlNode, bool&gt;(
        (ns, childrenEqual) =&gt;
        {
            switch (ns[0])
            {
                case SalaryNode s1 when ns[1] is SalaryNode s2:
                    return s1.Currency == s2.Currency
                        &amp;&amp; s1.Amount == s2.Amount;
                case TagNode t1 when ns[1] is TagNode t2:
                    return t1.Tag == t2.Tag;
                case AndNode a1 when ns[1] is AndNode a2:
                case OrNode o1 when ns[1] is OrNode o2:
                case NotNode n1 when ns[1] is NotNode n2:
                    return childrenEqual.All(c =&gt; c);
                default:
                    return false;
            }
        }
    );
</code></pre>
<p>Here's the Haskell transliteration of this <em>n</em>-ary zip-fold function, which <code>traverse</code>s in the <code>ZipList</code> <code>Applicative</code> to concisely zip <em>n</em> lists of children:</p>
<pre><code class="language-haskell">fermo :: Plated a =&gt; ([a] -&gt; [r] -&gt; r) -&gt; [a] -&gt; r
fermo f xs = f xs (
    map (fermo f) $ getZipList $ traverse (ZipList . toListOf plate) xs
    )
</code></pre>
<p><code>ZipFold</code> is available in <a href="https://www.nuget.org/packages/Sawmill/">version 1.3.0 of Sawmill</a>.</p>

]]></summary>
</entry>
<entry>
    <title>Functor Functors</title>
    <link href="http://www.benjamin.pizza/posts/2017-12-15-functor-functors.html" />
    <id>http://www.benjamin.pizza/posts/2017-12-15-functor-functors.html</id>
    <published>2017-12-15T00:00:00Z</published>
    <updated>2017-12-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>You can teach a new dog old tricks.</p>
<p>One of the fun things about category theory is that once you've learned an idea in one context it's easy to apply it to another one. Of the numerous categories available to Haskell programmers, <strong>Hask</strong>, the category of Haskell types and functions, gets the lion's share of the attention. Working with standard abstractions in more overlooked categories is a great way to reuse ideas: it makes you look clever, like you've invented something new, but actually all you've done is put the building blocks together differently. I won't tell if you don't.</p>
<h2 id="templates-reusable-records"><a href="#templates-reusable-records">Templates: Reusable Records

</a></h2>
<p>Every now and then I'll see a question on Stack Overflow or Reddit in which a programmer is trying to work with a bunch of record types which share a similar structure. For a contrived example, in a shopping system you may want to differentiate between completed checkout forms, which are ready to be dispatched, and &quot;draft&quot; checkout forms, which the user is currently filling in. The simplest way to do this is to build separate types, and write a function to upgrade a draft form to a regular form if all of its fields are filled in.</p>
<pre><code class="language-haskell">data CardType = Visa | AmEx | Mastercard

data Form = Form {
    form_email :: Text,
    form_cardType :: CardType,
    form_cardNumber :: Text,
    form_cardExpiry :: Day
}

data DraftForm = DraftForm {
    draftForm_email :: Maybe Text,
    draftForm_cardType :: Maybe CardType,
    draftForm_cardNumber :: Maybe Text,
    draftForm_cardExpiry :: Maybe Day
}

toForm :: DraftForm -&gt; Maybe Form
toForm (DraftForm
    (Just email)
    (Just cardType)
    (Just cardNumber)
    (Just cardExpiry)) = Just $
        Form email cardType cardNumber cardExpiry
toForm _ = Nothing
</code></pre>
<p>Now, the standard trick to de-duplicate these two types is to derive both from what I'll call a <em>template</em> type, wrapping each field of the template in some type constructor <code>f</code>. You recover <code>Form</code> by setting <code>f</code> to the boring <code>Identity</code> functor, and you get <code>DraftForm</code> by setting <code>f</code> to <code>Maybe</code>.</p>
<pre><code class="language-haskell">data FormTemplate f = FormTemplate {
    _email :: f Text,
    _cardType :: f CardType,
    _cardNumber :: f Text,
    _cardExpiry :: f Day
}
type Form = FormTemplate Identity
type DraftForm = FormTemplate Maybe
</code></pre>
<p>So a template is a record type parameterised by a type constructor. It'll generally have a kind of <code>(* -&gt; *) -&gt; *</code>. The fields of the record are the type constructor applied to a variety of different type arguments. Working with a template typically involves coming up with an interesting type constructor <code>(* -&gt; *)</code> and plugging it in to get interestingly-typed fields. You can think of a record as a container of <code>f</code>s.</p>
<p>This trick has become Haskell folklore - I couldn't tell you where I first saw it - but I've only seen a few people talk about what happens when you treat templates as first class citizens. To get used to this style, a simple example is giving names to specific instantiations of arbitrary templates:</p>
<pre><code class="language-haskell">type Record t = t Identity
type Partial t = t Maybe

type Form = Record FormTemplate
type DraftForm = Partial FormTemplate
</code></pre>
<p>The rest of this blog post is about treating template types intuitively as fixed-size containers of functors. I'll be taking familiar tools for working with containers of <em>values</em> - <code>Functor</code>, <code>Traversable</code>, <code>Representable</code> - and applying them to the context of containers of <em>functors</em>.</p>
<h2 id="functors-from-the-category-of-endofunctors"><a href="#functors-from-the-category-of-endofunctors">Functors from the Category of Endofunctors

</a></h2>
<p>In Haskell, categories are represented as a <em>kind</em> <code>k</code> of objects and a <em>type constructor</em> <code>c :: k -&gt; k -&gt; *</code> of morphisms between those objects. If the category <code>C</code> has objects in <code>k1</code> and morphisms in <code>c</code>, and <code>D</code> has objects in <code>k2</code> and morphisms in <code>d</code>, then a functor from <code>C</code> to <code>D</code> is a type constructor <code>f :: k1 -&gt; k2</code> mapping objects paired with an operation <code>fmap :: c a b -&gt; d (f a) (f b)</code> mapping the morphisms. The standard <code>Functor</code> class is for <em>endofunctors on <strong>Hask</strong></em> - the special case in which <code>k1 ~ k2 ~ *</code> and <code>c ~ d ~ (-&gt;)</code>.</p>
<img src="/images/2017-12-15-functor-functors/hask.jpg" alt="Endofunctors on Hask" />
<p>Given two categories <code>C</code> and <code>D</code>, you can construct the category of functors between <code>C</code> and <code>D</code>, written as <code>[C, D]</code>. Objects in this category are functors from <code>C</code> to <code>D</code>, and morphisms are natural transformations between those functors. Since <code>[C, D]</code> is a regular category, you can of course have functors mapping that category to other categories. So in Haskell that'd be a type of kind <code>(k1 -&gt; k2) -&gt; k3</code>. I'll call such types <em>functor functors</em>.</p>
<p>We're talking about record templates of kind <code>(* -&gt; *) -&gt; *</code>. This fits the pattern of a functor from the functor category, with <code>k1 ~ k2 ~ k3 ~ *</code>. So the functor category in question is the category of endofunctors on <strong>Hask</strong> (that is, members of the standard <code>Functor</code> class), and the destination category is <strong>Hask</strong>. So it's reasonable to expect record templates to be functorial in their argument:</p>
<pre><code class="language-haskell">-- natural transformations between functors f and g
type f ~&gt; g = forall x. f x -&gt; g x

-- &quot;functor functors&quot;, functors from the functor category
class FFunctor f where
    ffmap :: (Functor g, Functor h) =&gt; (g ~&gt; h) -&gt; f g -&gt; f h

instance FFunctor FormTemplate where
    ffmap eta (FormTemplate email cardType cardNumber cardExpiry)
        = FormTemplate
            (eta email)
            (eta cardType)
            (eta cardNumber)
            (eta cardExpiry)
</code></pre>
<p><code>FFunctor</code> comes with the usual functor laws. The only difference is the types.</p>
<pre><code class="language-haskell">-- identity
ffmap id = id

-- composition
ffmap (eta . phi) = ffmap eta . ffmap phi
</code></pre>
<img src="/images/2017-12-15-functor-functors/ffunctor.jpg" alt="Functor functors" />
<p><code>ffmap</code> encodes the notion of generalising the functor a template has been instantiated with. If you can embed the functor <code>f</code> into <code>g</code>, then you can map a record of <code>f</code>s to a record of <code>g</code>s by embedding each <code>f</code>. (This is also sometimes called &quot;hoisting&quot;.) For example, the boring <code>Identity</code> functor can be embedded into an arbitrary <code>Applicative</code> by injecting the contained value using <code>pure</code>. We can use this to turn a total record into a partial one:</p>
<pre><code class="language-haskell">generalise :: Applicative f =&gt; Identity a -&gt; f a
generalise (Identity x) = pure x

toPartial :: FFunctor t =&gt; Record t -&gt; Partial t
toPartial = ffmap generalise
</code></pre>
<h2 id="traversing-records"><a href="#traversing-records">Traversing Records

</a></h2>
<p>Now that we have a new dog, it's natural to ask which old tricks we can teach it. With the intuition that a template <code>t f</code> is like a container of <code>f</code>s, what does it mean to traverse such a container? <code>sequenceA :: Applicative f =&gt; t (f a) -&gt; f (t a)</code> takes a container of strategies to produce values and sequences them to get a strategy to produce a container of values. Replacing <em>value</em> with <em>functor</em> in the above sentence, it's clear that we need to decide on a notion of &quot;strategy to produce a functor&quot;. <a href="https://stackoverflow.com/questions/44187945/what-should-a-higher-order-traversable-class-look-like">With thanks to Li-yao Xia</a>, the simplest of such notions is a regular applicative functor <code>a</code> returning a functorial value <code>g x</code> - that is, <code>Compose a g</code>.</p>
<pre><code class="language-haskell">class FFunctor t =&gt; FTraversable t where
    ftraverse :: (Functor f, Functor g, Applicative a)
              =&gt; (f ~&gt; Compose a g) -&gt; t f -&gt; a (t g)
    ftraverse eta = fsequence . ffmap eta
    fsequence :: (Functor f, Applicative a)
              =&gt; t (Compose a f) -&gt; a (t f)
    fsequence = ftraverse id

ffmapDefault :: (Functor f, Functor g, FTraversable t)
             =&gt; (f ~&gt; g) -&gt; t f -&gt; t g
ffmapDefault eta =
    runIdentity . ftraverse (Compose . Identity . eta)

fsequence' :: (FTraversable t, Applicative a) =&gt; t a -&gt; a (Record t)
fsequence' = ftraverse (Compose . fmap Identity)
</code></pre>
<p>The <code>FTraversable</code> laws come about by adjusting the <code>Traversable</code> laws to add some <code>Compose</code>-bookkeeping.</p>
<pre><code class="language-haskell">-- naturality
nu . ftraverse eta = ftraverse (Compose . nu . getCompose . eta)
-- for any applicative transformation nu

-- identity
ftraverse (Compose . Identity) = Identity

-- composition
ftraverse (Compose . Compose . fmap (getCompose.phi) . getCompose . eta)
    = Compose . fmap (ftraverse phi) . ftraverse eta
</code></pre>
<p>Implementations of <code>traverse</code> look like implementations of <code>fmap</code> but in an applicative context. Likewise, implementations of <code>ftraverse</code> look like implementations of <code>ffmap</code> in an applicative context, with a few <code>getCompose</code>s scattered around.</p>
<pre><code class="language-haskell">instance FTraversable FormTemplate where
    ftraverse eta (FormTemplate email cardType cardNumber cardExpiry)
        = FormTemplate &lt;$&gt;
            (getCompose $ eta email) &lt;*&gt;
            (getCompose $ eta cardType) &lt;*&gt;
            (getCompose $ eta cardNumber) &lt;*&gt;
            (getCompose $ eta cardExpiry)
</code></pre>
<p>This is where things start to get interesting. The <code>toForm</code> function, which converts a draft form to a regular form if all of its fields have been filled in, can be defined tersely in terms of <code>ftraverse</code>.</p>
<pre><code class="language-haskell">toRecord :: FTraversable t =&gt; Partial t -&gt; Maybe (Record t)
toRecord = ftraverse (Compose . fmap Identity)

toForm :: DraftForm -&gt; Maybe Form
toForm = toRecord
</code></pre>
<p>Here's another example: a generic program, defined by analogy to <code>Foldable</code>'s <code>foldMap</code>, to collapse the fields of a record into a monoidal value. Note that <code>f () -&gt; m</code> is isomorphic to, but simpler than, <code>forall x. f x -&gt; m</code>. Annoyingly, we have to give a type signature to <code>mkConst</code> to resolve the ambiguity over <code>g</code> in the call to <code>ftraverse</code>. I'm picking <code>Empty</code> as a way of demonstrating that I have nothing up my sleeves.</p>
<pre><code class="language-haskell">data Empty a deriving Functor

ffoldMap :: forall f t m. (Monoid m, Functor f, FTraversable t)
         =&gt; (f () -&gt; m) -&gt; t f -&gt; m
ffoldMap f = getConst . ftraverse mkConst
    where
        -- using ScopedTypeVariables to bind f
        mkConst :: f x -&gt; Compose (Const m) Empty x
        mkConst = Compose . Const . f . ($&gt; ())
</code></pre>
<h2 id="zipping-templates"><a href="#zipping-templates">Zipping templates

</a></h2>
<p>Given a pair of records of the same shape <code>t</code>, we should be able to combine them point-wise, matching up the fields of each: <code>fzip :: t f -&gt; t g -&gt; t (Product f g)</code>. In <strong>Hask</strong>, &quot;combining point-wise&quot; is exactly what the &quot;reader&quot; applicative <code>(-&gt;) r</code> does, so any functor which enjoys an isomorphism to <code>(-&gt;) r</code> for some <code>r</code> has at least a zippy <code>Applicative</code> instance. Such functors are called <em>representable functors</em> and they are members of the class <a href="https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html#t:Representable"><code>Representable</code></a>.</p>
<p>Of course, we're working with functors from the functor category, so the relevant notion of <code>Representable</code> will need a little adjustment. Instead of an isomorphism to a function <code>(-&gt;) r</code> we'll use an isomorphism to a natural transformation <code>(~&gt;) r</code>.</p>
<pre><code class="language-haskell">class FFunctor t =&gt; FRepresentable t where
    type FRep t :: * -&gt; *
    ftabulate :: (FRep t ~&gt; f) -&gt; t f
    findex :: t f -&gt; FRep t a -&gt; f a

fzipWith :: FRepresentable t
         =&gt; (forall x. f x -&gt; g x -&gt; h x)
         -&gt;            t f -&gt; t g -&gt; t h
fzipWith f t u = ftabulate $ \r -&gt; f (findex t r) (findex u r)

fzipWith3 :: FRepresentable t
          =&gt; (forall x. f x -&gt; g x -&gt; h x -&gt; k x)
          -&gt;            t f -&gt; t g -&gt; t h -&gt; t k
fzipWith3 f t u v = ftabulate $
    \r -&gt; f (findex t r) (findex u r) (findex v r)

fzip :: FRepresentable t =&gt; t f -&gt; t g -&gt; t (Product f g)
fzip = fzipWith Pair
</code></pre>
<p>The laws for <code>FRepresentable</code> simply state that <code>ftabulate</code> and <code>findex</code> must witness an isomorphism:</p>
<pre><code class="language-haskell">-- isomorphism
ftabulate . findex = findex . ftabulate = id
</code></pre>
<p><code>FRep</code> will typically be a GADT: it tells you what type of value one should expect to find at a given position in a record.</p>
<pre><code class="language-haskell">data FormTemplateRep a where
    Email :: FormTemplateRep Text
    CardType :: FormTemplateRep CardType
    CardNumber :: FormTemplateRep Text
    CardExpiry :: FormTemplateRep Day

instance FRepresentable FormTemplate where
    type FRep FormTemplate = FormTemplateRep

    ftabulate eta = FormTemplate
        (eta Email)
        (eta CardType)
        (eta CardNumber)
        (eta CardExpiry)
    
    findex p Email = _email p
    findex p CardType = _cardType p
    findex p CardNumber = _cardNumber p
    findex p CardExpiry = _cardExpiry p
</code></pre>
<p>Something useful you can do with this infrastructure: filling in defaults for missing values of a partial record. Or, looking at it the other way, overriding certain parts of a record.</p>
<pre><code class="language-haskell">with :: FRepresentable t =&gt; Record t -&gt; Partial t -&gt; Record t
with = fzipWith override
    where override x Nothing = x
          override _ (Just y) = Identity y

fillInDefaults :: FRepresentable t =&gt; Partial t -&gt; Record t -&gt; Record t
fillInDefaults t defaults = defaults `with` t
</code></pre>
<p>You can also make a record of <code>Monoid</code> values into a <code>Monoid</code>, once again by zipping.</p>
<pre><code class="language-haskell">newtype Wrap t f = Wrap { unWrap :: t f }
makeWrapped ''Wrap  -- from Control.Lens.Wrapped

instance (FRepresentable t, Monoid c) =&gt; Monoid (Wrap t (Const c)) where
    mempty = Wrap $ ftabulate (const (Const mempty))
    Wrap t `mappend` Wrap u = Wrap $ fzipWith mappend t u
</code></pre>
<h2 id="lenses"><a href="#lenses">Lenses

</a></h2>
<p>Rather than come up with a new notion of <code>Lens</code> formulated in terms of <code>FFunctor</code>, we can reuse the standard <code>Lens</code> type as long as we're careful about how polymorphic lenses should be. Specifically, a lens into a record template should express no opinion as to which functor the template should be instantiated with.</p>
<pre><code class="language-haskell">newtype FLens t a = FLens (forall f. Lens' (t f) (f a))
</code></pre>
<p>We can store a template's lenses in an instance of the template itself!</p>
<pre><code class="language-haskell">type Lenses t = t (FLens t)

class HasLenses t where
    lenses :: Lenses t

makeLenses ''FormTemplate
instance HasLenses FormTemplate where
    lenses = FormTemplate {
        _email = FLens email,
        _cardType = FLens cardType,
        _cardNumber = FLens cardNumber,
        _cardExpiry = FLens cardExpiry
    }
</code></pre>
<h2 id="compositional-validation"><a href="#compositional-validation">Compositional Validation

</a></h2>
<p>Now for an extended example: form validation. We'll be making use of all of the tools from above - zipping, traversing, and mapping - to design a typed API for validating individual fields of a form.</p>
<p><code>Either</code> isn't a great choice for a validation monad, because <code>Either</code> aborts the computation at the first failure. You typically want to report all the errors in a form. Instead, we'll be working with the following type, which is isomorphic to <code>Either</code> but with an <code>Applicative</code> instance which returns <em>all</em> of the failures in a given computation, combining the values using a <code>Monoid</code>. So it's kind of a Frankensteinian mishmash of the <code>Either</code> and <code>Writer</code> applicatives.</p>
<pre><code class="language-haskell">data Validation e a = Failure e | Success a deriving Functor

instance Bifunctor Validation where
    bimap f g (Failure e) = Failure (f e)
    bimap f g (Success x) = Success (g x)

instance Monoid e =&gt; Applicative (Validation e) where
    pure = Success
    Success f &lt;*&gt; Success x = Success (f x)
    Failure e1 &lt;*&gt; Failure e2 = Failure (e1 `mappend` e2)
    Failure e1 &lt;*&gt; _ = Failure e1
    _ &lt;*&gt; Failure e2 = Failure e2
</code></pre>
<p>This <code>Applicative</code> instance has no compatible <code>Monad</code> instance.</p>
<p>We'll build a library for validation processes which examine a single field of a record at a time. A validation rule for a field typed <code>a</code> is a function which takes an <code>a</code> and returns a <code>Validation e a</code>.</p>
<pre><code class="language-haskell">newtype Validator e a = Validator { runValidator :: a -&gt; Validation e a }

-- a validator which always succeeds
noop :: Validator e a
noop = Validator Success
</code></pre>
<p>If a given field has multiple validation rules, you can compose them under the assumption that each validator leaves its input unchanged.</p>
<pre><code class="language-haskell">(&amp;&gt;) :: Monoid e =&gt; Validator e a -&gt; Validator e a -&gt; Validator e a
Validator f &amp;&gt; Validator g = Validator $ \x -&gt; f x *&gt; g x

-- for example
emailValidator :: Validator [Text] Text
emailValidator = hasAtSymbol &amp;&gt; hasTopLevelDomain
    where
        hasAtSymbol = Validator $ \email -&gt; 
            if &quot;@&quot; `isInfixOf` email
            then Success email
            else Failure [&quot;No @ in email&quot;]
        hasTopLevelDomain = Validator $ \email -&gt;
            if any (`isSuffixOf` email) topLevelDomains
            then Success email
            else Failure [&quot;Invalid TLD&quot;]
        topLevelDomains = [&quot;.com&quot;, &quot;.org&quot;, &quot;.co.uk&quot;]  -- etc
</code></pre>
<p>The plan is to store these <code>Validator</code>s in a record template, zip them along an instance of the record itself, and then traverse the result to get either a validated record or a collection of errors. To make things interesting, we'll store the validation results for a given field in the matching field of another record.</p>
<pre><code class="language-haskell">type Validators e t = t (Validator e)
type Errors e t = t (Const e)

-- turn a record of validators into a validator of records
validate :: (HasLenses t, FTraversable t, FRepresentable t, Monoid e)
         =&gt; Validators e t
         -&gt; Validator (Errors e t) (Record t)
validate validators = Validator $ \record -&gt;
    first unWrap $
    fsequence' $
    fzipWith3 applyValidator lenses validators record
    where
        applyValidator
            (FLens lens)
            (Validator validator)
            (Identity value) =
                let setError e = mempty &amp; _Wrapped'.lens._Wrapped' .~ e
                in first setError $ validator value
</code></pre>
<p><code>applyValidator</code> takes a lens into a record field, a validator for that field and the value in that field. It applies the validator to the value; upon failure it stores the error message (<code>e</code>) in the correct field of the <code>Errors</code> record using the lens. <code>fzipWith3</code> handles the logic of running <code>applyValidator</code> for each field of the record, then <code>fsequence'</code> combines the resulting <code>Validation</code> applicative actions into a single one. So all of the errors from all of the fields are eventually collected into the matching fields of the <code>Errors</code> record and combined monoidally.</p>
<p>A quick test, wherein I test validation on the email field:</p>
<pre><code>ghci&gt; let formValidator = validate
    $ FormTemplate emailValidator noop noop noop
ghci&gt; let today = read &quot;2017-08-17&quot; :: Day

ghci&gt; let form1 = FormTemplate
    (Identity &quot;bhodgson@stackoverflow.com&quot;)
    (Identity Visa)
    (Identity &quot;1234567890123456&quot;)
    (Identity today)
ghci&gt; runValidator formValidator form1
Success (FormTemplate {
    _email = Identity &quot;bhodgson@stackoverflow.com&quot;,
    _cardType = Identity Visa,
    _cardNumber = Identity &quot;1234567890123456&quot;,
    _cardExpiry = Identity 2017-08-17
    })

ghci&gt; let form2 = FormTemplate
    (Identity &quot;notanemail&quot;)
    (Identity Visa)
    (Identity &quot;1234567890123456&quot;)
    (Identity today)
ghci&gt; runValidator formValidator form2
Failure (FormTemplate {
    _email = Const [&quot;No @ in email&quot;,&quot;Invalid TLD&quot;],
    _cardType = Const [],
    _cardNumber = Const [],
    _cardExpiry = Const []
    })
</code></pre>
<h2 id="code-review"><a href="#code-review">Code review

</a></h2>
<p>So we have a categorical framework for working with records and templates. Other things fit into this framework, more or less neatly:</p>
<ul>
<li>Monad transformers are often functorial in their <code>m</code> argument.
</li>
<li><code>Fix f</code> (a &quot;list of <code>f</code>s&quot;, if you will) is also a functor functor, where <code>ffmap</code>ping represents a change of variables.
</li>
<li>Since the <code>Const</code>, <code>Sum</code>, <code>Product</code> and <code>Compose</code> type combinators are poly-kinded, they can be reused as functor functors too.
</li>
<li>Add another primitive <code>FFunctor</code> to apply a functor to a type, <code>newtype At a f = At { getAt :: f a }</code>, and you have a kit to build polynomial functor functors with which you can build templates and write generic programs.
</li>
</ul>
<p>One design decision I made when developing the <code>FFunctor</code> class was to give <code>ffmap</code> a <code>(Functor f, Functor g)</code> constraint, so you can only <code>ffmap</code> between types that are in fact functors. This is mathematically principled in some sense, but it has certain engineering tradeoffs compared to an unconstrained type for <code>ffmap</code>. It enables more instances of <code>FFunctor</code> - for example, you can only write <code>Fix</code>'s <code>ffmap</code> with a <code>Functor</code> constraint for either the input or output type parameters - but it rules out certain usages of <code>ffmap</code>. You can't <code>ffmap</code> over a template containing <code>Validator</code>s, for example, because <code>Validator</code> is not a <code>Functor</code>. I <em>didn't</em> put the same <code>Functor</code> constraints into <code>FRepresentable</code>'s methods. An <code>FRep</code> type typically won't be functorial - it'll be GADT-like - so adding a <code>Functor (FRep t)</code> constraint would be far too restrictive.</p>
<p>You'll notice that the concept of an applicative functor functor is conspicuously absent from my presentation above. <code>FApplicative</code> would probably look something like this:</p>
<pre><code class="language-haskell">newtype (f :-&gt; g) a = Morph { getMorph :: f a -&gt; g a }

class FFunctor t =&gt; FApplicative t where
    fpure :: (forall a. f a) -&gt; t f
    fap :: t (f :-&gt; g) -&gt; t f -&gt; t g

fliftA :: FApplicative t =&gt; (f ~&gt; g) -&gt; t f -&gt; t g
fliftA eta t = fpure (Morph eta) `fap` t

instance FApplicative FormTemplate where
    fpure x = FormTemplate x x x x
    fap
        (FormTemplate
            (Morph f1)
            (Morph f2)
            (Morph f3)
            (Morph f4))
        (FormTemplate
            email
            cardType
            cardNumber
            cardExpiry)
        = FormTemplate
            (f1 email)
            (f2 cardType)
            (f3 cardNumber)
            (f4 cardExpiry)
</code></pre>
<p><code>FApplicative</code> is a more general interface than <code>FRepresentable</code>, in that it supports notions of composition other than zipping. However, that bookkeeping <code>:-&gt;</code> <code>newtype</code> wrapper is inconvenient. With the normal <code>Applicative</code> class you can map an <em>n</em>-ary function over <em>n</em> applicative values directly: <code>f &lt;$&gt; x &lt;*&gt; y &lt;*&gt; z</code>. With <code>FApplicative</code> you have to apply the <code>Morph</code> constructor as many times as <code>f</code> has arguments: <code>fpure (Morph $ \x -&gt; Morph $ \y -&gt; Morph $ \z -&gt; f x y z) `fap` t `fap` u `fap` v</code>, which becomes very unwieldy very quickly. (<a href="https://www.reddit.com/r/haskell/comments/78xxql/structures_of_arrays_functors_and_continuations/doy80ft/">/u/rampion has come up with nicer syntax for this</a>, but it involves <a href="https://gist.github.com/rampion/20291bde6c8568c11f9cc5923d9639eb#file-ffunctor-hs-L28">a more complicated formulation of <code>FApplicative</code></a>.) On the other hand, <code>FApplicative</code> does open up some interesting options for the design of <code>FTraversable</code>: one can traverse in an <code>FApplicative</code> rather than an <code>Applicative</code>. This gives some nice type signatures - <code>fsequence :: (FTraversable t, FApplicative f) =&gt; t f -&gt; f t</code> - and is strictly more general than the <code>FTraversable</code> I gave above, since any <code>Applicative</code> can be lifted into an <code>FApplicative</code> by composition (<code>newtype ComposeAt a f g = ComposeAt { getComposeAt :: f (g a) }</code>).</p>
<p>How useful are these tools in practice? Would I structure a production application around functor functors? Probably not. It's a question of balance - while it's useful to recognise functorial structures in categories other than <strong>Hask</strong> as a thinking tool, actually representing such abstractions in code doesn't always pay off. Haskell already has a rich ecosystem of tools for working with the <code>Functor</code> family, but there's much less code in the wild that's structured around functor functors. This is partly because <code>Functor</code> has the advantage of being a standard class in <code>base</code>, but it's also because code built around functor functors is a little less convenient to work with, typically requiring some tedious <code>newtype</code> bookkeeping.</p>
<p>Over the course of putting together this article I came across some work by others on this very topic. I've spotted versions of these classes being packaged with bigger libraries such as <a href="http://hackage.haskell.org/package/hedgehog-0.5.1/docs/Hedgehog-Internal-HTraversable.html"><code>hedgehog</code></a> and <a href="https://hackage.haskell.org/package/quickcheck-state-machine-0.2.0/docs/Test-StateMachine-Types-HFunctor.html"><code>quickcheck-state-machine</code></a>. There are also a few packages providing similar tools. The most mature of these seems to be <a href="http://hackage.haskell.org/package/rank2classes"><code>rank2classes</code></a>, which includes some Template Haskell tools for deriving instances; there's also <a href="http://hackage.haskell.org/package/conkin">the Conkin package</a>, which has <a href="https://github.com/rampion/conkin/blob/master/README.md">a well-written tutorial</a> focusing on working with data in column-major order.</p>
<p>Haskell's full of big ideas and powerful programming idioms. In this post we saw an example of reinterpreting some familiar tools - <code>Functor</code>, <code>Traversable</code> and <code>Representable</code> - in a new context. With the intuition that a record template is a container of functors, and the formalism of functors from the functor category, we were able to reuse intuitions about those familiar tools to write terse and generic programs.</p>

]]></summary>
</entry>
<entry>
    <title>Recursion Without Recursion</title>
    <link href="http://www.benjamin.pizza/posts/2017-11-13-recursion-without-recursion.html" />
    <id>http://www.benjamin.pizza/posts/2017-11-13-recursion-without-recursion.html</id>
    <published>2017-11-13T00:00:00Z</published>
    <updated>2017-11-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<p>If you visit <a href="https://www.stackoverflow.com/jobs">Stack Overflow Jobs</a> you'll see that our job search form supports a simple advanced search syntax, including Boolean operators and a number of custom filters such as technology tags and minimum salary. For example, I hate writing JavaScript, but my loyalties can be bought, so I might type <a href="https://stackoverflow.com/jobs?sort=i&amp;q=%5Bc%23%5D+and+(not+%5Bjavascript%5D+or+salary%3A50000gbp)"><code>[c#] and (not [javascript] or salary:50000gbp)</code></a> into the search box. This advanced search syntax is called JQL, for <em>Jobs Query Language</em>.</p>
<p>It should come as no surprise that our codebase contains a miniature compiler for our miniature query language. Our compiler looks much like any other compiler: there's a parser which produces an abstract syntax tree (hereafter <em>AST</em>), a pipeline of analysers and transformations which operate on that AST, and a code generator which turns the JQL into an ElasticSearch query. (Actually, queries that are simple enough end up skipping the Elastic code generation step, instead being used by an interpreter to search an in-memory cache of jobs.)</p>
<img src="/images/2017-11-13-recursion-without-recursion/compiler.jpg" alt="Compiler overview" />
<p>In this post I'm going to focus on the middle part of that pipeline: how to write operations traversing a tree with a minimum of boilerplate.</p>
<h2 id="asts-and-operations"><a href="#asts-and-operations">ASTs and operations

</a></h2>
<p>The JQL AST looks roughly like this:</p>
<pre><code class="language-csharp">abstract class JqlNode {}
class AndNode : JqlNode
{
    public JqlNode Left { get; }
    public JqlNode Right { get; }
}
class OrNode : JqlNode
{
    public JqlNode Left { get; }
    public JqlNode Right { get; }
}
class NotNode : JqlNode
{
    public JqlNode Operand { get; }
}
class SalaryNode : JqlNode
{
    public int Amount { get; }
    public string Currency { get; }
}
class TagNode : JqlNode
{
    public string Tag { get; }
}
</code></pre>
<p>Each syntactic form in the source language is represented as a subclass of <code>JqlNode</code>. Using the example I gave above, the input string <code>[c#] and (not [javascript] or salary:50000gbp)</code> would be represented as:</p>
<pre><code class="language-csharp">new AndNode(
    new TagNode(&quot;c#&quot;),
    new OrNode(
        new NotNode(new TagNode(&quot;javascript&quot;)),
        new SalaryNode(50000, &quot;gbp&quot;)
    )
)
</code></pre>
<img src="/images/2017-11-13-recursion-without-recursion/ast.jpg" alt="The abstract syntax tree" />
<p>When you need to analyse a <code>JqlNode</code>, you use pattern matching to see what type of node you have, and recursively query the operands of <code>And</code>/<code>Or</code>/<code>Not</code> nodes. Here's a function which searches for the <code>TagNode</code>s in a tree:</p>
<pre><code class="language-csharp">IEnumerable&lt;string&gt; ExtractTags(JqlNode node)
{
    switch (node)
    {
        case TagNode t:
            return new[] { t.Tag };
        case AndNode a:
            // recursively extract the tags from the two operands
            return ExtractTags(a.Left).Concat(ExtractTags(a.Right));
        case OrNode o:
            return ExtractTags(o.Left).Concat(ExtractTags(o.Right));
        case NotNode n:
            return ExtractTags(n.Operand);
        case SalaryNode s:
            return Enumerable.Empty&lt;string&gt;();
        default:
            throw new ArgumentOutOfRangeException(nameof(node));
    }
}
</code></pre>
<p>Transforming a <code>JqlNode</code> to produce a new <code>JqlNode</code> is a similar story: you recursively traverse the tree, taking it apart and putting it back together. Here's an example of an optimisation step which never doesn't remove double-negatives, so a query like <code>not (not [java])</code> gets simplified to <code>[java]</code>:</p>
<pre><code class="language-csharp">JqlNode SimplifyDoubleNegatives(JqlNode node)
{
    switch (node)
    {
        case NotNode n1 when n1.Operand is NotNode n2:
            return SimplifyDoubleNegatives(n2.Operand);
        case TagNode t:
            return t;
        case SalaryNode s:
            return s;
        case AndNode a:
            // recursively process the operands and rebuild the node
            return new AndNode(
                SimplifyDoubleNegatives(a.Left),
                SimplifyDoubleNegatives(a.Right)
            );
        case OrNode o:
            return new OrNode(
                SimplifyDoubleNegatives(o.Left),
                SimplifyDoubleNegatives(o.Right)
            );
        case NotNode n:
            return new NotNode(
                SimplifyDoubleNegatives(n.Operand)
            );
        default:
            throw new ArgumentOutOfRangeException(nameof(node));
    }
}
</code></pre>
<p>This type of code gets pretty tedious pretty quickly! In both of these functions, only one of the <code>case</code>s was interesting (<code>case TagNode t</code> in <code>ExtractTags</code> and <code>case NotNode n1 when n1.Operand is NotNode n2</code> in <code>SimplifyDoubleNegatives</code>); the rest of each function was just boilerplate to recursively operate on nodes' children. You're interested in a particular syntactic pattern, but searching the whole tree for that pattern requires more code than finding the pattern does. In the real JQL compiler we have about a dozen subclasses of <code>JqlNode</code>, so around 90% of the code in each operation is boilerplate!</p>
<h2 id="easier-querying"><a href="#easier-querying">Easier Querying

</a></h2>
<p>Here's the first insight that'll help us improve on this situation. In <code>ExtractTags</code> we were searching the tree for nodes satisfying a particular pattern. But supposing you had a list of every possible subtree - the root node, all of its children, all of their children, and so on - you could use LINQ to query that list to find nodes satisfying the pattern you're looking for. We'll call the function which extracts the list of subtrees <code>SelfAndDescendants</code>.</p>
<p>Given a tree like the example from above (<code>[c#] and (not [javascript] or salary:50000gbp)</code>), <code>SelfAndDescendants</code> will yield every subtree in a depth-first, left-to-right manner:</p>
<pre><code class="language-csharp">new JqlNode[]
{
    new AndNode(
        new TagNode(&quot;c#&quot;),
        new OrNode(
            new NotNode(new TagNode(&quot;javascript&quot;)),
            new SalaryNode(50000, &quot;gbp&quot;)
        )
    ),
    new TagNode(&quot;c#&quot;),
    new OrNode(
        new NotNode(new TagNode(&quot;javascript&quot;)),
        new SalaryNode(50000, &quot;gbp&quot;)
    ),
    new NotNode(new TagNode(&quot;javascript&quot;)),
    new TagNode(&quot;javascript&quot;),
    new SalaryNode(50000, &quot;gbp&quot;)
}
</code></pre>
<p>Here's <code>SelfAndDescendants</code> in use:</p>
<pre><code class="language-csharp">IEnumerable&lt;string&gt; ExtractTags(JqlNode node)
    =&gt; node
        .SelfAndDescendants()
        .OfType&lt;TagNode&gt;()
        .Select(n =&gt; n.Tag);
</code></pre>
<p>What an improvement! This code is much shorter, but more importantly it's clearer and more direct. You can directly read off the intention of the code, rather than having to decipher the pattern of recursive calls. It's also harder to get wrong - I personally am rather prone to forgetting to make a recursive call when I'm writing these sorts of functions. What's more, <code>SelfAndDescendants</code> is totally reusable. If you can write a LINQ query, you can get whatever information you need from a <code>JqlNode</code>.</p>
<p>Of course, the pattern-matching and recursion has to go somewhere, and that somewhere is the reusable <code>SelfAndDescendants</code> function.</p>
<pre><code class="language-csharp">public static IEnumerable&lt;JqlNode&gt; SelfAndDescendants(this JqlNode node)
{
    yield return node;
    switch (node)
    {
        case TagNode t:
            yield break;
        case SalaryNode s:
            yield break;
        case AndNode a:
            foreach (var descendant in SelfAndDescendants(a.Left))
                yield return descendant;
            foreach (var descendant in SelfAndDescendants(a.Right))
                yield return descendant;
        case OrNode o:
            foreach (var descendant in SelfAndDescendants(o.Left))
                yield return descendant;
            foreach (var descendant in SelfAndDescendants(o.Right))
                yield return descendant;
        case NotNode n:
            foreach (var descendant in SelfAndDescendants(n.Operand))
                yield return descendant;
        default:
            throw new ArgumentOutOfRangeException(nameof(node));
    }
}
</code></pre>
<p>Google crawls links between pages for you, so you can search the Web for a specific piece of information; <code>SelfAndDescendants</code> crawls pointers between nodes for you, so you can search a tree for a specific piece of information. Programming tree traversals by hand is like manually clicking every link on the Web!</p>
<h2 id="a-reusable-transformer"><a href="#a-reusable-transformer">A Reusable Transformer

</a></h2>
<p>How about transforming a JQL AST? <code>SimplifyDoubleNegatives</code> searches a JQL tree for a pattern and rebuilds a new version of the tree. Can this be extracted into a reusable function?</p>
<p>To rewrite a tree, you search the tree for nodes satisfying the pattern you're looking for and replace them. As with <code>SelfAndDescendants</code>, the trick is to separate the responsibilities of <em>looking at every node in the tree</em> and <em>deciding whether to replace a given node</em>. You can write a higher-order function - let's call it <code>Rewrite</code> - which applies a <code>Func</code> to every node in a JQL tree from bottom to top; then it's the <code>Func</code>'s job to decide what to do with each node.</p>
<p>For example, <code>Rewrite</code> will take the query above (<code>[c#] and (not [javascript] or salary:50000gbp)</code>) and a function <code>transformer</code>, and compute the expression:</p>
<pre><code class="language-csharp">transformer(new AndNode(
    transformer(new TagNode(&quot;c#&quot;)),
    transformer(new OrNode(
        transformer(new NotNode(
            transformer(new TagNode(&quot;javascript&quot;))
        )),
        transformer(new SalaryNode(50000, &quot;gbp&quot;))
    ))
))
</code></pre>
<p>So <code>transformer</code> gets applied to every subtree exactly once. <code>Rewrite</code> is a mapping operation, like LINQ's <code>Select</code>. Here's how it's implemented.</p>
<pre><code class="language-csharp">static JqlNode Rewrite(
    this JqlNode node,
    Func&lt;JqlNode, JqlNode&gt; transformer
)
{
    switch (node)
    {
        case TagNode t:
            return transformer(t);
        case SalaryNode s:
            return transformer(s);
        case AndNode a:
            return new AndNode(
                transformer(a.Left),
                transformer(a.Right)
            );
        case OrNode o:
            return new OrNode(
                transformer(o.Left),
                transformer(o.Right)
            );
        case NotNode n:
            return new NotNode(transformer(n.Operand));
        default:
            throw new ArgumentOutOfRangeException(nameof(node));
    }
}
</code></pre>
<p>To use this <code>Rewrite</code> method, you write a transformation function which calculates a replacement for each node. If there's no replacing to do, it just returns the same node. Like this:</p>
<pre><code class="language-csharp">JqlNode SimplifyDoubleNegatives(JqlNode node)
    =&gt; node.Rewrite(
        n =&gt; n is NotNode n1 &amp;&amp; n1.Operand is NotNode n2
            ? n2.Operand
            : n
    );
</code></pre>
<p>Once again, this code is a huge improvement over the verbose version which used <code>switch</code> and recursion. <code>Rewrite</code> allows us to get straight to the point and only think about the parts of the tree we're interested in.</p>
<h2 id="from-pattern-to-library"><a href="#from-pattern-to-library">From Pattern to Library

</a></h2>
<p><code>Rewrite</code> and <code>SelfAndDescendants</code> wrap up two particular types of recursion, for reuse in a wide variety of operations. This is a powerful way to program - gone are the days of writing a bespoke traversal for every operation! - and these two functions form the basis of most of the operations in the production JQL compiler, but in this form they don't constitute a library. <code>SelfAndDescendants</code> and <code>Rewrite</code>, as written above, have knowledge of <code>JqlNode</code> baked in to them; you have to hand-write equivalent functions to work on your own datatypes.</p>
<p>We can turn this design into something generic, though, by abstracting over tree-shaped structures. What do we mean when we say a datatype is tree-shaped? The distinguishing feature which makes a tree a tree, unlike any other datatype, is recursion: each node in a tree has <em>children</em> which are also nodes.</p>
<img src="/images/2017-11-13-recursion-without-recursion/children.jpg" alt="Nodes and their children" />
<p>As the picture shows, you can reach every node in a tree just by looking at each node's children. If you can show me how to replace your children, I can replace your children's children and so on. So let's use an interface to model the notion of an object with a collection of children.</p>
<pre><code class="language-csharp">interface IRewritable&lt;T&gt; where T : IRewritable&lt;T&gt;
{
    IEnumerable&lt;T&gt; GetChildren();
    T SetChildren(IEnumerable&lt;T&gt; newChildren);
}
</code></pre>
<p>A type <code>T</code> is <em>rewritable</em> if it knows how to access its immediate children - in other words, if you can get and set an <code>IEnumerable&lt;T&gt;</code> representing a node's children. We're working with immutable trees, remember, so <code>SetChildren</code> doesn't modify the current instance - it returns a new <code>T</code> the same as the current instance but with different children. Part of the contract of <code>IRewritable</code> is that you shouldn't call <code>SetChildren</code> with a different number of children to what you got from <code>GetChildren</code>. For example, an <code>And</code> node always has two children, so you shouldn't try to call <code>SetChildren</code> with only one child (because, how would the <code>And</code> node rebuild itself?).</p>
<p>Now we can package up those <code>Rewrite</code> and <code>SelfAndDescendants</code> functions for any rewritable object, once and for all. If you show me how to reach each node's immediate children, I can recursively apply that recipe to look at the children's children and so on.</p>
<pre><code class="language-csharp">static IEnumerable&lt;T&gt; SelfAndDescendants&lt;T&gt;(this T node)
    where T : IRewritable&lt;T&gt;
{
    yield return node;
    foreach (var child in node.GetChildren())
        foreach (var descendant in SelfAndDescendants(child))
            yield return descendant;
}
static T Rewrite&lt;T&gt;(this T node, Func&lt;T, T&gt; transformer)
    where T : IRewritable&lt;T&gt;
{
    var children = node.GetChildren();
    var newChildren = children.Select(c =&gt; c.Rewrite(transformer)).ToList();
    var nodeWithNewChildren = node.SetChildren(newChildren);
    return transformer(nodeWithNewChildren);
}
</code></pre>
<p>You typically implement <code>IRewritable</code> abstractly on the base type, using overrides on each subclass to find the children.</p>
<pre><code class="language-csharp">abstract class JqlNode : IRewritable&lt;JqlNode&gt;
{
    public abstract IEnumerable&lt;JqlNode&gt; GetChildren();
    public abstract JqlNode SetChildren(IEnumerable&lt;JqlNode&gt; newChildren);
}
class AndNode : JqlNode
{
    // fields as before
    public override IEnumerable&lt;JqlNode&gt; GetChildren()
        =&gt; new[] { Left, Right };
    public override JqlNode SetChildren(IEnumerable&lt;JqlNode&gt; newChildren)
        =&gt; new AndNode(
            newChildren.ElementAt(0), 
            newChildren.ElementAt(1)
        );
}
class OrNode : JqlNode
{
    public override IEnumerable&lt;JqlNode&gt; GetChildren()
        =&gt; new[] { Left, Right };
    public override JqlNode SetChildren(IEnumerable&lt;JqlNode&gt; newChildren)
        =&gt; new OrNode(
            newChildren.ElementAt(0), 
            newChildren.ElementAt(1)
        );
}
class NotNode : JqlNode
{
    public override IEnumerable&lt;JqlNode&gt; GetChildren()
        =&gt; new[] { Operand };
    public override JqlNode SetChildren(IEnumerable&lt;JqlNode&gt; newChildren)
        =&gt; new NotNode(newChildren.Single());
}
class SalaryNode : JqlNode
{
    public override IEnumerable&lt;JqlNode&gt; GetChildren()
        =&gt; Enumerable.Empty&lt;JqlNode&gt;();
    public override JqlNode SetChildren(IEnumerable&lt;JqlNode&gt; newChildren)
        =&gt; this;
}
class TagNode : JqlNode
{
    public override IEnumerable&lt;JqlNode&gt; GetChildren()
        =&gt; Enumerable.Empty&lt;JqlNode&gt;();
    public override JqlNode SetChildren(IEnumerable&lt;JqlNode&gt; newChildren)
        =&gt; this;
}
</code></pre>
<p>Note that there isn't a single line of recursion in the JQL-specific code. It's all wrapped up in the <code>SelfAndDescendants</code> and <code>Rewrite</code> functions, which are totally generic and reusable for any type of tree.</p>
<p>The old-fashioned way of writing reusable tree traversals is the Visitor pattern: you put the recursive traversal code in a base class, with virtual methods for each type of node that can be overridden to carry out specific operations. (This is how the Roslyn API works, for example.) <code>IRewritable</code> is a clear improvement over the Visitor pattern. Being designed around base classes and overriding, the Visitor pattern is far clunkier to use than the functional API I outlined above; and <code>IRewritable</code> allows operations like <code>Rewrite</code> can be written totally generically, whereas with the Visitor pattern every type of tree has its own Visitor base class.</p>
<h2 id="sawmill"><a href="#sawmill">Sawmill

</a></h2>
<p>I've named this generic tree-processing library Sawmill - because it's all about taking trees apart! - and it's available on <a href="https://www.nuget.org/packages/Sawmill">NuGet</a> and <a href="https://github.com/benjamin-hodgson/Sawmill">GitHub</a>. I'll outline some improvements on the design I demonstrated above, which you'll find in Sawmill.</p>
<p>First, what I find remarkable about this design is its power-to-weight ratio. <code>IRewritable</code> is a very simple interface with an easily-grasped meaning, but you can build a load of rich, generic tools on top of it. Sawmill contains versions of <code>SelfAndDescendants</code> and <code>Rewrite</code>, but also a bunch of other extension methods at varying levels of nicheness, all getting squeezed through the <code>IRewritable</code> interface:</p>
<ul>
<li>A family of versions of <code>SelfAndDescendants</code> capturing a variety of traversal orders (preorder, postorder and breadth-first), in both eager and lazy form
</li>
<li>A <code>Fold</code> method for reducing a whole tree to a value, like LINQ's <code>Aggregate</code>
</li>
<li>An iterative version of <code>Rewrite</code> which rewrites an expression repeatedly until it reaches a normal form
</li>
<li>Functions for replacing one node at a time
</li>
<li>A method to get an efficient mutable view of a node and its neighbours, which supports complex sequences of edits to a localised part of a tree
</li>
<li>Tools to help you implement <code>IRewriter</code>, either using a typed fluent interface or using reflection and code generation.
</li>
<li>Some minor API changes to the outline above, to enable greater efficiency for certain common cases.
</li>
</ul>
<p>I've also had success implementing <code>IRewritable</code> for a variety of tree-like types. Sawmill comes bundled with versions of all of these extension methods for some well-known tree types - <code>Expression</code>, <code>XmlNode</code>, and <code>XElement</code> - and I've written extension packages which do the same for <code>Newtonsoft.Json.Linq</code> and Roslyn's syntax trees. (These implementations actually use a separate <code>IRewriter</code> interface, because of course I can't add a new interface to the above types.) Realising that I could use Sawmill to layer a simple, uniform API on top of preexisting objects felt like a real validation of the design.</p>
<p>Sawmill's version of <code>Rewrite</code> also makes an important optimisation which I glossed over above: parts of the tree which the <code>transformer</code> function didn't change are <em>shared</em> between the new and old versions of the tree. If you change a single node, you only have to rebuild that node's ancestors (because their children have changed), not the parts of the tree you didn't touch.</p>
<img src="/images/2017-11-13-recursion-without-recursion/sharing.jpg" alt="The sharing optimisation" />
<p>(This is safe for immutable trees like those in Roslyn; for mutable trees like <code>XmlNode</code> the whole tree has to be copied if any part of it changes. This makes me sad - in my view those types should have been immutable all along.)</p>
<p>Finally and most importantly, I want to acknowledge Neil Mitchell's great work in his <a href="https://hackage.haskell.org/package/uniplate"><code>uniplate</code> Haskell library</a> (and <a href="https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Plated.html">its modernised port in <code>lens</code></a>), upon which Sawmill is based. I wouldn't even have thought of this C# library if I hadn't already encountered it in Haskell. It's weird to think that <a href="http://ndmitchell.com/downloads/paper-uniform_boilerplate_and_list_processing-30_sep_2007.pdf"><code>uniplate</code>'s accompanying article</a> was published in 2007! Someone - my mum, if you must know - once told me that in the field of medicine it takes a decade for new research to reach mainstream practice. I think that process might take even longer in computer science, but I hope that in writing this I've helped these ideas along a little.</p>

]]></summary>
</entry>

</feed>
